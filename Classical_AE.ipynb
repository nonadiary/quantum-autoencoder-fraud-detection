{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4078590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Core libraries for data processing and machine learning\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "# ConvergenceWarning 무시\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ea7c1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 946 samples, 30 features\n",
      "Fraud rate: 0.5000 (473 fraud cases)\n",
      "\n",
      "Training set: (756, 4)\n",
      "Test set: (190, 4)\n",
      "PCA explained variance ratio: [0.38421646 0.10954544 0.06067923 0.05752846]\n",
      "Total variance explained: 0.6120\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Data Loading and Preprocessing Pipeline\n",
    "# ==========================================\n",
    "\n",
    "# Load preprocessed credit card fraud dataset\n",
    "df = pd.read_csv(\"preprocessed-creditcard.csv\")\n",
    "X = df.drop(\"Class\", axis=1).values  # Feature matrix\n",
    "y = df[\"Class\"].values                # Target labels (0: normal, 1: fraud)\n",
    "\n",
    "print(f\"Dataset loaded: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "print(f\"Fraud rate: {np.mean(y):.4f} ({np.sum(y)} fraud cases)\")\n",
    "\n",
    "# Stratified train-test split to maintain class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Feature standardization using Z-score normalization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "# Dimensionality reduction using PCA to match quantum register size\n",
    "pca = PCA(n_components=4, random_state=42)\n",
    "X_train_4d = pca.fit_transform(X_train)\n",
    "X_test_4d  = pca.transform(X_test)\n",
    "\n",
    "print(f\"\\nTraining set: {X_train_4d.shape}\")\n",
    "print(f\"Test set: {X_test_4d.shape}\")\n",
    "print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total variance explained: {np.sum(pca.explained_variance_ratio_):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ac6b2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QUANTUM AUTOENCODER - ANGLE vs ENHANCED qVAE COMPARISON\n",
      "================================================================================\n",
      "Enhanced qVAE Configuration:\n",
      "  - Data Re-uploading: True\n",
      "  - Parallel Embedding: 2x (8 data qubits)\n",
      "  - Alternate RY/RX: True\n",
      "  - SWAP Test: True\n",
      "  - Total qubits: 13 (8 data + 2 ref + 2 trash + 1 control)\n",
      "\n",
      "Training Configuration: {'epochs_angle': 12, 'epochs_qvae': 15, 'epochs_classical': 15, 'batch_size_angle': 20, 'batch_size_qvae': 8, 'batch_size_classical': 20, 'learning_rate': 0.05}\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Configuration for Two-Strategy Comparison\n",
    "# ==========================================\n",
    "\n",
    "# ENHANCED qVAE FEATURES\n",
    "USE_DATA_REUPLOADING = True     # Embed data at each variational layer\n",
    "USE_PARALLEL_EMBEDDING = 2      # Replicate data across multiple qubits (2x = 8 data qubits)\n",
    "USE_ALTERNATE_EMBEDDING = True  # Alternate between RY and RX rotations\n",
    "USE_SWAP_TEST = True           # Use quantum SWAP test for accurate fidelity measurement\n",
    "\n",
    "# QUANTUM ARCHITECTURE PARAMETERS\n",
    "N_REFERENCE_QUBITS = 2  # Reference qubits for SWAP test\n",
    "N_TRASH_QUBITS = 2     # Trash qubits for SWAP test\n",
    "\n",
    "# TRAINING CONFIGURATION\n",
    "TRAINING_CONFIG = {\n",
    "    'epochs_angle': 12,        # Standard angle embedding\n",
    "    'epochs_qvae': 15,         # Enhanced qVAE (needs more epochs)\n",
    "    'epochs_classical': 15,\n",
    "    'batch_size_angle': 20,    # Standard strategy\n",
    "    'batch_size_qvae': 8,      # Enhanced qVAE (memory intensive)\n",
    "    'batch_size_classical' : 20,\n",
    "    'learning_rate': 0.05      # Adam optimizer stepsize\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"QUANTUM AUTOENCODER - ANGLE vs ENHANCED qVAE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Enhanced qVAE Configuration:\")\n",
    "print(f\"  - Data Re-uploading: {USE_DATA_REUPLOADING}\")\n",
    "print(f\"  - Parallel Embedding: {USE_PARALLEL_EMBEDDING}x (8 data qubits)\")\n",
    "print(f\"  - Alternate RY/RX: {USE_ALTERNATE_EMBEDDING}\")\n",
    "print(f\"  - SWAP Test: {USE_SWAP_TEST}\")\n",
    "print(f\"  - Total qubits: 13 (8 data + 2 ref + 2 trash + 1 control)\")\n",
    "print(f\"\\nTraining Configuration: {TRAINING_CONFIG}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1b6f801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classical_batch_cost(samples, model):\n",
    "    \"\"\"\n",
    "    Classical AE용 배치 손실 계산 함수.\n",
    "    \n",
    "    Args:\n",
    "        samples: (배치) 원본 데이터, shape = (batch_size, n_features)\n",
    "        model: 학습된 MLPRegressor 오토인코더\n",
    "    \n",
    "    Returns:\n",
    "        linear_loss: 평균 절댓값 오차 (MAE)\n",
    "        squared_loss: 평균 제곱 오차 (MSE)\n",
    "    \"\"\"\n",
    "    recon = model.predict(samples)\n",
    "    errors = recon - samples\n",
    "    linear_loss   = np.mean(np.abs(errors))\n",
    "    squared_loss  = np.mean(errors**2)\n",
    "    return linear_loss, squared_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d3ce320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classical_ae_strategy():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TRAINING: CLASSICAL AUTOENCODER STRATEGY\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    total_epochs = 50\n",
    "    ae = MLPRegressor(\n",
    "        hidden_layer_sizes=(2,),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        learning_rate_init=TRAINING_CONFIG['learning_rate'],\n",
    "        max_iter=1,           # 한 에포크씩 학습\n",
    "        warm_start=True,\n",
    "        batch_size=TRAINING_CONFIG['batch_size_classical'],\n",
    "        random_state=42,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    linear_losses  = []\n",
    "    squared_losses = []\n",
    "\n",
    "    for epoch in range(1, total_epochs+1):\n",
    "        # 한 에포크 학습\n",
    "        ae.fit(X_train_4d, X_train_4d)\n",
    "\n",
    "        # 전체 학습 세트에 대한 loss 계산\n",
    "        lin_loss, sq_loss = compute_classical_batch_cost(X_train_4d, ae)\n",
    "        linear_losses.append(lin_loss)\n",
    "        squared_losses.append(sq_loss)\n",
    "\n",
    "        # 5 에포크마다 출력\n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print(f\"  Epoch {epoch:2d}/{total_epochs} — \"\n",
    "                  f\"Linear Loss: {lin_loss:.6f}, \"\n",
    "                  f\"Squared Loss: {sq_loss:.6f}\")\n",
    "\n",
    "    # 테스트 세트 평가\n",
    "    recon_test  = ae.predict(X_test_4d)\n",
    "    test_errors = np.mean((X_test_4d - recon_test)**2, axis=1)\n",
    "    threshold   = np.percentile(\n",
    "        np.mean((X_train_4d - ae.predict(X_train_4d))**2, axis=1),\n",
    "        TRAINING_CONFIG.get('threshold_percentile', 95)\n",
    "    )\n",
    "    y_pred = (test_errors > threshold).astype(int)\n",
    "\n",
    "    print(f\"\\n  - Threshold (95th percentile of train MSE): {threshold:.6f}\")\n",
    "    print(f\"  - Test set anomaly rate: {y_pred.mean():.4f}\")\n",
    "\n",
    "    return {\n",
    "        'strategy'       : 'classical_ae',\n",
    "        'model'          : ae,\n",
    "        'threshold'      : threshold,\n",
    "        'y_pred'         : y_pred,\n",
    "        'linear_losses'  : linear_losses,\n",
    "        'squared_losses' : squared_losses\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50deb8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING: CLASSICAL AUTOENCODER STRATEGY\n",
      "============================================================\n",
      "  Epoch  1/50 — Linear Loss: 0.772104, Squared Loss: 1.573793\n",
      "  Epoch  5/50 — Linear Loss: 0.606633, Squared Loss: 1.263942\n",
      "  Epoch 10/50 — Linear Loss: 0.571217, Squared Loss: 1.144388\n",
      "  Epoch 15/50 — Linear Loss: 0.589840, Squared Loss: 1.064400\n",
      "  Epoch 20/50 — Linear Loss: 0.610080, Squared Loss: 1.074977\n",
      "  Epoch 25/50 — Linear Loss: 0.587429, Squared Loss: 1.018109\n",
      "  Epoch 30/50 — Linear Loss: 0.566136, Squared Loss: 1.042183\n",
      "  Epoch 35/50 — Linear Loss: 0.563553, Squared Loss: 0.992692\n",
      "  Epoch 40/50 — Linear Loss: 0.589225, Squared Loss: 0.987001\n",
      "  Epoch 45/50 — Linear Loss: 0.568764, Squared Loss: 0.984173\n",
      "  Epoch 50/50 — Linear Loss: 0.609792, Squared Loss: 0.997257\n",
      "\n",
      "  - Threshold (95th percentile of train MSE): 3.625495\n",
      "  - Test set anomaly rate: 0.0263\n",
      "✓ classical strategy completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Train Angle strategy\n",
    "results = {}\n",
    "total_start_time = time.time()\n",
    "try:\n",
    "    classical_result = train_classical_ae_strategy()\n",
    "    results['classical'] = classical_result\n",
    "    print(f\"✓ classical strategy completed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ classical strategy failed: {str(e)}\")\n",
    "    results['classical'] = {'error': str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b06007",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qiskit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
