{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c16f265",
   "metadata": {},
   "source": [
    "# ğŸ”¬ ì¢…í•© ì‹¤í—˜ ì„¤ê³„: ì–‘ì ë° ë¹„ì–‘ì ì‚¬ê¸° íƒì§€ ì‹œìŠ¤í…œ ë¹„êµ ë¶„ì„\n",
    "\n",
    "**ì‹¤í—˜ ì„¤ê³„ ì§€ì¹¨ì„ ë°”íƒ•ìœ¼ë¡œ í•œ í¬ê´„ì ì¸ ì„±ëŠ¥ ë¹„êµ ì—°êµ¬**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š ì‹¤í—˜ ê°œìš”\n",
    "\n",
    "ë³¸ ì—°êµ¬ëŠ” **ì‹¤í—˜ ì„¤ê³„ ì§€ì¹¨**ì„ ì—„ê²©íˆ ë”°ë¼ ë‹¤ìŒ **8ê°€ì§€ ë¨¸ì‹ ëŸ¬ë‹ ë°©ë²•ë¡ **ì˜ ì‚¬ê¸° íƒì§€ ì„±ëŠ¥ì„ ê³µì •í•˜ê²Œ ë¹„êµí•©ë‹ˆë‹¤:\n",
    "\n",
    "### ğŸ›ï¸ Classical Methods (ë¹„ì–‘ì ë°©ë²• 4ê°€ì§€)\n",
    "1. **Random Forest**: íŠ¹ì„± ì¬êµ¬ì„± ê¸°ë°˜ ì´ìƒíƒì§€ *(more_qubits ê¸°ë°˜)*\n",
    "2. **IsolationForest**: ë¹„ì§€ë„ ì´ìƒíƒì§€ *(more_qubits ê¸°ë°˜)*\n",
    "3. **CNN Autoencoder**: 1D Convolutional Neural Network ê¸°ë°˜ ì¬êµ¬ì„± ì´ìƒíƒì§€ *(more_qubits ê¸°ë°˜)*\n",
    "4. **Classical Autoencoder**: ì „í†µì  ì‹ ê²½ë§ ì˜¤í† ì¸ì½”ë” *(more_qubits ê¸°ë°˜)*\n",
    "\n",
    "### âš›ï¸ Quantum Machine Learning Methods (ì–‘ì ë°©ë²• 4ê°€ì§€)\n",
    "5. **QAE Angle**: ê°ë„ ì„ë² ë”© ê¸°ë°˜ ì–‘ì ì˜¤í† ì¸ì½”ë” *(more_qubits ê¸°ë°˜)*\n",
    "6. **Enhanced qVAE**: ê³ ê¸‰ ì–‘ì ë³€ë¶„ ì˜¤í† ì¸ì½”ë” (ë°ì´í„° ì¬ì—…ë¡œë”©, SWAP í…ŒìŠ¤íŠ¸ í¬í•¨) *(more_qubits ê¸°ë°˜)*\n",
    "7. **DIFE QAE**: íŒŒê´´ì  ê°„ì„­ ì¶©ì‹¤ë„ ì¶”ì • ê¸°ë°˜ ì–‘ì ì˜¤í† ì¸ì½”ë” *(NewFile ê¸°ë°˜)*\n",
    "8. **LS-SWAP QAE**: ì ì¬ ê³µê°„ SWAP í…ŒìŠ¤íŠ¸ ê¸°ë°˜ ì–‘ì ì˜¤í† ì¸ì½”ë” *(NewFile ê¸°ë°˜)*\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ ì‹¤í—˜ ì„¤ê³„ ì›ì¹™\n",
    "\n",
    "### ğŸ“‹ í†µì œ ë³€ì¸ (Controlled Variables) - ëª¨ë“  ì‹¤í—˜ì—ì„œ ë™ì¼í•˜ê²Œ ìœ ì§€\n",
    "- **ë°ì´í„°ì…‹**: ë™ì¼í•œ preprocessed-creditcard.csv ì‚¬ìš©\n",
    "- **ë°ì´í„° ë¶„í• **: ë™ì¼í•œ train/test split (stratified)\n",
    "- **ë°ì´í„° ì „ì²˜ë¦¬**: ë™ì¼í•œ StandardScaler ì ìš©\n",
    "- **ì—í¬í¬ ë° ë°°ì¹˜ í¬ê¸°**: ëª¨ë“  ëª¨ë¸ì— ë™ì¼í•œ í›ˆë ¨ ì¡°ê±´\n",
    "- **í‰ê°€ ì§€í‘œ**: AUC-ROC, G-Mean, F1-Score, ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨\n",
    "- **ìµœì¢… í…ŒìŠ¤íŠ¸**: ë™ì¼í•œ unseen í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹\n",
    "\n",
    "### ğŸ”¬ ë…ë¦½ ë³€ì¸ (Independent Variables) - ë¹„êµ ëŒ€ìƒ\n",
    "- **ë°©ë²•ë¡ ë³„ ê³ ìœ  ì•„í‚¤í…ì²˜**: ê° ë°©ë²•ì˜ íŠ¹ì„±ì„ ì‚´ë¦° ìµœì  êµ¬ì¡°\n",
    "- **í•˜ì´í¼íŒŒë¼ë¯¸í„°**: ê° ë°©ë²•ì— ë§ëŠ” ìµœì í™”\n",
    "- **ì–‘ì íšŒë¡œ ì„¤ê³„**: SWAP, DIFE, LS-SWAP ë“± ê³ ìœ  ê¸°ë²•\n",
    "\n",
    "### ğŸ“Š ì¢…ì† ë³€ì¸ (Dependent Variables) - ì¸¡ì • ì§€í‘œ\n",
    "- **ëª¨ë¸ ì„±ëŠ¥**: AUC-ROC, G-Mean, F1-Score ë“±\n",
    "- **ìì› íš¨ìœ¨ì„±**: í•„ìš” íë¹„íŠ¸ ìˆ˜, íšŒë¡œ ê¹Šì´, í›ˆë ¨ ì‹œê°„\n",
    "- **í›ˆë ¨ ê°€ëŠ¥ì„±**: ê¸°ìš¸ê¸° ë¶„ì‚°, ìˆ˜ë ´ ì†ë„, ì¡ìŒ ë‚´ì„±\n",
    "\n",
    "### ğŸ”„ ë°˜ë³µ ì¸¡ì •\n",
    "- **í†µê³„ì  ì‹ ë¢°ì„±**: ê° ë°©ë²•ì„ ì—¬ëŸ¬ ë²ˆ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰\n",
    "- **í‰ê·  ë° í‘œì¤€í¸ì°¨**: ê²°ê³¼ì˜ í‰ê· ê³¼ ë¶„ì‚° ë³´ê³ \n",
    "- **ëœë¤ ì‹œë“œ ê´€ë¦¬**: ì¬í˜„ ê°€ëŠ¥í•œ ì‹¤í—˜ í™˜ê²½\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ› ï¸ ì‹¤í—˜ êµ¬ì„±\n",
    "\n",
    "### ë°ì´í„° ì „ì²˜ë¦¬ (í†µì¼ëœ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸)\n",
    "- âœ… **í‘œì¤€í™”**: ëª¨ë“  íŠ¹ì„±ì— StandardScaler ì ìš©\n",
    "- âœ… **ë°ì´í„° ì •ì œ**: ê²°ì¸¡ì¹˜ ë° ì¤‘ë³µ ì œê±°  \n",
    "- âœ… **ì°¨ì› ì¶•ì†Œ**: ì–‘ì ë°©ë²•ìš© PCA ë³€í™˜\n",
    "- âœ… **ë°ì´í„° ë¶„í• **: ê³„ì¸µí™” ë¶„í• ë¡œ í´ë˜ìŠ¤ ê· í˜• ìœ ì§€\n",
    "\n",
    "### í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\n",
    "- **ë¹„ì–‘ì ë°©ë²•**: ëœë¤ ì„œì¹˜ë¡œ í†µì¼ëœ ìµœì í™”\n",
    "- **ì–‘ì ë°©ë²•**: ê° ë°©ë²•ë³„ ì „ìš© íŒŒë¼ë¯¸í„° ì¡°í•©\n",
    "\n",
    "### í‰ê°€ ì „ëµ\n",
    "- **G-Mean ìµœì í™”**: ë¶ˆê· í˜• ë°ì´í„°ì— ìµœì í™”ëœ í‰ê°€ ì§€í‘œ\n",
    "- **ì„ê³„ê°’ ìµœì í™”**: ê° ë°©ë²•ë³„ ìµœì  ë¶„ë¥˜ ì„ê³„ê°’ íƒìƒ‰\n",
    "- **ì¢…í•© ì„±ëŠ¥ ë¹„êµ**: ë‹¤ì¤‘ ì§€í‘œ ê¸°ë°˜ ìˆœìœ„ ê²°ì •\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05255b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\my312\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”§ COMPREHENSIVE EXPERIMENTAL SYSTEM CONFIGURATION\n",
      "============================================================\n",
      "ğŸ“Š NumPy: 2.0.1\n",
      "ğŸ¼ Pandas: 2.2.3\n",
      "ğŸ¤– TensorFlow: 2.18.1\n",
      "âš›ï¸  PennyLane: 0.41.1\n",
      "ğŸ”¬ Scikit-learn: 1.6.1\n",
      "============================================================\n",
      "âœ… All libraries loaded successfully!\n",
      "ğŸ¯ Ready for comprehensive 8-method comparison\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# ğŸ“¦ Dependencies & Library Imports\n",
    "# ==========================================\n",
    "\n",
    "# Core Scientific Computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Scikit-learn: Classical ML & Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, ParameterSampler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, precision_score,\n",
    "    recall_score, f1_score, roc_auc_score, roc_curve\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, IsolationForest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Deep Learning: TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Quantum Machine Learning: PennyLane\n",
    "import pennylane as qml\n",
    "import pennylane.numpy as pnp\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "# Version Information\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ”§ COMPREHENSIVE EXPERIMENTAL SYSTEM CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ğŸ“Š NumPy: {np.__version__}\")\n",
    "print(f\"ğŸ¼ Pandas: {pd.__version__}\")\n",
    "print(f\"ğŸ¤– TensorFlow: {tf.__version__}\")\n",
    "print(f\"âš›ï¸  PennyLane: {qml.__version__}\")\n",
    "print(f\"ğŸ”¬ Scikit-learn: {getattr(__import__('sklearn'), '__version__', 'Unknown')}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"âœ… All libraries loaded successfully!\")\n",
    "print(\"ğŸ¯ Ready for comprehensive 8-method comparison\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9b36d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ ì‹¤í—˜ ì„¤ê³„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...\n",
      "================================================================================\n",
      "  COMPREHENSIVE EXPERIMENTAL SETUP SUMMARY\n",
      "================================================================================\n",
      " ì´ ë¹„êµ ë°©ë²•: 8ê°€ì§€ (ë¹„ì–‘ì 4ê°€ì§€ + ì–‘ì 4ê°€ì§€)\n",
      " í•˜ì´í¼íŒŒë¼ë¯¸í„° ê²€ìƒ‰: âŒ ë¹„í™œì„±í™”\n",
      " ê²€ìƒ‰ ë°˜ë³µ íšŸìˆ˜: 10\n",
      " í†µê³„ì  ë°˜ë³µ íšŸìˆ˜: 5\n",
      " PCA ì°¨ì› (ì–‘ììš©): 4\n",
      "\n",
      "ğŸ›ï¸  CLASSICAL METHODS (4ê°€ì§€):\n",
      " 1. Random Forest: 1 íŒŒë¼ë¯¸í„° ì¡°í•©\n",
      " 2. IsolationForest: 1 íŒŒë¼ë¯¸í„° ì¡°í•©\n",
      " 3. CNN Autoencoder: 1 íŒŒë¼ë¯¸í„° ì¡°í•©\n",
      " 4. Classical Autoencoder: 1 íŒŒë¼ë¯¸í„° ì¡°í•©\n",
      "\n",
      "âš›ï¸  QUANTUM METHODS (4ê°€ì§€):\n",
      " 5. QAE Angle: 5 ì „ìš© ì¡°í•© (4 qubits)\n",
      " 6. Enhanced qVAE: 5 ì „ìš© ì¡°í•© (13 qubits)\n",
      " 7. DIFE QAE: 5 ì „ìš© ì¡°í•© (4 qubits)\n",
      " 8. LS-SWAP QAE: 5 ì „ìš© ì¡°í•© (7 qubits)\n",
      "\n",
      "ğŸ”¬ ENHANCED qVAE FEATURES:\n",
      " â€¢ ë°ì´í„° ì¬ì—…ë¡œë”©: âœ…\n",
      " â€¢ ë³‘ë ¬ ì„ë² ë”©: âœ… (2x)\n",
      " â€¢ êµëŒ€ ì„ë² ë”©: âœ… (RY/RX)\n",
      " â€¢ SWAP í…ŒìŠ¤íŠ¸: âœ…\n",
      "\n",
      "â±ï¸  TRAINING EPOCHS:\n",
      " â€¢ QAE Angle: 100 epochs\n",
      " â€¢ Enhanced qVAE: 100 epochs\n",
      " â€¢ DIFE QAE: 100 epochs\n",
      " â€¢ LS-SWAP QAE: 100 epochs\n",
      "================================================================================\n",
      " âœ… ì¢…í•© ì‹¤í—˜ ì„¤ì • ì™„ë£Œ!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# ğŸ”§ COMPREHENSIVE EXPERIMENTAL CONFIGURATION\n",
    "# ==========================================\n",
    "\n",
    "print(\"ğŸ”§ ì‹¤í—˜ ì„¤ê³„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...\")\n",
    "\n",
    "# ==========================================\n",
    "# ğŸ“Š EXPERIMENTAL PARAMETERS (í†µì œ ë³€ì¸)\n",
    "# ==========================================\n",
    "\n",
    "# ê³µí†µ ì‹¤í—˜ ì„¤ì • (ëª¨ë“  ë°©ë²•ì— ë™ì¼ ì ìš©)\n",
    "EXPERIMENTAL_CONFIG = {\n",
    "    'test_size': 0.2,           # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨ (í†µì œ)\n",
    "    'random_state': 42,         # ì¬í˜„ì„±ì„ ìœ„í•œ ê¸°ë³¸ ëœë¤ ì‹œë“œ (í†µì œ)\n",
    "    'validation_split': 0.2,    # ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ (í†µì œ)\n",
    "    'pca_dimensions': 4,        # ì–‘ì ë°©ë²•ìš© PCA ì°¨ì› (í†µì œ)\n",
    "    'max_epochs': 100,          # ìµœëŒ€ ì—í¬í¬ ìˆ˜ (í†µì œ)\n",
    "    'batch_size': 16,           # ëª¨ë“  ë°©ë²•ì—ì„œ ê³ ì •ëœ ë°°ì¹˜ í¬ê¸° (í†µì œ)\n",
    "    'quantum_layers': 4,        # ëª¨ë“  ì–‘ì ë°©ë²•ì—ì„œ ê³ ì •ëœ ë ˆì´ì–´ ìˆ˜ (í†µì œ)\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# ğŸ”„ ë°˜ë³µ ì¸¡ì • ë° í†µê³„ì  ë¶„ì„ ì„¤ì • (ì‹¤í—˜ì„¤ê³„.txt êµ¬í˜„)\n",
    "# ==========================================\n",
    "\n",
    "# ë°˜ë³µ ì¸¡ì •ì„ ìœ„í•œ ì„¤ì • (ì‹¤í—˜ì„¤ê³„.txt ì œì•ˆ êµ¬í˜„)\n",
    "STATISTICAL_CONFIG = {\n",
    "    'n_repetitions': 5,              # ê° ë°©ë²•ë³„ ë°˜ë³µ ì‹¤í–‰ íšŸìˆ˜ (ì‹ ë¢°ë„ í–¥ìƒ)\n",
    "    'confidence_level': 0.95,        # ì‹ ë¢°ë„ ìˆ˜ì¤€\n",
    "    'report_std': True,              # í‘œì¤€í¸ì°¨ ë³´ê³  ì—¬ë¶€\n",
    "    'random_seed_start': 42,         # ì‹œì‘ ì‹œë“œ (ê° ë°˜ë³µë§ˆë‹¤ ë‹¤ë¥¸ ì‹œë“œ ì‚¬ìš©)\n",
    "    'enable_repetition': True,       # ë°˜ë³µ ì‹¤í—˜ í™œì„±í™”/ë¹„í™œì„±í™”\n",
    "    'save_individual_results': True, # ê°œë³„ ì‹¤í–‰ ê²°ê³¼ ì €ì¥ ì—¬ë¶€\n",
    "}\n",
    "\n",
    "# ë‚œìˆ˜ ì‹œë“œ ê´€ë¦¬ í•¨ìˆ˜ë“¤\n",
    "def get_experiment_seeds(n_repetitions, base_seed=42):\n",
    "    \"\"\"\n",
    "    ì‹¤í—˜ì„¤ê³„.txtì— ë”°ë¥¸ ë‚œìˆ˜ ì‹œë“œ ìƒì„±\n",
    "    \n",
    "    ê° ë°˜ë³µ ì‹¤í—˜ì´ ë…ë¦½ì ìœ¼ë¡œ ì´ë£¨ì–´ì§€ë„ë¡ ë§¤ ì‹¤í–‰ë§ˆë‹¤ ë‹¤ë¥¸ ì‹œë“œë¥¼ ìƒì„±\n",
    "    \n",
    "    Args:\n",
    "        n_repetitions: ë°˜ë³µ íšŸìˆ˜\n",
    "        base_seed: ê¸°ë³¸ ì‹œë“œ\n",
    "        \n",
    "    Returns:\n",
    "        list: ê° ë°˜ë³µ ì‹¤í—˜ìš© ì‹œë“œ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    np.random.seed(base_seed)\n",
    "    seeds = np.random.randint(1, 10000, size=n_repetitions).tolist()\n",
    "    return seeds\n",
    "\n",
    "def set_experiment_seed(seed):\n",
    "    \"\"\"\n",
    "    ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ëŒ€í•´ ì¼ê´€ëœ ì‹œë“œ ì„¤ì •\n",
    "    \n",
    "    Args:\n",
    "        seed: ì„¤ì •í•  ì‹œë“œ ê°’\n",
    "    \"\"\"\n",
    "    import random\n",
    "    \n",
    "    # Python ê¸°ë³¸ random\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # TensorFlow (GPU ì‹œë“œë„ í¬í•¨)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "    # PennyLane (ë‚´ë¶€ì ìœ¼ë¡œ NumPy ì‚¬ìš©)\n",
    "    pnp.random.seed(seed)\n",
    "    \n",
    "    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ì¬í˜„ì„±ì„ ìœ„í•œ ì¶”ê°€ ì„¤ì •)\n",
    "    import os\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "def compute_statistical_summary_advanced(results_list, method_name):\n",
    "    \"\"\"\n",
    "    ì‹¤í—˜ì„¤ê³„.txtì— ë”°ë¥¸ ê³ ê¸‰ í†µê³„ì  ìš”ì•½ ê³„ì‚°\n",
    "    \n",
    "    ë°˜ë³µ ì‹¤í—˜ ê²°ê³¼ì˜ í‰ê· , í‘œì¤€í¸ì°¨, ì‹ ë¢°êµ¬ê°„ì„ ê³„ì‚°í•˜ê³ \n",
    "    í†µê³„ì  ìœ ì˜ì„±ì„ ë¶„ì„\n",
    "    \n",
    "    Args:\n",
    "        results_list: ê° ë°˜ë³µ ì‹¤í–‰ì˜ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸\n",
    "        method_name: ë°©ë²•ë¡  ì´ë¦„\n",
    "        \n",
    "    Returns:\n",
    "        dict: í¬ê´„ì ì¸ í†µê³„ ìš”ì•½\n",
    "    \"\"\"\n",
    "    if not results_list:\n",
    "        return None\n",
    "    \n",
    "    # ê° ì§€í‘œë³„ ê°’ë“¤ ìˆ˜ì§‘\n",
    "    metrics = ['auc', 'accuracy', 'precision', 'recall', 'f1_score', 'gmean', 'specificity']\n",
    "    summary = {\n",
    "        'method': method_name,\n",
    "        'n_repetitions': len(results_list),\n",
    "        'individual_results': results_list  # ê°œë³„ ê²°ê³¼ ì €ì¥\n",
    "    }\n",
    "    \n",
    "    for metric in metrics:\n",
    "        values = [result[metric] for result in results_list if metric in result]\n",
    "        if values:\n",
    "            mean_val = np.mean(values)\n",
    "            std_val = np.std(values, ddof=1)  # í‘œë³¸ í‘œì¤€í¸ì°¨ (N-1)\n",
    "            \n",
    "            summary[metric] = {\n",
    "                'mean': mean_val,\n",
    "                'std': std_val,\n",
    "                'min': np.min(values),\n",
    "                'max': np.max(values),\n",
    "                'median': np.median(values),\n",
    "                'n': len(values),\n",
    "                'cv': std_val / mean_val if mean_val != 0 else np.inf,  # ë³€ë™ê³„ìˆ˜\n",
    "                'values': values  # ì›ë³¸ ê°’ë“¤ ì €ì¥\n",
    "            }\n",
    "            \n",
    "            # 95% ì‹ ë¢°êµ¬ê°„ ê³„ì‚° (t-ë¶„í¬ ì‚¬ìš©)\n",
    "            from scipy import stats\n",
    "            confidence_level = STATISTICAL_CONFIG['confidence_level']\n",
    "            alpha = 1 - confidence_level\n",
    "            degrees_freedom = len(values) - 1\n",
    "            \n",
    "            if degrees_freedom > 0:\n",
    "                t_critical = stats.t.ppf(1 - alpha/2, degrees_freedom)\n",
    "                margin_error = t_critical * (std_val / np.sqrt(len(values)))\n",
    "                \n",
    "                summary[metric]['ci_lower'] = mean_val - margin_error\n",
    "                summary[metric]['ci_upper'] = mean_val + margin_error\n",
    "                summary[metric]['margin_error'] = margin_error\n",
    "            else:\n",
    "                summary[metric]['ci_lower'] = mean_val\n",
    "                summary[metric]['ci_upper'] = mean_val\n",
    "                summary[metric]['margin_error'] = 0\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def print_statistical_results(summary):\n",
    "    \"\"\"\n",
    "    í†µê³„ì  ìš”ì•½ ê²°ê³¼ë¥¼ ì‹¤í—˜ì„¤ê³„.txt í˜•ì‹ìœ¼ë¡œ ì¶œë ¥\n",
    "    \n",
    "    í‰ê·  Â± í‘œì¤€í¸ì°¨ í˜•íƒœë¡œ ê²°ê³¼ ë³´ê³ \n",
    "    \n",
    "    Args:\n",
    "        summary: compute_statistical_summary_advanced()ì˜ ê²°ê³¼\n",
    "    \"\"\"\n",
    "    if not summary:\n",
    "        print(\"í†µê³„ ìš”ì•½ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"\\nğŸ“Š {summary['method']} í†µê³„ì  ìš”ì•½ ({summary['n_repetitions']}íšŒ ë°˜ë³µ)\")\n",
    "    print(\"â”€\" * 80)\n",
    "    \n",
    "    # ì£¼ìš” ì§€í‘œë“¤ì„ í‰ê·  Â± í‘œì¤€í¸ì°¨ í˜•íƒœë¡œ ì¶œë ¥\n",
    "    main_metrics = ['auc', 'gmean', 'f1_score', 'accuracy', 'precision', 'recall']\n",
    "    \n",
    "    for metric in main_metrics:\n",
    "        if metric in summary:\n",
    "            data = summary[metric]\n",
    "            mean_val = data['mean']\n",
    "            std_val = data['std']\n",
    "            ci_lower = data.get('ci_lower', mean_val)\n",
    "            ci_upper = data.get('ci_upper', mean_val)\n",
    "            cv = data['cv']\n",
    "            \n",
    "            # ì§€í‘œëª… í•œêµ­ì–´ ë³€í™˜\n",
    "            metric_names = {\n",
    "                'auc': 'ğŸ¯ AUC-ROC',\n",
    "                'gmean': 'ğŸ“ G-Mean',\n",
    "                'f1_score': 'ğŸ† F1-Score',\n",
    "                'accuracy': 'âš–ï¸  ì •í™•ë„',\n",
    "                'precision': 'ğŸª ì •ë°€ë„',\n",
    "                'recall': 'ğŸ” ì¬í˜„ìœ¨'\n",
    "            }\n",
    "            \n",
    "            display_name = metric_names.get(metric, metric)\n",
    "            \n",
    "            print(f\"  {display_name}: {mean_val:.4f} Â± {std_val:.4f}\")\n",
    "            print(f\"    â””â”€ 95% ì‹ ë¢°êµ¬ê°„: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "            print(f\"    â””â”€ ë³€ë™ê³„ìˆ˜: {cv:.4f} ({'ë‚®ìŒ' if cv < 0.1 else 'ë³´í†µ' if cv < 0.2 else 'ë†’ìŒ'})\")\n",
    "    \n",
    "    print(\"â”€\" * 80)\n",
    "    \n",
    "    # ë³€ë™ì„± ë¶„ì„\n",
    "    gmean_cv = summary.get('gmean', {}).get('cv', 0)\n",
    "    if gmean_cv < 0.05:\n",
    "        stability = \"ë§¤ìš° ì•ˆì •ì \"\n",
    "    elif gmean_cv < 0.1:\n",
    "        stability = \"ì•ˆì •ì \"\n",
    "    elif gmean_cv < 0.2:\n",
    "        stability = \"ë³´í†µ\"\n",
    "    else:\n",
    "        stability = \"ë¶ˆì•ˆì •\"\n",
    "    \n",
    "    print(f\"ğŸ“ˆ ì„±ëŠ¥ ì•ˆì •ì„±: {stability} (G-Mean ë³€ë™ê³„ìˆ˜: {gmean_cv:.4f})\")\n",
    "    \n",
    "    # ìµœê³ /ìµœì € ì„±ëŠ¥\n",
    "    if 'gmean' in summary:\n",
    "        gmean_data = summary['gmean']\n",
    "        print(f\"ğŸ“Š G-Mean ë²”ìœ„: {gmean_data['min']:.4f} ~ {gmean_data['max']:.4f}\")\n",
    "        print(f\"ğŸ“Š G-Mean ì¤‘ì•™ê°’: {gmean_data['median']:.4f}\")\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ê²€ìƒ‰ ì„¤ì • (í†µì œ)\n",
    "HYPERPARAMETER_SEARCH = {\n",
    "    'enable_search': False,           # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í™œì„±í™”\n",
    "    'search_iterations': 10,         # ëœë¤ ì„œì¹˜ ë°˜ë³µ íšŸìˆ˜ (ë¹„ì–‘ì ë°©ë²•)\n",
    "    'validation_split': 0.2          # ê²€ì¦ ë°ì´í„° ë¹„ìœ¨\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# ğŸ›ï¸ DEFAULT HYPERPARAMETERS (ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì¡°í•©)\n",
    "# ==========================================\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ ë¹„í™œì„±í™”ëœ ê²½ìš° ì‚¬ìš©í•  ê¸°ë³¸ íŒŒë¼ë¯¸í„°\n",
    "DEFAULT_HYPERPARAMETERS = {\n",
    "    # ğŸ›ï¸ Classical Methods ê¸°ë³¸ íŒŒë¼ë¯¸í„°\n",
    "    'random_forest': {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 20,\n",
    "        'min_samples_split': 5,\n",
    "        'min_samples_leaf': 2\n",
    "    },\n",
    "    \n",
    "    'isolation_forest': {\n",
    "        'n_estimators': 100,\n",
    "        'contamination': 0.1,\n",
    "        'max_samples': 'auto'\n",
    "    },\n",
    "    \n",
    "    'cnn_autoencoder': {\n",
    "        'learning_rate': 0.001,\n",
    "        'filters1': 32,\n",
    "        'filters2': 64,\n",
    "        'filters3': 32,\n",
    "        'dropout_conv1': 0.2,\n",
    "        'dropout_conv2': 0.2,\n",
    "        'dense_units1': 128,\n",
    "        'dense_units2': 64,\n",
    "        'dropout_dense1': 0.3,\n",
    "        'dropout_dense2': 0.2\n",
    "    },\n",
    "    \n",
    "    'classical_autoencoder': {\n",
    "        'learning_rate': 0.001,\n",
    "        'encoding_dim': 16,\n",
    "        'hidden_layers': 2,\n",
    "        'dropout_rate': 0.2\n",
    "    },\n",
    "    \n",
    "    # âš›ï¸ Quantum Methods ê¸°ë³¸ íŒŒë¼ë¯¸í„°\n",
    "    'qae_angle': {\n",
    "        'learning_rate': 0.001,\n",
    "        'layers': 4,  # ê³ ì •\n",
    "        'batch_size': 16  # ê³ ì •\n",
    "    },\n",
    "    \n",
    "    'enhanced_qvae': {\n",
    "        'learning_rate': 0.001,\n",
    "        'layers': 4,  # ê³ ì •\n",
    "        'batch_size': 16  # ê³ ì •\n",
    "    },\n",
    "    \n",
    "    'dife_qae': {\n",
    "        'learning_rate': 0.001,\n",
    "        'layers': 4,  # ê³ ì •\n",
    "        'batch_size': 16  # ê³ ì •\n",
    "    },\n",
    "    \n",
    "    'ls_swap_qae': {\n",
    "        'learning_rate': 0.001,\n",
    "        'layers': 4,  # ê³ ì •\n",
    "        'batch_size': 16  # ê³ ì •\n",
    "    }\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# ğŸ›ï¸ CLASSICAL METHODS CONFIGURATION\n",
    "# ==========================================\n",
    "\n",
    "# Random Forest Parameters (Method 1)\n",
    "RF_PARAM_GRID = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# IsolationForest Parameters (Method 2)\n",
    "IF_PARAM_GRID = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'contamination': [0.05, 0.1, 0.15, 0.2],\n",
    "    'max_samples': ['auto', 0.5, 0.8]\n",
    "}\n",
    "\n",
    "# CNN Autoencoder Parameters (Method 3)\n",
    "CNN_PARAM_GRID = {\n",
    "    'learning_rate': [0.0005, 0.001, 0.002, 0.005],\n",
    "    'filters1': [32, 48, 64],\n",
    "    'filters2': [64, 96, 128],\n",
    "    'filters3': [32, 48, 64],\n",
    "    'dropout_conv1': [0.1, 0.15, 0.2, 0.25],\n",
    "    'dropout_conv2': [0.1, 0.15, 0.2, 0.25],\n",
    "    'dense_units1': [128, 192, 256],\n",
    "    'dense_units2': [64, 96, 128],\n",
    "    'dropout_dense1': [0.3, 0.35, 0.4],\n",
    "    'dropout_dense2': [0.2, 0.25, 0.3]\n",
    "}\n",
    "\n",
    "# Classical Autoencoder Parameters (Method 4)\n",
    "AE_PARAM_GRID = {\n",
    "    'learning_rate': [0.001, 0.005, 0.01],\n",
    "    'encoding_dim': [8, 16, 32],\n",
    "    'hidden_layers': [1, 2, 3],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# âš›ï¸  QUANTUM METHODS CONFIGURATION\n",
    "# ==========================================\n",
    "\n",
    "# QAE Angle ì „ìš© íŒŒë¼ë¯¸í„° ì¡°í•© (Method 5 - from more_qubits)\n",
    "QAE_ANGLE_HYPERPARAMETER_COMBINATIONS = [\n",
    "    {'learning_rate': 0.01, 'layers': 4, 'batch_size': 16},   # ì¤‘ê°„ í•™ìŠµë¥ \n",
    "    {'learning_rate': 0.005, 'layers': 4, 'batch_size': 16},  # ë‚®ì€ í•™ìŠµë¥ \n",
    "    {'learning_rate': 0.02, 'layers': 4, 'batch_size': 16},   # ë†’ì€ í•™ìŠµë¥ \n",
    "    {'learning_rate': 0.001, 'layers': 4, 'batch_size': 16},  # ë§¤ìš° ë‚®ì€ í•™ìŠµë¥ \n",
    "    {'learning_rate': 0.015, 'layers': 4, 'batch_size': 16}   # ê¸°ë³¸ ì„¤ì •\n",
    "]\n",
    "\n",
    "# Enhanced qVAE ì „ìš© íŒŒë¼ë¯¸í„° ì¡°í•© (Method 6 - from more_qubits)\n",
    "ENHANCED_QVAE_HYPERPARAMETER_COMBINATIONS = [\n",
    "    {'learning_rate': 0.001, 'layers': 4, 'batch_size': 16},  # ë‚®ì€ í•™ìŠµë¥ \n",
    "    {'learning_rate': 0.002, 'layers': 4, 'batch_size': 16},  # ì¤‘ê°„ í•™ìŠµë¥ \n",
    "    {'learning_rate': 0.0005, 'layers': 4, 'batch_size': 16}, # ë§¤ìš° ë‚®ì€ í•™ìŠµë¥ \n",
    "    {'learning_rate': 0.003, 'layers': 4, 'batch_size': 16},  # ë†’ì€ í•™ìŠµë¥ \n",
    "    {'learning_rate': 0.0015, 'layers': 4, 'batch_size': 16}  # ê· í˜• ì¡íŒ ì„¤ì •\n",
    "]\n",
    "\n",
    "# DIFE QAE ì „ìš© íŒŒë¼ë¯¸í„° ì¡°í•© (Method 7 - from NewFile)\n",
    "DIFE_HYPERPARAMETER_COMBINATIONS = [\n",
    "    {'learning_rate': 0.001, 'layers': 4, 'batch_size': 16},  # ë‚®ì€ í•™ìŠµë¥ \n",
    "    {'learning_rate': 0.002, 'layers': 4, 'batch_size': 16},  # ì¤‘ê°„ í•™ìŠµë¥ \n",
    "    {'learning_rate': 0.0005, 'layers': 4, 'batch_size': 16}, # ë§¤ìš° ë‚®ì€ í•™ìŠµë¥ \n",
    "    {'learning_rate': 0.003, 'layers': 4, 'batch_size': 16},  # ìƒëŒ€ì ìœ¼ë¡œ ë†’ì€ í•™ìŠµë¥ \n",
    "    {'learning_rate': 0.0015, 'layers': 4, 'batch_size': 16}  # ê· í˜• ì„¤ì •\n",
    "]\n",
    "\n",
    "# LS-SWAP QAE ì „ìš© íŒŒë¼ë¯¸í„° ì¡°í•© (Method 8 - from NewFile)\n",
    "LS_SWAP_HYPERPARAMETER_COMBINATIONS = [\n",
    "    {'learning_rate': 0.001, 'layers': 4, 'batch_size': 16},  # ë‚®ì€ í•™ìŠµë¥ \n",
    "    {'learning_rate': 0.002, 'layers': 4, 'batch_size': 16},  # ì¤‘ê°„ í•™ìŠµë¥ \n",
    "    {'learning_rate': 0.0005, 'layers': 4, 'batch_size': 16}, # ë§¤ìš° ë‚®ì€ í•™ìŠµë¥ \n",
    "    {'learning_rate': 0.003, 'layers': 4, 'batch_size': 16},  # ìƒëŒ€ì ìœ¼ë¡œ ë†’ì€ í•™ìŠµë¥ \n",
    "    {'learning_rate': 0.0015, 'layers': 4, 'batch_size': 16}  # ê· í˜• ì„¤ì •\n",
    "]\n",
    "\n",
    "# Enhanced qVAE ê³ ê¸‰ ê¸°ëŠ¥ ì„¤ì • (from more_qubits)\n",
    "USE_DATA_REUPLOADING = True        # ê° ë³€ë¶„ ë ˆì´ì–´ì—ì„œ ë°ì´í„° ì¬ì„ë² ë”©\n",
    "USE_PARALLEL_EMBEDDING = 2         # ë°ì´í„° ë³‘ë ¬ ë³µì œ (2x = 2n data qubits)\n",
    "USE_ALTERNATE_EMBEDDING = True     # RYì™€ RX íšŒì „ êµëŒ€ ì‚¬ìš©\n",
    "USE_SWAP_TEST = True              # ì–‘ì SWAP í…ŒìŠ¤íŠ¸ë¡œ ì •í™•í•œ ì¶©ì‹¤ë„ ì¸¡ì •\n",
    "\n",
    "# ì–‘ì íšŒë¡œ ì•„í‚¤í…ì²˜ ë§¤ê°œë³€ìˆ˜\n",
    "N_REFERENCE_QUBITS = 2             # SWAP í…ŒìŠ¤íŠ¸ìš© ì°¸ì¡° íë¹„íŠ¸  \n",
    "N_TRASH_QUBITS = 2                 # SWAP í…ŒìŠ¤íŠ¸ìš© íŠ¸ë˜ì‹œ íë¹„íŠ¸\n",
    "QAE_QUBITS = EXPERIMENTAL_CONFIG['pca_dimensions']        # í‘œì¤€ QAE íë¹„íŠ¸ ìˆ˜\n",
    "QVAE_DATA_QUBITS = QAE_QUBITS * USE_PARALLEL_EMBEDDING   # Enhanced qVAE ë°ì´í„° íë¹„íŠ¸\n",
    "QVAE_TOTAL_QUBITS = QVAE_DATA_QUBITS + N_REFERENCE_QUBITS + N_TRASH_QUBITS + 1  # ì´ íë¹„íŠ¸\n",
    "\n",
    "# DIFE ë° LS-SWAP ì„¤ì • (from NewFile)\n",
    "DIFE_TOTAL_QUBITS = QAE_QUBITS                    # DIFEëŠ” ancilla-free (4 qubits)\n",
    "LS_SWAP_LATENT_QUBITS = 2                         # LS-SWAP ì ì¬ ê³µê°„ íë¹„íŠ¸\n",
    "LS_SWAP_TOTAL_QUBITS = QAE_QUBITS + N_REFERENCE_QUBITS + 1  # LS-SWAP ì´ íë¹„íŠ¸ (7 qubits)\n",
    "\n",
    "# ì–‘ì í›ˆë ¨ ì„¤ì •\n",
    "QUANTUM_TRAINING_CONFIG = {\n",
    "    'epochs_qae_angle': 100,         # QAE Angle ì—í¬í¬\n",
    "    'epochs_enhanced_qvae': 100,     # Enhanced qVAE ì—í¬í¬\n",
    "    'epochs_dife': 100,               # DIFE ì—í¬í¬ (ë” ë¹ ë¥¸ ìˆ˜ë ´)\n",
    "    'epochs_ls_swap': 100,            # LS-SWAP ì—í¬í¬ (ë” ë¹ ë¥¸ ìˆ˜ë ´)\n",
    "    'validation_epochs': 15,         # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê²€ì¦ìš© ì—í¬í¬\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# ì‹¤í—˜ ê²°ê³¼ ì €ì¥ì†Œ ì´ˆê¸°í™” (ì‹¤í—˜ì„¤ê³„.txt êµ¬í˜„)\n",
    "# ==========================================\n",
    "\n",
    "# ë°˜ë³µ ì‹¤í—˜ ê²°ê³¼ë¥¼ ì €ì¥í•  ì „ì—­ ì €ì¥ì†Œ\n",
    "experiment_results_store = {\n",
    "    'random_forest': [],\n",
    "    'isolation_forest': [], \n",
    "    'cnn_autoencoder': [],\n",
    "    'classical_autoencoder': [],\n",
    "    'qae_angle': [],\n",
    "    'enhanced_qvae': [],\n",
    "    'dife_qae': [],\n",
    "    'ls_swap_qae': []\n",
    "}\n",
    "\n",
    "# ì‹¤í—˜ ì‹œë“œ ë¯¸ë¦¬ ìƒì„± (ì¬í˜„ì„± ë³´ì¥)\n",
    "experiment_seeds = get_experiment_seeds(\n",
    "    STATISTICAL_CONFIG['n_repetitions'], \n",
    "    STATISTICAL_CONFIG['random_seed_start']\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# ğŸ“‹ ì‹¤í—˜ ì„¤ì • ìš”ì•½ ì¶œë ¥\n",
    "# ==========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"  COMPREHENSIVE EXPERIMENTAL SETUP SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\" ì´ ë¹„êµ ë°©ë²•: 8ê°€ì§€ (ë¹„ì–‘ì 4ê°€ì§€ + ì–‘ì 4ê°€ì§€)\")\n",
    "print(f\" ê³ ì • ë°°ì¹˜ í¬ê¸°: {EXPERIMENTAL_CONFIG['batch_size']} (ëª¨ë“  ë°©ë²• í†µì¼)\")\n",
    "print(f\" ê³ ì • ì–‘ì ë ˆì´ì–´: {EXPERIMENTAL_CONFIG['quantum_layers']}ê°œ (ëª¨ë“  ì–‘ì ë°©ë²• í†µì¼)\")\n",
    "print(f\" í•˜ì´í¼íŒŒë¼ë¯¸í„° ê²€ìƒ‰: {'âœ… í™œì„±í™”' if HYPERPARAMETER_SEARCH['enable_search'] else 'âŒ ë¹„í™œì„±í™” (ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©)'}\")\n",
    "\n",
    "# ğŸ”„ ë°˜ë³µ ì‹¤í—˜ ì„¤ì • ì¶œë ¥ (ì‹¤í—˜ì„¤ê³„.txt êµ¬í˜„)\n",
    "print(f\"\\nğŸ”„ ë°˜ë³µ ì‹¤í—˜ ì„¤ì • (ì‹¤í—˜ì„¤ê³„.txt êµ¬í˜„):\")\n",
    "print(f\" â€¢ ë°˜ë³µ ì‹¤í—˜: {'âœ… í™œì„±í™”' if STATISTICAL_CONFIG['enable_repetition'] else 'âŒ ë¹„í™œì„±í™”'}\")\n",
    "if STATISTICAL_CONFIG['enable_repetition']:\n",
    "    print(f\" â€¢ ë°˜ë³µ íšŸìˆ˜: {STATISTICAL_CONFIG['n_repetitions']}íšŒ (ê° ë°©ë²•ë³„)\")\n",
    "    print(f\" â€¢ ì‹ ë¢°ë„ ìˆ˜ì¤€: {STATISTICAL_CONFIG['confidence_level']*100}%\")\n",
    "    print(f\" â€¢ ì‹œë“œ ê´€ë¦¬: {len(experiment_seeds)}ê°œ ë…ë¦½ ì‹œë“œ ìƒì„±\")\n",
    "    print(f\" â€¢ ì‹œë“œ ëª©ë¡: {experiment_seeds}\")\n",
    "    print(f\" â€¢ í†µê³„ ë³´ê³ : í‰ê·  Â± í‘œì¤€í¸ì°¨ í˜•ì‹\")\n",
    "    print(f\" â€¢ ì‹ ë¢°êµ¬ê°„: t-ë¶„í¬ ê¸°ë°˜ 95% êµ¬ê°„ ê³„ì‚°\")\n",
    "else:\n",
    "    print(f\" â€¢ ë‹¨ì¼ ì‹¤í–‰ ëª¨ë“œ: ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©\")\n",
    "\n",
    "if HYPERPARAMETER_SEARCH['enable_search']:\n",
    "    print(f\" ê²€ìƒ‰ ë°˜ë³µ íšŸìˆ˜: {HYPERPARAMETER_SEARCH['search_iterations']}\")\n",
    "else:\n",
    "    print(f\" ê¸°ë³¸ íŒŒë¼ë¯¸í„° ëª¨ë“œ: ë¹ ë¥¸ ì‹¤í–‰ì„ ìœ„í•œ ê²€ì¦ëœ íŒŒë¼ë¯¸í„° ì¡°í•© ì‚¬ìš©\")\n",
    "\n",
    "print(f\" PCA ì°¨ì› (ì–‘ììš©): {EXPERIMENTAL_CONFIG['pca_dimensions']}\")\n",
    "print()\n",
    "\n",
    "if HYPERPARAMETER_SEARCH['enable_search']:\n",
    "    print(\"ğŸ›ï¸  CLASSICAL METHODS (4ê°€ì§€) - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹:\")\n",
    "    print(f\" 1. Random Forest: {len(list(ParameterSampler(RF_PARAM_GRID, n_iter=1)))} íŒŒë¼ë¯¸í„° ì¡°í•©\")\n",
    "    print(f\" 2. IsolationForest: {len(list(ParameterSampler(IF_PARAM_GRID, n_iter=1)))} íŒŒë¼ë¯¸í„° ì¡°í•©\")\n",
    "    print(f\" 3. CNN Autoencoder: {len(list(ParameterSampler(CNN_PARAM_GRID, n_iter=1)))} íŒŒë¼ë¯¸í„° ì¡°í•©\")\n",
    "    print(f\" 4. Classical Autoencoder: {len(list(ParameterSampler(AE_PARAM_GRID, n_iter=1)))} íŒŒë¼ë¯¸í„° ì¡°í•©\")\n",
    "    print()\n",
    "    print(\"âš›ï¸  QUANTUM METHODS (4ê°€ì§€, ëª¨ë‘ 4ë ˆì´ì–´ + ë°°ì¹˜16) - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹:\")\n",
    "    print(f\" 5. QAE Angle: {len(QAE_ANGLE_HYPERPARAMETER_COMBINATIONS)} ì „ìš© ì¡°í•© ({QAE_QUBITS} qubits)\")\n",
    "    print(f\" 6. Enhanced qVAE: {len(ENHANCED_QVAE_HYPERPARAMETER_COMBINATIONS)} ì „ìš© ì¡°í•© ({QVAE_TOTAL_QUBITS} qubits)\")\n",
    "    print(f\" 7. DIFE QAE: {len(DIFE_HYPERPARAMETER_COMBINATIONS)} ì „ìš© ì¡°í•© ({DIFE_TOTAL_QUBITS} qubits)\")\n",
    "    print(f\" 8. LS-SWAP QAE: {len(LS_SWAP_HYPERPARAMETER_COMBINATIONS)} ì „ìš© ì¡°í•© ({LS_SWAP_TOTAL_QUBITS} qubits)\")\n",
    "else:\n",
    "    print(\"ğŸ›ï¸  CLASSICAL METHODS (4ê°€ì§€) - ê¸°ë³¸ íŒŒë¼ë¯¸í„°:\")\n",
    "    print(f\" 1. Random Forest: {DEFAULT_HYPERPARAMETERS['random_forest']}\")\n",
    "    print(f\" 2. IsolationForest: {DEFAULT_HYPERPARAMETERS['isolation_forest']}\")\n",
    "    print(f\" 3. CNN Autoencoder: Learning Rate = {DEFAULT_HYPERPARAMETERS['cnn_autoencoder']['learning_rate']}\")\n",
    "    print(f\" 4. Classical Autoencoder: Learning Rate = {DEFAULT_HYPERPARAMETERS['classical_autoencoder']['learning_rate']}\")\n",
    "    print()\n",
    "    print(\"âš›ï¸  QUANTUM METHODS (4ê°€ì§€, ëª¨ë‘ 4ë ˆì´ì–´ + ë°°ì¹˜16) - ê¸°ë³¸ íŒŒë¼ë¯¸í„°:\")\n",
    "    print(f\" 5. QAE Angle: LR={DEFAULT_HYPERPARAMETERS['qae_angle']['learning_rate']} ({QAE_QUBITS} qubits)\")\n",
    "    print(f\" 6. Enhanced qVAE: LR={DEFAULT_HYPERPARAMETERS['enhanced_qvae']['learning_rate']} ({QVAE_TOTAL_QUBITS} qubits)\")\n",
    "    print(f\" 7. DIFE QAE: LR={DEFAULT_HYPERPARAMETERS['dife_qae']['learning_rate']} ({DIFE_TOTAL_QUBITS} qubits)\")\n",
    "    print(f\" 8. LS-SWAP QAE: LR={DEFAULT_HYPERPARAMETERS['ls_swap_qae']['learning_rate']} ({LS_SWAP_TOTAL_QUBITS} qubits)\")\n",
    "\n",
    "print()\n",
    "print(\"ğŸ”¬ ENHANCED qVAE FEATURES:\")\n",
    "print(f\" â€¢ ë°ì´í„° ì¬ì—…ë¡œë”©: {'âœ…' if USE_DATA_REUPLOADING else 'âŒ'}\")\n",
    "print(f\" â€¢ ë³‘ë ¬ ì„ë² ë”©: {'âœ…' if USE_PARALLEL_EMBEDDING > 1 else 'âŒ'} ({USE_PARALLEL_EMBEDDING}x)\")\n",
    "print(f\" â€¢ êµëŒ€ ì„ë² ë”©: {'âœ…' if USE_ALTERNATE_EMBEDDING else 'âŒ'} (RY/RX)\")\n",
    "print(f\" â€¢ SWAP í…ŒìŠ¤íŠ¸: {'âœ…' if USE_SWAP_TEST else 'âŒ'}\")\n",
    "print()\n",
    "print(\"âš™ï¸  CONTROLLED VARIABLES (í†µì œ ë³€ì¸):\")\n",
    "print(f\" â€¢ ë°°ì¹˜ í¬ê¸°: {EXPERIMENTAL_CONFIG['batch_size']} (ëª¨ë“  ë°©ë²• ê³ ì •)\")\n",
    "print(f\" â€¢ ì–‘ì ë ˆì´ì–´: {EXPERIMENTAL_CONFIG['quantum_layers']}ê°œ (ëª¨ë“  ì–‘ì ë°©ë²• ê³ ì •)\")\n",
    "print(f\" â€¢ ê¸°ë³¸ ëœë¤ ì‹œë“œ: {EXPERIMENTAL_CONFIG['random_state']} (ì¬í˜„ì„± ë³´ì¥)\")\n",
    "print(f\" â€¢ PCA ì°¨ì›: {EXPERIMENTAL_CONFIG['pca_dimensions']}D (ì–‘ì ë°©ë²• í†µì¼)\")\n",
    "print()\n",
    "print(\"â±ï¸  TRAINING EPOCHS:\")\n",
    "print(f\" â€¢ QAE Angle: {QUANTUM_TRAINING_CONFIG['epochs_qae_angle']} epochs\")\n",
    "print(f\" â€¢ Enhanced qVAE: {QUANTUM_TRAINING_CONFIG['epochs_enhanced_qvae']} epochs\")\n",
    "print(f\" â€¢ DIFE QAE: {QUANTUM_TRAINING_CONFIG['epochs_dife']} epochs\")\n",
    "print(f\" â€¢ LS-SWAP QAE: {QUANTUM_TRAINING_CONFIG['epochs_ls_swap']} epochs\")\n",
    "print(\"=\" * 80)\n",
    "print(\" âœ… ì¢…í•© ì‹¤í—˜ ì„¤ì • ì™„ë£Œ! (í†µì œ ë³€ì¸ ê°•í™” + ë°˜ë³µ ì¸¡ì • í†µê³„ ë¶„ì„)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f36220e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ í†µì¼ëœ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘...\n",
      "ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: 946ê°œ ìƒ˜í”Œ, 30ê°œ íŠ¹ì„±\n",
      "ì‚¬ê¸°ìœ¨: 0.5000 (473ê±´)\n",
      "\n",
      "ë°ì´í„° ë¶„í•  ì™„ë£Œ:\n",
      "  ì „ì²´ í›ˆë ¨ì…‹: (756, 30)\n",
      "  ì •ìƒ í›ˆë ¨ì…‹: (378, 30)\n",
      "  í…ŒìŠ¤íŠ¸ì…‹: (190, 30)\n",
      "\n",
      "ì–‘ììš© 4D PCA ë°ì´í„°: í›ˆë ¨ (756, 4), í…ŒìŠ¤íŠ¸ (190, 4)\n",
      "PCA ì„¤ëª…ë¶„ì‚°: 0.6120\n",
      "CNNìš© ë°ì´í„°: í›ˆë ¨ (378, 30, 1), í…ŒìŠ¤íŠ¸ (190, 30, 1)\n",
      "\n",
      "ğŸ“Š ë°ì´í„° ì „ì²˜ë¦¬ ìš”ì•½:\n",
      "  â€¢ ì›ë³¸ íŠ¹ì„± ìˆ˜: 30\n",
      "  â€¢ PCA ì¶•ì†Œ ì°¨ì›: 4\n",
      "  â€¢ í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¹„ìœ¨: 80% / 20%\n",
      "  â€¢ ì •ìƒ/ì‚¬ê¸° ë¹„ìœ¨ (í›ˆë ¨): 50.0% / 50.0%\n",
      "  â€¢ ì •ìƒ/ì‚¬ê¸° ë¹„ìœ¨ (í…ŒìŠ¤íŠ¸): 50.0% / 50.0%\n",
      "\n",
      "âœ… í†µì¼ëœ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!\n",
      "ğŸ¯ ëª¨ë“  ë°©ë²•ì´ ë™ì¼í•œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³µì •í•œ ë¹„êµ ë³´ì¥\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# ğŸ“Š Data Loading & Preprocessing Pipeline (í†µì œ ë³€ì¸)\n",
    "# ==========================================\n",
    "\n",
    "print(\"ğŸ”§ í†µì¼ëœ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘...\")\n",
    "\n",
    "# ë°ì´í„° ë¡œë”© ë° ë¶„í•  (ëª¨ë“  ë°©ë²•ì— ë™ì¼ ì ìš©)\n",
    "df = pd.read_csv(\"preprocessed-creditcard.csv\")\n",
    "X = df.drop(\"Class\", axis=1).values\n",
    "y = df[\"Class\"].values\n",
    "\n",
    "print(f\"ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {X.shape[0]}ê°œ ìƒ˜í”Œ, {X.shape[1]}ê°œ íŠ¹ì„±\")\n",
    "print(f\"ì‚¬ê¸°ìœ¨: {np.mean(y):.4f} ({np.sum(y)}ê±´)\")\n",
    "\n",
    "# ê³„ì¸µí™” ë¶„í•  (í†µì œ ë³€ì¸)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=EXPERIMENTAL_CONFIG['test_size'], \n",
    "    stratify=y, \n",
    "    random_state=EXPERIMENTAL_CONFIG['random_state']\n",
    ")\n",
    "\n",
    "# ì •ìƒ ë°ì´í„°ë§Œ ì¶”ì¶œ (ì´ìƒíƒì§€ìš©)\n",
    "normal_mask = y_train == 0\n",
    "X_train_normal = X_train[normal_mask]\n",
    "\n",
    "# í‘œì¤€í™” (ëª¨ë“  ë°©ë²•ì— ë™ì¼í•œ ìŠ¤ì¼€ì¼ëŸ¬ ì ìš©) - í†µì œ ë³€ì¸\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_train_normal_scaled = scaler.transform(X_train_normal)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\në°ì´í„° ë¶„í•  ì™„ë£Œ:\")\n",
    "print(f\"  ì „ì²´ í›ˆë ¨ì…‹: {X_train_scaled.shape}\")\n",
    "print(f\"  ì •ìƒ í›ˆë ¨ì…‹: {X_train_normal_scaled.shape}\")\n",
    "print(f\"  í…ŒìŠ¤íŠ¸ì…‹: {X_test_scaled.shape}\")\n",
    "\n",
    "# ì–‘ì ì•Œê³ ë¦¬ì¦˜ìš© PCA ì°¨ì› ì¶•ì†Œ (í†µì œ ë³€ì¸)\n",
    "pca_dims = EXPERIMENTAL_CONFIG['pca_dimensions']\n",
    "pca = PCA(n_components=pca_dims, random_state=EXPERIMENTAL_CONFIG['random_state'])\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print(f\"\\nì–‘ììš© {pca_dims}D PCA ë°ì´í„°: í›ˆë ¨ {X_train_pca.shape}, í…ŒìŠ¤íŠ¸ {X_test_pca.shape}\")\n",
    "print(f\"PCA ì„¤ëª…ë¶„ì‚°: {np.sum(pca.explained_variance_ratio_):.4f}\")\n",
    "\n",
    "# CNNìš© ë°ì´í„° í˜•íƒœ ë³€í™˜\n",
    "X_train_normal_cnn = X_train_normal_scaled.reshape(X_train_normal_scaled.shape[0], X_train_normal_scaled.shape[1], 1)\n",
    "X_test_cnn = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
    "\n",
    "print(f\"CNNìš© ë°ì´í„°: í›ˆë ¨ {X_train_normal_cnn.shape}, í…ŒìŠ¤íŠ¸ {X_test_cnn.shape}\")\n",
    "\n",
    "# ë°ì´í„° ìš”ì•½ ì •ë³´\n",
    "print(f\"\\nğŸ“Š ë°ì´í„° ì „ì²˜ë¦¬ ìš”ì•½:\")\n",
    "print(f\"  â€¢ ì›ë³¸ íŠ¹ì„± ìˆ˜: {X.shape[1]}\")\n",
    "print(f\"  â€¢ PCA ì¶•ì†Œ ì°¨ì›: {pca_dims}\")\n",
    "print(f\"  â€¢ í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¹„ìœ¨: {100*(1-EXPERIMENTAL_CONFIG['test_size']):.0f}% / {100*EXPERIMENTAL_CONFIG['test_size']:.0f}%\")\n",
    "print(f\"  â€¢ ì •ìƒ/ì‚¬ê¸° ë¹„ìœ¨ (í›ˆë ¨): {100*(1-np.mean(y_train)):.1f}% / {100*np.mean(y_train):.1f}%\")\n",
    "print(f\"  â€¢ ì •ìƒ/ì‚¬ê¸° ë¹„ìœ¨ (í…ŒìŠ¤íŠ¸): {100*(1-np.mean(y_test)):.1f}% / {100*np.mean(y_test):.1f}%\")\n",
    "\n",
    "print(\"\\nâœ… í†µì¼ëœ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!\")\n",
    "print(\"ğŸ¯ ëª¨ë“  ë°©ë²•ì´ ë™ì¼í•œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³µì •í•œ ë¹„êµ ë³´ì¥\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac76c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ› ï¸  ì¢…í•© ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\n",
      "   â”œâ”€ ì„ê³„ê°’ ìµœì í™” í•¨ìˆ˜ (G-Mean ê¸°ì¤€)\n",
      "   â”œâ”€ ì¢…í•© í‰ê°€ í•¨ìˆ˜ (ëª¨ë“  ì§€í‘œ ê³„ì‚°)\n",
      "   â”œâ”€ í†µê³„ì  ìš”ì•½ í•¨ìˆ˜ (í‰ê· , í‘œì¤€í¸ì°¨, ì‹ ë¢°êµ¬ê°„)\n",
      "   â”œâ”€ QAE Angle ë¹„ìš© í•¨ìˆ˜ (ì œê³± ì†ì‹¤)\n",
      "   â”œâ”€ Enhanced qVAE ë¹„ìš© í•¨ìˆ˜ (ì„ í˜•/ì œê³± ì†ì‹¤)\n",
      "   â”œâ”€ DIFE ë¹„ìš© í•¨ìˆ˜ (ì„ í˜• ì†ì‹¤)\n",
      "   â””â”€ LS-SWAP ë¹„ìš© í•¨ìˆ˜ (ì„ í˜• ì†ì‹¤)\n",
      "âœ… ëª¨ë“  ë°©ë²•ì— ë™ì¼í•œ í‰ê°€ ê¸°ì¤€ ì ìš© ì¤€ë¹„ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# ğŸ› ï¸ Comprehensive Utility Functions (í‰ê°€ í†µì¼í™”)\n",
    "# ==========================================\n",
    "\n",
    "def get_default_params(method_name):\n",
    "    \"\"\"\n",
    "    í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ ë¹„í™œì„±í™”ëœ ê²½ìš° ê¸°ë³¸ íŒŒë¼ë¯¸í„° ë°˜í™˜\n",
    "    \n",
    "    Args:\n",
    "        method_name: ë°©ë²• ì´ë¦„ ('random_forest', 'isolation_forest', 'cnn_autoencoder', \n",
    "                             'classical_autoencoder', 'qae_angle', 'enhanced_qvae', \n",
    "                             'dife_qae', 'ls_swap_qae')\n",
    "    \n",
    "    Returns:\n",
    "        ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬\n",
    "    \"\"\"\n",
    "    if method_name in DEFAULT_HYPERPARAMETERS:\n",
    "        params = DEFAULT_HYPERPARAMETERS[method_name].copy()\n",
    "        print(f\"âœ… ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš© ({method_name}): {params}\")\n",
    "        return params\n",
    "    else:\n",
    "        print(f\"âš ï¸  '{method_name}' ë°©ë²•ì˜ ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return {}\n",
    "\n",
    "def should_optimize_hyperparameters():\n",
    "    \"\"\"\n",
    "    í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ìˆ˜í–‰ ì—¬ë¶€ í™•ì¸\n",
    "    \n",
    "    Returns:\n",
    "        bool: í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ìˆ˜í–‰ ì—¬ë¶€\n",
    "    \"\"\"\n",
    "    if HYPERPARAMETER_SEARCH['enable_search']:\n",
    "        print(\"ğŸ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ëª¨ë“œ: í™œì„±í™”\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"âš¡ ë¹ ë¥¸ ì‹¤í–‰ ëª¨ë“œ: ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©\")\n",
    "        return False\n",
    "\n",
    "def find_optimal_threshold(y_true, scores, metric='gmean_optimized'):\n",
    "    \"\"\"\n",
    "    ìµœì  ì„ê³„ê°’ ì°¾ê¸° (ëª¨ë“  ë°©ë²•ì— ë™ì¼í•œ ì„ê³„ê°’ ìµœì í™”)\n",
    "    \n",
    "    Args:\n",
    "        y_true: ì‹¤ì œ ë ˆì´ë¸”\n",
    "        scores: ì˜ˆì¸¡ ì ìˆ˜ (ì´ìƒì¹˜ ì ìˆ˜)\n",
    "        metric: ìµœì í™”í•  ë©”íŠ¸ë¦­ ('gmean_optimized' ë˜ëŠ” 'f1')\n",
    "    \n",
    "    Returns:\n",
    "        best_threshold: ìµœì  ì„ê³„ê°’\n",
    "        best_score: ìµœì  ì ìˆ˜\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(np.min(scores), np.max(scores), 100)\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (scores >= threshold).astype(int)\n",
    "        \n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "        \n",
    "        if metric == 'gmean_optimized':\n",
    "            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "            score = np.sqrt(sensitivity * specificity)\n",
    "        elif metric == 'f1':\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "            score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_threshold, best_score\n",
    "\n",
    "\n",
    "def evaluate_with_optimal_threshold(y_true, scores):\n",
    "    \"\"\"\n",
    "    ìµœì  ì„ê³„ê°’ì„ ì‚¬ìš©í•œ ì¢…í•© í‰ê°€ (ëª¨ë“  ë°©ë²•ì— ë™ì¼í•œ í‰ê°€ ê¸°ì¤€)\n",
    "    \n",
    "    Args:\n",
    "        y_true: ì‹¤ì œ ë ˆì´ë¸”\n",
    "        scores: ì˜ˆì¸¡ ì ìˆ˜ (ì´ìƒì¹˜ ì ìˆ˜)\n",
    "    \n",
    "    Returns:\n",
    "        dict: ëª¨ë“  í‰ê°€ ì§€í‘œë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬\n",
    "    \"\"\"\n",
    "    # ìµœì  ì„ê³„ê°’ ì°¾ê¸° (G-Mean ìµœì í™”)\n",
    "    best_threshold, best_gmean = find_optimal_threshold(y_true, scores, 'gmean_optimized')\n",
    "    \n",
    "    # ì˜ˆì¸¡\n",
    "    y_pred = (scores >= best_threshold).astype(int)\n",
    "    \n",
    "    # í‰ê°€ ì§€í‘œ ê³„ì‚°\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    auc = roc_auc_score(y_true, scores)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'specificity': specificity,\n",
    "        'gmean': best_gmean,\n",
    "        'auc': auc,\n",
    "        'threshold': best_threshold\n",
    "    }\n",
    "\n",
    "\n",
    "def print_results(method_name, metrics, additional_info=None):\n",
    "    \"\"\"\n",
    "    ê²°ê³¼ë¥¼ ì¼ê´€ëœ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥\n",
    "    \n",
    "    Args:\n",
    "        method_name: ë°©ë²•ë¡  ì´ë¦„\n",
    "        metrics: í‰ê°€ ì§€í‘œ ë”•ì…”ë„ˆë¦¬\n",
    "        additional_info: ì¶”ê°€ ì •ë³´ (íë¹„íŠ¸ ìˆ˜, í›ˆë ¨ ì‹œê°„ ë“±)\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ“Š {method_name} ì„±ëŠ¥ ê²°ê³¼:\")\n",
    "    print(\"â”€\" * 60)\n",
    "    print(f\"  ğŸ¯ AUC-ROC:  {metrics['auc']:.4f}\")\n",
    "    print(f\"  âš–ï¸  ì •í™•ë„:    {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  ğŸª ì •ë°€ë„:    {metrics['precision']:.4f}\")\n",
    "    print(f\"  ğŸ” ì¬í˜„ìœ¨:    {metrics['recall']:.4f}\")\n",
    "    print(f\"  ğŸ† F1-Score: {metrics['f1_score']:.4f}\")\n",
    "    print(f\"  ğŸ“ G-Mean:   {metrics['gmean']:.4f}\")\n",
    "    print(f\"  ğŸ­ íŠ¹ì´ë„:    {metrics['specificity']:.4f}\")\n",
    "    \n",
    "    if additional_info:\n",
    "        print(\"  ğŸ“‹ ì¶”ê°€ ì •ë³´:\")\n",
    "        for key, value in additional_info.items():\n",
    "            print(f\"     â€¢ {key}: {value}\")\n",
    "    print(\"â”€\" * 60)\n",
    "\n",
    "\n",
    "def compute_statistical_summary(results_list):\n",
    "    \"\"\"\n",
    "    ë°˜ë³µ ì‹¤í—˜ ê²°ê³¼ì˜ í†µê³„ì  ìš”ì•½ ê³„ì‚°\n",
    "    \n",
    "    Args:\n",
    "        results_list: ê° ë°˜ë³µ ì‹¤í–‰ì˜ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸\n",
    "        \n",
    "    Returns:\n",
    "        dict: í‰ê· , í‘œì¤€í¸ì°¨, ì‹ ë¢°êµ¬ê°„ì„ í¬í•¨í•œ í†µê³„ ìš”ì•½\n",
    "    \"\"\"\n",
    "    if not results_list:\n",
    "        return None\n",
    "    \n",
    "    # ê° ì§€í‘œë³„ ê°’ë“¤ ìˆ˜ì§‘\n",
    "    metrics = ['auc', 'accuracy', 'precision', 'recall', 'f1_score', 'gmean', 'specificity']\n",
    "    summary = {}\n",
    "    \n",
    "    for metric in metrics:\n",
    "        values = [result[metric] for result in results_list if metric in result]\n",
    "        if values:\n",
    "            summary[metric] = {\n",
    "                'mean': np.mean(values),\n",
    "                'std': np.std(values),\n",
    "                'min': np.min(values),\n",
    "                'max': np.max(values),\n",
    "                'n': len(values)\n",
    "            }\n",
    "            \n",
    "            # 95% ì‹ ë¢°êµ¬ê°„ ê³„ì‚° (t-ë¶„í¬ ì‚¬ìš©)\n",
    "            from scipy import stats\n",
    "            confidence_level = STATISTICAL_CONFIG['confidence_level']\n",
    "            alpha = 1 - confidence_level\n",
    "            degrees_freedom = len(values) - 1\n",
    "            t_critical = stats.t.ppf(1 - alpha/2, degrees_freedom) if degrees_freedom > 0 else 1.96\n",
    "            margin_error = t_critical * (np.std(values) / np.sqrt(len(values)))\n",
    "            \n",
    "            summary[metric]['ci_lower'] = summary[metric]['mean'] - margin_error\n",
    "            summary[metric]['ci_upper'] = summary[metric]['mean'] + margin_error\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# ğŸ”„ ë°˜ë³µ ì‹¤í—˜ ì§€ì› í•¨ìˆ˜ë“¤ (ì‹¤í—˜ì„¤ê³„.txt êµ¬í˜„)\n",
    "# ==========================================\n",
    "\n",
    "def run_multiple_experiments(experiment_function, method_name, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    ì‹¤í—˜ì„¤ê³„.txtì— ë”°ë¥¸ ë°˜ë³µ ì‹¤í—˜ ì‹¤í–‰ ë˜í¼\n",
    "    \n",
    "    ê° ë°©ë²•ì„ ì—¬ëŸ¬ ë²ˆ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰í•˜ê³  í†µê³„ì  ìš”ì•½ì„ ê³„ì‚°\n",
    "    \n",
    "    Args:\n",
    "        experiment_function: ì‹¤í–‰í•  ì‹¤í—˜ í•¨ìˆ˜\n",
    "        method_name: ë°©ë²•ë¡  ì´ë¦„ (ì €ì¥ì†Œ í‚¤ìš©)\n",
    "        *args, **kwargs: ì‹¤í—˜ í•¨ìˆ˜ì— ì „ë‹¬í•  ì¸ìˆ˜ë“¤\n",
    "        \n",
    "    Returns:\n",
    "        dict: í†µê³„ì  ìš”ì•½ì´ í¬í•¨ëœ ìµœì¢… ê²°ê³¼\n",
    "    \"\"\"\n",
    "    if not STATISTICAL_CONFIG['enable_repetition']:\n",
    "        # ë°˜ë³µ ì‹¤í—˜ì´ ë¹„í™œì„±í™”ëœ ê²½ìš° ë‹¨ì¼ ì‹¤í–‰\n",
    "        print(f\"ğŸ”„ ë‹¨ì¼ ì‹¤í–‰ ëª¨ë“œ: {method_name}\")\n",
    "        set_experiment_seed(EXPERIMENTAL_CONFIG['random_state'])\n",
    "        return experiment_function(*args, **kwargs)\n",
    "    \n",
    "    print(f\"\\nğŸ”„ ë°˜ë³µ ì‹¤í—˜ ì‹œì‘: {method_name} ({STATISTICAL_CONFIG['n_repetitions']}íšŒ)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    individual_results = []\n",
    "    method_key = method_name.lower().replace(' ', '_').replace('-', '_')\n",
    "    \n",
    "    for i, seed in enumerate(experiment_seeds[:STATISTICAL_CONFIG['n_repetitions']]):\n",
    "        print(f\"\\nğŸ“ ì‹¤í–‰ {i+1}/{STATISTICAL_CONFIG['n_repetitions']} - ì‹œë“œ: {seed}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # ì‹¤í—˜ ì „ ì‹œë“œ ì„¤ì •\n",
    "        set_experiment_seed(seed)\n",
    "        \n",
    "        try:\n",
    "            # ê°œë³„ ì‹¤í—˜ ì‹¤í–‰\n",
    "            result = experiment_function(*args, **kwargs)\n",
    "            \n",
    "            if result is not None:\n",
    "                result['repetition_id'] = i + 1\n",
    "                result['seed'] = seed\n",
    "                individual_results.append(result)\n",
    "                \n",
    "                # ê°œë³„ ê²°ê³¼ë¥¼ ì „ì—­ ì €ì¥ì†Œì— ì €ì¥\n",
    "                if method_key in experiment_results_store:\n",
    "                    experiment_results_store[method_key].append(result)\n",
    "                \n",
    "                print(f\"âœ… ì‹¤í–‰ {i+1} ì™„ë£Œ - G-Mean: {result['gmean']:.4f}\")\n",
    "            else:\n",
    "                print(f\"âŒ ì‹¤í–‰ {i+1} ì‹¤íŒ¨\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì‹¤í–‰ {i+1} ì˜¤ë¥˜: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # í†µê³„ì  ìš”ì•½ ê³„ì‚°\n",
    "    if individual_results:\n",
    "        statistical_summary = compute_statistical_summary_advanced(individual_results, method_name)\n",
    "        \n",
    "        # í†µê³„ ê²°ê³¼ ì¶œë ¥\n",
    "        print_statistical_results(statistical_summary)\n",
    "        \n",
    "        # ëŒ€í‘œê°’ìœ¼ë¡œ í‰ê·  ì„±ëŠ¥ ê²°ê³¼ ìƒì„±\n",
    "        representative_result = create_representative_result(statistical_summary, individual_results[0])\n",
    "        representative_result['statistical_summary'] = statistical_summary\n",
    "        representative_result['individual_results'] = individual_results\n",
    "        \n",
    "        print(f\"\\nğŸ {method_name} ë°˜ë³µ ì‹¤í—˜ ì™„ë£Œ\")\n",
    "        print(f\"   ì„±ê³µí•œ ì‹¤í–‰: {len(individual_results)}/{STATISTICAL_CONFIG['n_repetitions']}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        return representative_result\n",
    "    else:\n",
    "        print(f\"\\nâŒ {method_name} ë°˜ë³µ ì‹¤í—˜ ì‹¤íŒ¨ - ì„±ê³µí•œ ì‹¤í–‰ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "\n",
    "def create_representative_result(statistical_summary, template_result):\n",
    "    \"\"\"\n",
    "    í†µê³„ ìš”ì•½ìœ¼ë¡œë¶€í„° ëŒ€í‘œ ê²°ê³¼ ìƒì„±\n",
    "    \n",
    "    Args:\n",
    "        statistical_summary: í†µê³„ì  ìš”ì•½\n",
    "        template_result: ê²°ê³¼ êµ¬ì¡° í…œí”Œë¦¿\n",
    "        \n",
    "    Returns:\n",
    "        dict: í‰ê·  ì„±ëŠ¥ì„ ë‚˜íƒ€ë‚´ëŠ” ëŒ€í‘œ ê²°ê³¼\n",
    "    \"\"\"\n",
    "    representative = template_result.copy()\n",
    "    \n",
    "    # í†µê³„ì  í‰ê· ê°’ìœ¼ë¡œ ì„±ëŠ¥ ì§€í‘œ ì—…ë°ì´íŠ¸\n",
    "    for metric in ['auc', 'accuracy', 'precision', 'recall', 'f1_score', 'gmean', 'specificity']:\n",
    "        if metric in statistical_summary:\n",
    "            representative[metric] = statistical_summary[metric]['mean']\n",
    "    \n",
    "    # ì¶”ê°€ í†µê³„ ì •ë³´\n",
    "    representative['repetitions_completed'] = statistical_summary['n_repetitions']\n",
    "    representative['is_statistical_summary'] = True\n",
    "    \n",
    "    return representative\n",
    "\n",
    "def save_experiment_results():\n",
    "    \"\"\"\n",
    "    ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (ì„ íƒì )\n",
    "    \n",
    "    ì‹¤í—˜ì„¤ê³„.txtì˜ ê²°ê³¼ ê¸°ë¡ ìš”êµ¬ì‚¬í•­ êµ¬í˜„\n",
    "    \"\"\"\n",
    "    if not STATISTICAL_CONFIG.get('save_individual_results', False):\n",
    "        return\n",
    "    \n",
    "    import json\n",
    "    import pickle\n",
    "    from datetime import datetime\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # JSON í˜•íƒœë¡œ ìš”ì•½ ì €ì¥\n",
    "    summary_data = {}\n",
    "    for method_key, results in experiment_results_store.items():\n",
    "        if results:\n",
    "            # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜\n",
    "            serializable_results = []\n",
    "            for result in results:\n",
    "                clean_result = {}\n",
    "                for key, value in result.items():\n",
    "                    if isinstance(value, (int, float, str, bool, list)):\n",
    "                        clean_result[key] = value\n",
    "                    elif isinstance(value, np.ndarray):\n",
    "                        clean_result[key] = value.tolist()\n",
    "                    elif key in ['auc', 'accuracy', 'precision', 'recall', 'f1_score', 'gmean', 'specificity']:\n",
    "                        clean_result[key] = float(value)\n",
    "                serializable_results.append(clean_result)\n",
    "            \n",
    "            summary_data[method_key] = {\n",
    "                'n_repetitions': len(serializable_results),\n",
    "                'individual_results': serializable_results\n",
    "            }\n",
    "    \n",
    "    filename = f\"experiment_results_{timestamp}.json\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"ğŸ“ ì‹¤í—˜ ê²°ê³¼ ì €ì¥ë¨: {filename}\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# ì–‘ì íšŒë¡œìš© ë¹„ìš© í•¨ìˆ˜ë“¤\n",
    "# ==========================================\n",
    "\n",
    "def compute_batch_cost_qae_angle(samples, circuit, weights):\n",
    "    \"\"\"\n",
    "    ë°°ì¹˜ ë¹„ìš© ê³„ì‚° - QAE Angleìš© (í‘œì¤€ ì œê³± ì†ì‹¤)\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    for sample in samples:\n",
    "        features = pnp.array(sample, requires_grad=False)\n",
    "        expval = circuit(features, weights)\n",
    "        \n",
    "        # ì¶©ì‹¤ë„(fidelity) ê³„ì‚°\n",
    "        fidelity = (expval + 1.0) / 2.0\n",
    "        \n",
    "        # ì œê³± ì˜¤ì°¨ ê³„ì‚°\n",
    "        error = (1.0 - fidelity) ** 2\n",
    "        errors.append(error)\n",
    "    \n",
    "    return pnp.mean(pnp.stack(errors))\n",
    "\n",
    "\n",
    "def compute_batch_cost_enhanced_qvae(samples, circuit, weights, use_swap_test=True):\n",
    "    \"\"\"\n",
    "    ë°°ì¹˜ ë¹„ìš© ê³„ì‚° - Enhanced qVAEìš© (ì„ í˜•/ì œê³± ì†ì‹¤)\n",
    "    \"\"\"\n",
    "    linear_errors = []\n",
    "    squared_errors = []\n",
    "    \n",
    "    for sample in samples:\n",
    "        features = pnp.array(sample, requires_grad=False)\n",
    "        expval = circuit(features, weights)\n",
    "        \n",
    "        if use_swap_test:\n",
    "            # SWAP testëŠ” [-1, 1] ë²”ìœ„ì—ì„œ [0, 1] ì¶©ì‹¤ë„ë¡œ ë³€í™˜\n",
    "            fidelity = (expval + 1.0) / 2.0\n",
    "        else:\n",
    "            # í‘œì¤€ ê¸°ëŒ“ê°’ì„ ì¶©ì‹¤ë„ë¡œ ë³€í™˜\n",
    "            fidelity = (expval + 1.0) / 2.0\n",
    "        \n",
    "        # ì„ í˜• ë° ì œê³± ì†ì‹¤ ê³„ì‚°\n",
    "        linear_error = 1.0 - fidelity\n",
    "        squared_error = (1.0 - fidelity) ** 2\n",
    "        \n",
    "        linear_errors.append(linear_error)\n",
    "        squared_errors.append(squared_error)\n",
    "    \n",
    "    linear_loss = pnp.mean(pnp.stack(linear_errors))\n",
    "    squared_loss = pnp.mean(pnp.stack(squared_errors))\n",
    "    \n",
    "    # SWAP í…ŒìŠ¤íŠ¸ ì‚¬ìš© ì‹œ ì„ í˜• ì†ì‹¤, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ì œê³± ì†ì‹¤ ë°˜í™˜\n",
    "    return linear_loss if use_swap_test else squared_loss\n",
    "\n",
    "\n",
    "def compute_batch_cost_dife(samples, circuit, weights):\n",
    "    \"\"\"\n",
    "    ë°°ì¹˜ ë¹„ìš© ê³„ì‚° - DIFEìš© (ì„ í˜• ì†ì‹¤)\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    for sample in samples:\n",
    "        features = pnp.array(sample, requires_grad=False)\n",
    "        expval = circuit(features, weights)\n",
    "        \n",
    "        # DIFEì—ì„œ expvalì€ ì´ë¯¸ ì¶©ì‹¤ë„ ì¸¡ì •ê°’\n",
    "        fidelity = expval\n",
    "        \n",
    "        # ì„ í˜• ì†ì‹¤ ê³„ì‚° (DIFE íŠ¹ì„±ìƒ ì„ í˜• ì†ì‹¤ì´ ë” ì í•©)\n",
    "        error = 1.0 - fidelity\n",
    "        errors.append(error)\n",
    "    \n",
    "    return pnp.mean(pnp.stack(errors))\n",
    "\n",
    "\n",
    "def compute_batch_cost_ls_swap(samples, circuit, weights):\n",
    "    \"\"\"\n",
    "    ë°°ì¹˜ ë¹„ìš© ê³„ì‚° - LS-SWAPìš© (ì„ í˜• ì†ì‹¤)\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    for sample in samples:\n",
    "        features = pnp.array(sample, requires_grad=False)\n",
    "        expval = circuit(features, weights)\n",
    "        \n",
    "        # LS-SWAP test ì¶©ì‹¤ë„ ë³€í™˜\n",
    "        fidelity = (expval + 1.0) / 2.0\n",
    "        \n",
    "        # ì„ í˜• ì†ì‹¤ ê³„ì‚°\n",
    "        error = 1.0 - fidelity\n",
    "        errors.append(error)\n",
    "    \n",
    "    return pnp.mean(pnp.stack(errors))\n",
    "\n",
    "\n",
    "print(\"ğŸ› ï¸  ì¢…í•© ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")\n",
    "print(\"   â”œâ”€ ì„ê³„ê°’ ìµœì í™” í•¨ìˆ˜ (G-Mean ê¸°ì¤€)\")\n",
    "print(\"   â”œâ”€ ì¢…í•© í‰ê°€ í•¨ìˆ˜ (ëª¨ë“  ì§€í‘œ ê³„ì‚°)\") \n",
    "print(\"   â”œâ”€ í†µê³„ì  ìš”ì•½ í•¨ìˆ˜ (í‰ê· , í‘œì¤€í¸ì°¨, ì‹ ë¢°êµ¬ê°„)\")\n",
    "print(\"   â”œâ”€ ğŸ”„ ë°˜ë³µ ì‹¤í—˜ ë˜í¼ í•¨ìˆ˜ (ì‹¤í—˜ì„¤ê³„.txt êµ¬í˜„)\")\n",
    "print(\"   â”œâ”€ ğŸ“Š í†µê³„ì  ë¶„ì„ í•¨ìˆ˜ (t-ë¶„í¬ ì‹ ë¢°êµ¬ê°„)\")\n",
    "print(\"   â”œâ”€ ğŸ¯ ëŒ€í‘œê°’ ìƒì„± í•¨ìˆ˜ (í‰ê·  ì„±ëŠ¥ ê²°ê³¼)\")\n",
    "print(\"   â”œâ”€ QAE Angle ë¹„ìš© í•¨ìˆ˜ (ì œê³± ì†ì‹¤)\")\n",
    "print(\"   â”œâ”€ Enhanced qVAE ë¹„ìš© í•¨ìˆ˜ (ì„ í˜•/ì œê³± ì†ì‹¤)\")\n",
    "print(\"   â”œâ”€ DIFE ë¹„ìš© í•¨ìˆ˜ (ì„ í˜• ì†ì‹¤)\")\n",
    "print(\"   â””â”€ LS-SWAP ë¹„ìš© í•¨ìˆ˜ (ì„ í˜• ì†ì‹¤)\")\n",
    "print(\"âœ… ëª¨ë“  ë°©ë²•ì— ë™ì¼í•œ í‰ê°€ ê¸°ì¤€ ì ìš© + ë°˜ë³µ ì‹¤í—˜ í†µê³„ ë¶„ì„ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ”„ ë°˜ë³µ ì‹¤í—˜: {'í™œì„±í™” (%díšŒ)' % STATISTICAL_CONFIG['n_repetitions'] if STATISTICAL_CONFIG['enable_repetition'] else 'ë¹„í™œì„±í™”'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9a0649",
   "metadata": {},
   "source": [
    "# ğŸ›ï¸ Classical Methods Implementation (ë¹„ì–‘ì ë°©ë²• 4ê°€ì§€)\n",
    "\n",
    "ì´ ì„¹ì…˜ì—ì„œëŠ” **more_qubits íŒŒì¼ì˜ ê²€ì¦ëœ êµ¬í˜„**ì„ ê¸°ë°˜ìœ¼ë¡œ 4ê°€ì§€ ë¹„ì–‘ì ë¨¸ì‹ ëŸ¬ë‹ ë°©ë²•ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ“‹ êµ¬í˜„ ìˆœì„œ\n",
    "\n",
    "### 1. **Random Forest** - íŠ¹ì„± ì¬êµ¬ì„± ê¸°ë°˜ ì´ìƒíƒì§€\n",
    "- **ì›ë¦¬**: ì •ìƒ ë°ì´í„°ë¡œ Random Forestë¥¼ í›ˆë ¨í•˜ì—¬ íŠ¹ì„± ì¬êµ¬ì„±\n",
    "- **ì´ìƒ íƒì§€**: ì¬êµ¬ì„± ì˜¤ì°¨ê°€ í° ìƒ˜í”Œì„ ì‚¬ê¸°ë¡œ ë¶„ë¥˜\n",
    "- **ì¥ì **: í•´ì„ ê°€ëŠ¥ì„±, ì•ˆì •ì„±\n",
    "\n",
    "### 2. **IsolationForest** - ë¹„ì§€ë„ ì´ìƒíƒì§€\n",
    "- **ì›ë¦¬**: ì •ìƒ íŒ¨í„´ì—ì„œ ê²©ë¦¬ë˜ê¸° ì‰¬ìš´ ìƒ˜í”Œì„ ì´ìƒì¹˜ë¡œ íƒì§€\n",
    "- **ì´ìƒ íƒì§€**: ìŒì˜ ì´ìƒì¹˜ ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¥˜\n",
    "- **ì¥ì **: ë ˆì´ë¸” ì—†ëŠ” ì´ìƒíƒì§€, ë¹ ë¥¸ ì²˜ë¦¬\n",
    "\n",
    "### 3. **CNN Autoencoder** - 1D Convolutional Neural Network\n",
    "- **ì›ë¦¬**: 1D CNNìœ¼ë¡œ íŠ¹ì„±ì˜ ì‹œí€€ìŠ¤ íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ ì¬êµ¬ì„±\n",
    "- **ì´ìƒ íƒì§€**: ì¬êµ¬ì„± ì˜¤ì°¨ë¥¼ í†µí•œ ì´ìƒ íƒì§€\n",
    "- **ì¥ì **: íŒ¨í„´ ì¸ì‹, íŠ¹ì„± ê°„ ê´€ê³„ í•™ìŠµ\n",
    "\n",
    "### 4. **Classical Autoencoder** - ì „í†µì  ì‹ ê²½ë§ ì˜¤í† ì¸ì½”ë”\n",
    "- **ì›ë¦¬**: Dense ë ˆì´ì–´ ê¸°ë°˜ ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ë¡œ ì°¨ì› ì¶•ì†Œ ë° ë³µì›\n",
    "- **ì´ìƒ íƒì§€**: ì •ìƒ ë°ì´í„°ì˜ ì¬êµ¬ì„± í’ˆì§ˆì„ ê¸°ì¤€ìœ¼ë¡œ ì´ìƒ íƒì§€\n",
    "- **ì¥ì **: ê°•ë ¥í•œ í‘œí˜„ë ¥, ê²€ì¦ëœ ì´ìƒíƒì§€ ì„±ëŠ¥\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ ê³µí†µ ìµœì í™” ì „ëµ (í†µì œ ë³€ì¸)\n",
    "\n",
    "- **í•˜ì´í¼íŒŒë¼ë¯¸í„° ê²€ìƒ‰**: ëœë¤ ì„œì¹˜ + êµì°¨ ê²€ì¦ (ëª¨ë“  ë°©ë²• ë™ì¼)\n",
    "- **í‰ê°€ ì§€í‘œ**: G-Mean ìµœì í™” (ë¶ˆê· í˜• ë°ì´í„° ì í•©)  \n",
    "- **ì¡°ê¸° ì¢…ë£Œ**: ê³¼ì í•© ë°©ì§€ (ë”¥ëŸ¬ë‹ ë°©ë²•)\n",
    "- **ì¬í˜„ì„±**: ê³ ì •ëœ ëœë¤ ì‹œë“œ ì‚¬ìš©\n",
    "- **ì„ê³„ê°’ ìµœì í™”**: ë™ì¼í•œ G-Mean ê¸°ë°˜ ì„ê³„ê°’ íƒìƒ‰\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8534fe36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Random Forest ë°©ë²• ì‹¤í–‰ ì¤‘...\n",
      "\n",
      "============================================================\n",
      "ğŸŒ³ METHOD 1: RANDOM FOREST í›ˆë ¨ ì‹œì‘\n",
      "============================================================\n",
      "ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì¤‘...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€ ì¤‘...\n",
      "\n",
      "ğŸ“Š Random Forest ì„±ëŠ¥ ê²°ê³¼:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  ğŸ¯ AUC-ROC:  0.9222\n",
      "  âš–ï¸  ì •í™•ë„:    0.8737\n",
      "  ğŸª ì •ë°€ë„:    0.9277\n",
      "  ğŸ” ì¬í˜„ìœ¨:    0.8105\n",
      "  ğŸ† F1-Score: 0.8652\n",
      "  ğŸ“ G-Mean:   0.8714\n",
      "  ğŸ­ íŠ¹ì´ë„:    0.9368\n",
      "  ğŸ“‹ ì¶”ê°€ ì •ë³´:\n",
      "     â€¢ í›ˆë ¨ì‹œê°„: 1.51ì´ˆ\n",
      "     â€¢ ëª¨ë¸ íƒ€ì…: Feature Reconstruction\n",
      "     â€¢ ìµœì  ì„ê³„ê°’: 0.391175\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ… Random Forest ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Method 1: Random Forest (íŠ¹ì„± ì¬êµ¬ì„± ê¸°ë°˜ ì´ìƒíƒì§€)\n",
    "# ==========================================\n",
    "\n",
    "def optimize_rf_hyperparameters(X_train, X_val, y_val):\n",
    "    \"\"\"Random Forest í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\"\"\"\n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    \n",
    "    param_sampler = ParameterSampler(RF_PARAM_GRID, n_iter=HYPERPARAMETER_SEARCH['search_iterations'], random_state=42)\n",
    "    \n",
    "    print(f\"Random Forest í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘ ({HYPERPARAMETER_SEARCH['search_iterations']}íšŒ ë°˜ë³µ)\")\n",
    "    \n",
    "    for i, params in enumerate(param_sampler):\n",
    "        try:\n",
    "            print(f\"  ì¡°í•© {i+1}/{HYPERPARAMETER_SEARCH['search_iterations']}: {params}\")\n",
    "            \n",
    "            # íŠ¹ì„± ì¬êµ¬ì„± ëª¨ë¸ í›ˆë ¨\n",
    "            rf_model = RandomForestRegressor(**params, random_state=42)\n",
    "            rf_model.fit(X_train, X_train)  # ìê¸° ìì‹ ì„ ì¬êµ¬ì„±\n",
    "            \n",
    "            # ê²€ì¦ ë°ì´í„°ë¡œ í‰ê°€\n",
    "            X_val_reconstructed = rf_model.predict(X_val)\n",
    "            reconstruction_errors = np.mean(np.square(X_val - X_val_reconstructed), axis=1)\n",
    "            \n",
    "            # G-Meanìœ¼ë¡œ í‰ê°€\n",
    "            metrics = evaluate_with_optimal_threshold(y_val, reconstruction_errors)\n",
    "            score = metrics['gmean']\n",
    "            \n",
    "            print(f\"    -> G-Mean: {score:.4f}\")\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = params\n",
    "                print(f\"    âœ“ ìƒˆë¡œìš´ ìµœê³  ì ìˆ˜!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    âœ— ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if best_params is None:\n",
    "        print(\"  ëª¨ë“  ì¡°í•© ì‹¤íŒ¨. ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©.\")\n",
    "        best_params = {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 20,\n",
    "            'min_samples_split': 5,\n",
    "            'min_samples_leaf': 2\n",
    "        }\n",
    "    \n",
    "    print(f\"Random Forest ìµœì í™” ì™„ë£Œ. ìµœê³  G-Mean: {best_score:.4f}\")\n",
    "    return best_params, best_score\n",
    "\n",
    "def _train_rf_single():\n",
    "    \"\"\"ë‹¨ì¼ Random Forest ëª¨ë¸ í•™ìŠµ (ë°˜ë³µ ì‹¤í—˜ìš© ë‚´ë¶€ í•¨ìˆ˜)\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸŒ³ METHOD 1: RANDOM FOREST í›ˆë ¨ ì‹œì‘\")\n",
    "    print(\"=\"*60)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë˜ëŠ” ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
    "        if should_optimize_hyperparameters():\n",
    "            # ê²€ì¦ ë°ì´í„° ë¶„í• \n",
    "            X_train_val, X_val_normal = train_test_split(\n",
    "                X_train_normal_scaled, test_size=HYPERPARAMETER_SEARCH['validation_split'], \n",
    "                random_state=np.random.randint(1, 1000)  # ë°˜ë³µ ì‹¤í—˜ë§ˆë‹¤ ë‹¤ë¥¸ ë¶„í• \n",
    "            )\n",
    "            \n",
    "            # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ fraud ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸°\n",
    "            fraud_mask = y_test == 1\n",
    "            X_fraud_val = X_test_scaled[fraud_mask][:len(X_val_normal)//10]\n",
    "            \n",
    "            X_val = np.vstack([X_val_normal, X_fraud_val])\n",
    "            y_val = np.hstack([np.zeros(len(X_val_normal)), np.ones(len(X_fraud_val))])\n",
    "            \n",
    "            print(\"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘...\")\n",
    "            best_params, best_score = optimize_rf_hyperparameters(X_train_val, X_val, y_val)\n",
    "            print(f\"ìµœì  íŒŒë¼ë¯¸í„°: {best_params}\")\n",
    "        else:\n",
    "            # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
    "            best_params = get_default_params('random_forest')\n",
    "        \n",
    "        # ìµœì¢… ëª¨ë¸ í›ˆë ¨\n",
    "        print(\"ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì¤‘...\")\n",
    "        rf_model = RandomForestRegressor(**best_params, random_state=np.random.randint(1, 1000))\n",
    "        rf_model.fit(X_train_normal_scaled, X_train_normal_scaled)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€\n",
    "        print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€ ì¤‘...\")\n",
    "        X_test_reconstructed = rf_model.predict(X_test_scaled)\n",
    "        reconstruction_errors = np.mean(np.square(X_test_scaled - X_test_reconstructed), axis=1)\n",
    "        \n",
    "        # í‰ê°€\n",
    "        metrics = evaluate_with_optimal_threshold(y_test, reconstruction_errors)\n",
    "        \n",
    "        result = {\n",
    "            'method': 'Random Forest',\n",
    "            'type': 'Classical ML',\n",
    "            'training_time': training_time,\n",
    "            'model': rf_model,\n",
    "            'best_params': best_params,\n",
    "            'reconstruction_errors': reconstruction_errors,\n",
    "            **metrics\n",
    "        }\n",
    "        \n",
    "        # ê²°ê³¼ ì¶œë ¥ (ë°˜ë³µ ì‹¤í—˜ ì‹œ ê°„ì†Œí™”)\n",
    "        if STATISTICAL_CONFIG['enable_repetition']:\n",
    "            print(f\"G-Mean: {metrics['gmean']:.4f}, í›ˆë ¨ì‹œê°„: {training_time:.2f}ì´ˆ\")\n",
    "        else:\n",
    "            additional_info = {\n",
    "                'í›ˆë ¨ì‹œê°„': f\"{training_time:.2f}ì´ˆ\",\n",
    "                'ëª¨ë¸ íƒ€ì…': 'Feature Reconstruction',\n",
    "                'ìµœì  ì„ê³„ê°’': f\"{metrics['threshold']:.6f}\"\n",
    "            }\n",
    "            print_results(\"Random Forest\", metrics, additional_info)\n",
    "        \n",
    "        print(\"âœ… Random Forest ì™„ë£Œ\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Random Forest ì‹¤íŒ¨: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def train_rf():\n",
    "    \"\"\"\n",
    "    Random Forest í›ˆë ¨ (ë°˜ë³µ ì‹¤í—˜ ì§€ì›)\n",
    "    ì‹¤í—˜ì„¤ê³„.txt êµ¬í˜„: ì—¬ëŸ¬ ë²ˆì˜ ë…ë¦½ì  ì‹¤í–‰ í›„ í†µê³„ì  ìš”ì•½\n",
    "    \"\"\"\n",
    "    return run_multiple_experiments(_train_rf_single, 'Random Forest')\n",
    "\n",
    "# Random Forest ì‹¤í–‰\n",
    "print(\"ğŸš€ Random Forest ë°©ë²• ì‹¤í–‰ ì¤‘...\")\n",
    "rf_result = train_rf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82806127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ IsolationForest ë°©ë²• ì‹¤í–‰ ì¤‘...\n",
      "\n",
      "============================================================\n",
      "ğŸŒ² METHOD 2: ISOLATIONFOREST í›ˆë ¨ ì‹œì‘\n",
      "============================================================\n",
      "ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€ ì¤‘...\n",
      "\n",
      "ğŸ“Š IsolationForest ì„±ëŠ¥ ê²°ê³¼:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  ğŸ¯ AUC-ROC:  0.9417\n",
      "  âš–ï¸  ì •í™•ë„:    0.9053\n",
      "  ğŸª ì •ë°€ë„:    0.8969\n",
      "  ğŸ” ì¬í˜„ìœ¨:    0.9158\n",
      "  ğŸ† F1-Score: 0.9062\n",
      "  ğŸ“ G-Mean:   0.9052\n",
      "  ğŸ­ íŠ¹ì´ë„:    0.8947\n",
      "  ğŸ“‹ ì¶”ê°€ ì •ë³´:\n",
      "     â€¢ í›ˆë ¨ì‹œê°„: 0.44ì´ˆ\n",
      "     â€¢ ëª¨ë¸ íƒ€ì…: Unsupervised Anomaly Detection\n",
      "     â€¢ ìµœì  ì„ê³„ê°’: -0.009944\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ… IsolationForest ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Method 2: IsolationForest (ë¹„ì§€ë„ ì´ìƒíƒì§€)\n",
    "# ==========================================\n",
    "\n",
    "def optimize_if_hyperparameters(X_train, X_val, y_val):\n",
    "    \"\"\"IsolationForest í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\"\"\"\n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    \n",
    "    param_sampler = ParameterSampler(IF_PARAM_GRID, n_iter=HYPERPARAMETER_SEARCH['search_iterations'], random_state=42)\n",
    "    \n",
    "    print(f\"IsolationForest í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘ ({HYPERPARAMETER_SEARCH['search_iterations']}íšŒ ë°˜ë³µ)\")\n",
    "    \n",
    "    for i, params in enumerate(param_sampler):\n",
    "        try:\n",
    "            print(f\"  ì¡°í•© {i+1}/{HYPERPARAMETER_SEARCH['search_iterations']}: {params}\")\n",
    "            \n",
    "            # ëª¨ë¸ í›ˆë ¨\n",
    "            if_model = IsolationForest(**params, random_state=np.random.randint(1, 1000))\n",
    "            if_model.fit(X_train)\n",
    "            \n",
    "            # ê²€ì¦ ë°ì´í„°ë¡œ í‰ê°€\n",
    "            anomaly_scores = -if_model.decision_function(X_val)  # ìŒìˆ˜ë¥¼ ì–‘ìˆ˜ë¡œ ë³€í™˜\n",
    "            \n",
    "            # G-Meanìœ¼ë¡œ í‰ê°€\n",
    "            metrics = evaluate_with_optimal_threshold(y_val, anomaly_scores)\n",
    "            score = metrics['gmean']\n",
    "            \n",
    "            print(f\"    -> G-Mean: {score:.4f}\")\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = params\n",
    "                print(f\"    âœ“ ìƒˆë¡œìš´ ìµœê³  ì ìˆ˜!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    âœ— ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if best_params is None:\n",
    "        print(\"  ëª¨ë“  ì¡°í•© ì‹¤íŒ¨. ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©.\")\n",
    "        best_params = {\n",
    "            'n_estimators': 100,\n",
    "            'contamination': 0.1,\n",
    "            'max_samples': 'auto'\n",
    "        }\n",
    "    \n",
    "    print(f\"IsolationForest ìµœì í™” ì™„ë£Œ. ìµœê³  G-Mean: {best_score:.4f}\")\n",
    "    return best_params, best_score\n",
    "\n",
    "def _train_if_single():\n",
    "    \"\"\"ë‹¨ì¼ IsolationForest ëª¨ë¸ í•™ìŠµ (ë°˜ë³µ ì‹¤í—˜ìš© ë‚´ë¶€ í•¨ìˆ˜)\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸŒ² METHOD 2: ISOLATIONFOREST í›ˆë ¨ ì‹œì‘\")\n",
    "    print(\"=\"*60)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë˜ëŠ” ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
    "        if should_optimize_hyperparameters():\n",
    "            # ê²€ì¦ ë°ì´í„° ë¶„í• \n",
    "            X_train_val, X_val, y_train_val, y_val = train_test_split(\n",
    "                X_train_normal_scaled, np.zeros(len(X_train_normal_scaled)), \n",
    "                test_size=HYPERPARAMETER_SEARCH['validation_split'], \n",
    "                random_state=np.random.randint(1, 1000)  # ë°˜ë³µ ì‹¤í—˜ë§ˆë‹¤ ë‹¤ë¥¸ ë¶„í• \n",
    "            )\n",
    "            \n",
    "            # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ fraud ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸°\n",
    "            fraud_mask = y_test == 1\n",
    "            X_fraud_val = X_test_scaled[fraud_mask][:len(X_val)//10]\n",
    "            y_fraud_val = np.ones(len(X_fraud_val))\n",
    "            \n",
    "            X_val = np.vstack([X_val, X_fraud_val])\n",
    "            y_val = np.hstack([y_val, y_fraud_val])\n",
    "            \n",
    "            print(\"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘...\")\n",
    "            best_params, best_score = optimize_if_hyperparameters(X_train_val, X_val, y_val)\n",
    "            print(f\"ìµœì  íŒŒë¼ë¯¸í„°: {best_params}\")\n",
    "        else:\n",
    "            # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
    "            best_params = get_default_params('isolation_forest')\n",
    "        \n",
    "        # ìµœì¢… ëª¨ë¸ í›ˆë ¨\n",
    "        print(\"ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì¤‘...\")\n",
    "        if_model = IsolationForest(**best_params, random_state=np.random.randint(1, 1000))\n",
    "        if_model.fit(X_train_normal_scaled)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€\n",
    "        print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€ ì¤‘...\")\n",
    "        anomaly_scores = -if_model.decision_function(X_test_scaled)\n",
    "        \n",
    "        # í‰ê°€\n",
    "        metrics = evaluate_with_optimal_threshold(y_test, anomaly_scores)\n",
    "        \n",
    "        result = {\n",
    "            'method': 'IsolationForest',\n",
    "            'type': 'Classical ML',\n",
    "            'training_time': training_time,\n",
    "            'model': if_model,\n",
    "            'best_params': best_params,\n",
    "            'anomaly_scores': anomaly_scores,\n",
    "            **metrics\n",
    "        }\n",
    "        \n",
    "        # ê²°ê³¼ ì¶œë ¥ (ë°˜ë³µ ì‹¤í—˜ ì‹œ ê°„ì†Œí™”)\n",
    "        if STATISTICAL_CONFIG['enable_repetition']:\n",
    "            print(f\"G-Mean: {metrics['gmean']:.4f}, í›ˆë ¨ì‹œê°„: {training_time:.2f}ì´ˆ\")\n",
    "        else:\n",
    "            additional_info = {\n",
    "                'í›ˆë ¨ì‹œê°„': f\"{training_time:.2f}ì´ˆ\",\n",
    "                'ëª¨ë¸ íƒ€ì…': 'Unsupervised Anomaly Detection',\n",
    "                'ìµœì  ì„ê³„ê°’': f\"{metrics['threshold']:.6f}\"\n",
    "            }\n",
    "            print_results(\"IsolationForest\", metrics, additional_info)\n",
    "        \n",
    "        print(\"âœ… IsolationForest ì™„ë£Œ\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ IsolationForest ì‹¤íŒ¨: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def train_if():\n",
    "    \"\"\"\n",
    "    IsolationForest í›ˆë ¨ (ë°˜ë³µ ì‹¤í—˜ ì§€ì›)\n",
    "    ì‹¤í—˜ì„¤ê³„.txt êµ¬í˜„: ì—¬ëŸ¬ ë²ˆì˜ ë…ë¦½ì  ì‹¤í–‰ í›„ í†µê³„ì  ìš”ì•½\n",
    "    \"\"\"\n",
    "    return run_multiple_experiments(_train_if_single, 'IsolationForest')\n",
    "\n",
    "# IsolationForest ì‹¤í–‰\n",
    "print(\"ğŸš€ IsolationForest ë°©ë²• ì‹¤í–‰ ì¤‘...\")\n",
    "if_result = train_if()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244690ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ CNN Autoencoder ë°©ë²• ì‹¤í–‰ ì¤‘...\n",
      "\n",
      "============================================================\n",
      "ğŸ§  METHOD 3: CNN AUTOENCODER í›ˆë ¨ ì‹œì‘\n",
      "============================================================\n",
      "ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
      "ë°ì´í„° í˜•íƒœ: Train (302, 30, 1), Val (76, 30, 1), Test (190, 30, 1)\n",
      "ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
      "Epoch 1/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - loss: 0.8996 - mae: 0.6834 - val_loss: 1.2733 - val_mae: 0.7316\n",
      "Epoch 2/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.9100 - mae: 0.6816 - val_loss: 1.2285 - val_mae: 0.7225\n",
      "Epoch 3/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.9203 - mae: 0.6865 - val_loss: 1.1638 - val_mae: 0.7024\n",
      "Epoch 4/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.8418 - mae: 0.6630 - val_loss: 1.0230 - val_mae: 0.6636\n",
      "Epoch 5/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.7466 - mae: 0.6327 - val_loss: 0.9095 - val_mae: 0.6277\n",
      "Epoch 6/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.7547 - mae: 0.6341 - val_loss: 0.8442 - val_mae: 0.6023\n",
      "Epoch 7/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6792 - mae: 0.5971 - val_loss: 0.7969 - val_mae: 0.5822\n",
      "Epoch 8/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.5909 - mae: 0.5693 - val_loss: 0.7331 - val_mae: 0.5586\n",
      "Epoch 9/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.6106 - mae: 0.5733 - val_loss: 0.7119 - val_mae: 0.5437\n",
      "Epoch 10/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.5629 - mae: 0.5512 - val_loss: 0.6929 - val_mae: 0.5328\n",
      "Epoch 11/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5485 - mae: 0.5484 - val_loss: 0.6668 - val_mae: 0.5199\n",
      "Epoch 12/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.5281 - mae: 0.5383 - val_loss: 0.6852 - val_mae: 0.5242\n",
      "Epoch 13/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.5022 - mae: 0.5234 - val_loss: 0.6164 - val_mae: 0.4922\n",
      "Epoch 14/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.4808 - mae: 0.5171 - val_loss: 0.6256 - val_mae: 0.4985\n",
      "Epoch 15/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.5161 - mae: 0.5163 - val_loss: 0.5903 - val_mae: 0.4803\n",
      "Epoch 16/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4637 - mae: 0.5062 - val_loss: 0.5883 - val_mae: 0.4809\n",
      "Epoch 17/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.4564 - mae: 0.5017 - val_loss: 0.5902 - val_mae: 0.4808\n",
      "Epoch 18/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.4660 - mae: 0.4998 - val_loss: 0.5727 - val_mae: 0.4690\n",
      "Epoch 19/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4482 - mae: 0.4938 - val_loss: 0.5603 - val_mae: 0.4635\n",
      "Epoch 20/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4356 - mae: 0.4840 - val_loss: 0.5455 - val_mae: 0.4588\n",
      "Epoch 21/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3976 - mae: 0.4734 - val_loss: 0.5171 - val_mae: 0.4422\n",
      "Epoch 22/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.4036 - mae: 0.4743 - val_loss: 0.5257 - val_mae: 0.4449\n",
      "Epoch 23/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.4413 - mae: 0.4831 - val_loss: 0.5377 - val_mae: 0.4528\n",
      "Epoch 24/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3876 - mae: 0.4638 - val_loss: 0.5256 - val_mae: 0.4484\n",
      "Epoch 25/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4037 - mae: 0.4648 - val_loss: 0.4977 - val_mae: 0.4288\n",
      "Epoch 26/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3724 - mae: 0.4560 - val_loss: 0.5230 - val_mae: 0.4352\n",
      "Epoch 27/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.4306 - mae: 0.4761 - val_loss: 0.4834 - val_mae: 0.4159\n",
      "Epoch 28/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3791 - mae: 0.4546 - val_loss: 0.5268 - val_mae: 0.4425\n",
      "Epoch 29/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3846 - mae: 0.4603 - val_loss: 0.4588 - val_mae: 0.4061\n",
      "Epoch 30/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3637 - mae: 0.4499 - val_loss: 0.4846 - val_mae: 0.4214\n",
      "Epoch 31/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3758 - mae: 0.4509 - val_loss: 0.4675 - val_mae: 0.4125\n",
      "Epoch 32/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3612 - mae: 0.4446 - val_loss: 0.4595 - val_mae: 0.4094\n",
      "Epoch 33/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3862 - mae: 0.4612 - val_loss: 0.4662 - val_mae: 0.4156\n",
      "Epoch 34/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3910 - mae: 0.4544 - val_loss: 0.4434 - val_mae: 0.4033\n",
      "Epoch 35/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3930 - mae: 0.4548 - val_loss: 0.4755 - val_mae: 0.4159\n",
      "Epoch 36/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3362 - mae: 0.4321 - val_loss: 0.4238 - val_mae: 0.3833\n",
      "Epoch 37/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3350 - mae: 0.4323 - val_loss: 0.4697 - val_mae: 0.4085\n",
      "Epoch 38/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3644 - mae: 0.4357 - val_loss: 0.4343 - val_mae: 0.3906\n",
      "Epoch 39/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3637 - mae: 0.4438 - val_loss: 0.4403 - val_mae: 0.3967\n",
      "Epoch 40/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3307 - mae: 0.4198 - val_loss: 0.4252 - val_mae: 0.3889\n",
      "Epoch 41/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3567 - mae: 0.4389 - val_loss: 0.4356 - val_mae: 0.3968\n",
      "Epoch 42/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3357 - mae: 0.4273 - val_loss: 0.4166 - val_mae: 0.3875\n",
      "Epoch 43/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3361 - mae: 0.4267 - val_loss: 0.4262 - val_mae: 0.3948\n",
      "Epoch 44/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3336 - mae: 0.4271 - val_loss: 0.4165 - val_mae: 0.3895\n",
      "Epoch 45/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3384 - mae: 0.4201 - val_loss: 0.4394 - val_mae: 0.4007\n",
      "Epoch 46/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3588 - mae: 0.4374 - val_loss: 0.4052 - val_mae: 0.3800\n",
      "Epoch 47/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3294 - mae: 0.4235 - val_loss: 0.4300 - val_mae: 0.3933\n",
      "Epoch 48/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3287 - mae: 0.4158 - val_loss: 0.3997 - val_mae: 0.3751\n",
      "Epoch 49/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2905 - mae: 0.4024 - val_loss: 0.3887 - val_mae: 0.3714\n",
      "Epoch 50/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3207 - mae: 0.4214 - val_loss: 0.4100 - val_mae: 0.3870\n",
      "Epoch 51/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3485 - mae: 0.4279 - val_loss: 0.3747 - val_mae: 0.3642\n",
      "Epoch 52/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3329 - mae: 0.4238 - val_loss: 0.4140 - val_mae: 0.3902\n",
      "Epoch 53/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3127 - mae: 0.4153 - val_loss: 0.3798 - val_mae: 0.3649\n",
      "Epoch 54/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.3336 - mae: 0.4269 - val_loss: 0.3902 - val_mae: 0.3746\n",
      "Epoch 55/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3130 - mae: 0.4123 - val_loss: 0.3828 - val_mae: 0.3707\n",
      "Epoch 56/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3089 - mae: 0.4090 - val_loss: 0.3758 - val_mae: 0.3673\n",
      "Epoch 57/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3311 - mae: 0.4186 - val_loss: 0.3864 - val_mae: 0.3761\n",
      "Epoch 58/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3131 - mae: 0.4121 - val_loss: 0.4226 - val_mae: 0.3968\n",
      "Epoch 59/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2882 - mae: 0.3978 - val_loss: 0.3825 - val_mae: 0.3599\n",
      "Epoch 60/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2944 - mae: 0.4042 - val_loss: 0.4109 - val_mae: 0.3809\n",
      "Epoch 61/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3177 - mae: 0.4111 - val_loss: 0.3628 - val_mae: 0.3539\n",
      "Epoch 62/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3099 - mae: 0.4021 - val_loss: 0.3740 - val_mae: 0.3657\n",
      "Epoch 63/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2890 - mae: 0.3992 - val_loss: 0.3547 - val_mae: 0.3558\n",
      "Epoch 64/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2973 - mae: 0.4017 - val_loss: 0.4178 - val_mae: 0.3956\n",
      "Epoch 65/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3243 - mae: 0.4197 - val_loss: 0.3781 - val_mae: 0.3676\n",
      "Epoch 66/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3260 - mae: 0.4162 - val_loss: 0.3859 - val_mae: 0.3778\n",
      "Epoch 67/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3013 - mae: 0.4025 - val_loss: 0.3693 - val_mae: 0.3682\n",
      "Epoch 68/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2800 - mae: 0.3911 - val_loss: 0.3771 - val_mae: 0.3716\n",
      "Epoch 69/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2809 - mae: 0.3970 - val_loss: 0.3833 - val_mae: 0.3738\n",
      "Epoch 70/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2831 - mae: 0.3929 - val_loss: 0.3656 - val_mae: 0.3598\n",
      "Epoch 71/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2980 - mae: 0.4033 - val_loss: 0.3783 - val_mae: 0.3670\n",
      "Epoch 72/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3083 - mae: 0.4049 - val_loss: 0.3633 - val_mae: 0.3637\n",
      "Epoch 73/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3158 - mae: 0.4116 - val_loss: 0.3834 - val_mae: 0.3777\n",
      "Epoch 73: early stopping\n",
      "Restoring model weights from the end of the best epoch: 63.\n",
      "í›ˆë ¨ ì™„ë£Œ: 11.24ì´ˆ\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€ ì¤‘...\n",
      "\n",
      "ğŸ“Š CNN Autoencoder ì„±ëŠ¥ ê²°ê³¼:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  ğŸ¯ AUC-ROC:  0.9483\n",
      "  âš–ï¸  ì •í™•ë„:    0.8842\n",
      "  ğŸª ì •ë°€ë„:    0.9506\n",
      "  ğŸ” ì¬í˜„ìœ¨:    0.8105\n",
      "  ğŸ† F1-Score: 0.8750\n",
      "  ğŸ“ G-Mean:   0.8811\n",
      "  ğŸ­ íŠ¹ì´ë„:    0.9579\n",
      "  ğŸ“‹ ì¶”ê°€ ì •ë³´:\n",
      "     â€¢ í›ˆë ¨ì‹œê°„: 11.24ì´ˆ\n",
      "     â€¢ ëª¨ë¸ íƒ€ì…: 1D CNN Reconstruction\n",
      "     â€¢ ìµœì  ì„ê³„ê°’: 1.501031\n",
      "     â€¢ ëª¨ë¸ íŒŒë¼ë¯¸í„°: 145,790\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ… CNN Autoencoder ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Method 3: CNN Autoencoder (1D Convolutional Neural Network)\n",
    "# ==========================================\n",
    "\n",
    "def create_cnn_model(input_shape, params):\n",
    "    \"\"\"\n",
    "    ê°œì„ ëœ CNN ëª¨ë¸ ìƒì„± í•¨ìˆ˜ (more_qubits ê¸°ë°˜)\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # ì²« ë²ˆì§¸ Conv1D - í° ì»¤ë„ë¡œ íŒ¨í„´ ì¶”ì¶œ\n",
    "        Conv1D(filters=params['filters1'], kernel_size=5, activation='relu', \n",
    "               padding='same', input_shape=input_shape),\n",
    "        Dropout(params['dropout_conv1']),\n",
    "        \n",
    "        # ë‘ ë²ˆì§¸ Conv1D - ì¤‘ê°„ í¬ê¸° ì»¤ë„\n",
    "        Conv1D(filters=params['filters2'], kernel_size=3, activation='relu', \n",
    "               padding='same'),\n",
    "        Dropout(params['dropout_conv2']),\n",
    "        \n",
    "        # ì„¸ ë²ˆì§¸ Conv1D - ì„¸ë¶€ íŠ¹ì„± ì¶”ì¶œ\n",
    "        Conv1D(filters=params['filters3'], kernel_size=3, activation='relu', \n",
    "               padding='same'),\n",
    "        \n",
    "        # Flatten í›„ Dense ë ˆì´ì–´ë“¤\n",
    "        Flatten(),\n",
    "        Dense(params['dense_units1'], activation='relu'),\n",
    "        Dropout(params['dropout_dense1']),\n",
    "        Dense(params['dense_units2'], activation='relu'),\n",
    "        Dropout(params['dropout_dense2']),\n",
    "        \n",
    "        # ì¶œë ¥ ë ˆì´ì–´ - ì…ë ¥ê³¼ ë™ì¼í•œ í¬ê¸°ë¡œ ì¬êµ¬ì„±\n",
    "        Dense(input_shape[0], activation='linear')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=params['learning_rate']),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def calculate_reconstruction_error(model, X_data, X_original):\n",
    "    \"\"\"\n",
    "    ì¬êµ¬ì„± ì˜¤ì°¨ ê³„ì‚° í•¨ìˆ˜ (í˜•íƒœ ë§ì¶¤)\n",
    "    \"\"\"\n",
    "    # ëª¨ë¸ ì˜ˆì¸¡ (ì¬êµ¬ì„±)\n",
    "    X_reconstructed = model.predict(X_data, verbose=0)\n",
    "    \n",
    "    # MSE ê³„ì‚° (2D í˜•íƒœë¡œ ë¹„êµ)\n",
    "    reconstruction_errors = np.mean(np.square(X_original - X_reconstructed), axis=1)\n",
    "    \n",
    "    return reconstruction_errors\n",
    "\n",
    "def optimize_cnn_hyperparameters(X_train, X_val, y_val):\n",
    "    \"\"\"CNN í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\"\"\"\n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    \n",
    "    print(f\"CNN í•˜ì´í¼íŒŒë¼ë¯¸í„° ëœë¤ ì„œì¹˜ ì‹œì‘ ({HYPERPARAMETER_SEARCH['search_iterations']}íšŒ ë°˜ë³µ)\")\n",
    "    \n",
    "    # ëœë¤ ì„œì¹˜ë¡œ íŒŒë¼ë¯¸í„° ì¡°í•© ìƒì„±\n",
    "    param_sampler = ParameterSampler(\n",
    "        CNN_PARAM_GRID, \n",
    "        n_iter=HYPERPARAMETER_SEARCH['search_iterations'], \n",
    "        random_state=np.random.randint(1, 1000)\n",
    "    )\n",
    "    \n",
    "    for i, params in enumerate(param_sampler):\n",
    "        try:\n",
    "            print(f\"  ì¡°í•© {i+1}/{HYPERPARAMETER_SEARCH['search_iterations']}: {params}\")\n",
    "            \n",
    "            # CNNìš© 3D í˜•íƒœë¡œ ë³€í™˜\n",
    "            X_train_reshaped = X_train.reshape(-1, X_train.shape[1], 1)\n",
    "            X_val_reshaped = X_val.reshape(-1, X_val.shape[1], 1)\n",
    "            \n",
    "            # ëª¨ë¸ ìƒì„±\n",
    "            input_shape = (X_train.shape[1], 1)\n",
    "            model = create_cnn_model(input_shape, params)\n",
    "            \n",
    "            # ì¡°ê¸° ì¢…ë£Œ ì„¤ì •\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor='loss',\n",
    "                patience=5,\n",
    "                restore_best_weights=True,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # ë¹ ë¥¸ ê²€ì¦ í›ˆë ¨ (15 ì—í¬í¬)\n",
    "            model.fit(\n",
    "                X_train_reshaped, \n",
    "                X_train,  # 2D íƒ€ê²Ÿ\n",
    "                epochs=15,  # ê²€ì¦ìš© ë‹¨ì¶• ì—í¬í¬\n",
    "                batch_size=EXPERIMENTAL_CONFIG['batch_size'],\n",
    "                callbacks=[early_stopping],\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # ê²€ì¦ ë°ì´í„°ë¡œ í‰ê°€\n",
    "            reconstruction_errors = calculate_reconstruction_error(model, X_val_reshaped, X_val)\n",
    "            \n",
    "            # G-Meanìœ¼ë¡œ í‰ê°€\n",
    "            metrics = evaluate_with_optimal_threshold(y_val, reconstruction_errors)\n",
    "            score = metrics['gmean']\n",
    "            \n",
    "            print(f\"    -> G-Mean: {score:.4f}\")\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = params\n",
    "                print(f\"    âœ“ ìƒˆë¡œìš´ ìµœê³  ì ìˆ˜!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    âœ— ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if best_params is None:\n",
    "        print(\"  ëª¨ë“  ì¡°í•© ì‹¤íŒ¨. ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©.\")\n",
    "        best_params = {\n",
    "            'learning_rate': 0.001,\n",
    "            'filters1': 32,\n",
    "            'filters2': 64,\n",
    "            'filters3': 32,\n",
    "            'dropout_conv1': 0.2,\n",
    "            'dropout_conv2': 0.2,\n",
    "            'dense_units1': 128,\n",
    "            'dense_units2': 64,\n",
    "            'dropout_dense1': 0.3,\n",
    "            'dropout_dense2': 0.2\n",
    "        }\n",
    "    \n",
    "    print(f\"CNN ìµœì í™” ì™„ë£Œ. ìµœê³  G-Mean: {best_score:.4f}\")\n",
    "    return best_params, best_score\n",
    "\n",
    "def _train_cnn_single():\n",
    "    \"\"\"ë‹¨ì¼ CNN ëª¨ë¸ í•™ìŠµ (ë°˜ë³µ ì‹¤í—˜ìš© ë‚´ë¶€ í•¨ìˆ˜)\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ§  METHOD 3: CNN AUTOENCODER í›ˆë ¨ ì‹œì‘\")\n",
    "    print(\"=\"*60)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # ì •ìƒ ë°ì´í„°ë§Œìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§\n",
    "        normal_scaler = StandardScaler()\n",
    "        X_train_normal_rescaled = normal_scaler.fit_transform(X_train_normal)\n",
    "        X_test_rescaled = normal_scaler.transform(X_test)\n",
    "        \n",
    "        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë˜ëŠ” ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
    "        if should_optimize_hyperparameters():\n",
    "            # ê²€ì¦ ë°ì´í„° ë¶„í• \n",
    "            X_train_val, X_val_normal = train_test_split(\n",
    "                X_train_normal_rescaled, test_size=0.2, \n",
    "                random_state=np.random.randint(1, 1000)\n",
    "            )\n",
    "            \n",
    "            # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ fraud ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸° (ì†ŒëŸ‰)\n",
    "            fraud_mask = y_test == 1\n",
    "            fraud_samples = X_test_rescaled[fraud_mask]\n",
    "            n_fraud_val = min(len(fraud_samples), len(X_val_normal) // 10)\n",
    "            X_fraud_val = fraud_samples[:n_fraud_val]\n",
    "            \n",
    "            X_val = np.vstack([X_val_normal, X_fraud_val])\n",
    "            y_val = np.hstack([np.zeros(len(X_val_normal)), np.ones(len(X_fraud_val))])\n",
    "            \n",
    "            print(\"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘...\")\n",
    "            best_params, best_score = optimize_cnn_hyperparameters(X_train_val, X_val, y_val)\n",
    "            print(f\"ìµœì  íŒŒë¼ë¯¸í„° ì„ íƒ ì™„ë£Œ. ìµœê³  G-Mean: {best_score:.4f}\")\n",
    "        else:\n",
    "            # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
    "            best_params = get_default_params('cnn_autoencoder')\n",
    "        \n",
    "        # Train/Validation ìµœì¢… ë¶„í• \n",
    "        X_train_split, X_val_split = train_test_split(\n",
    "            X_train_normal_rescaled, test_size=0.2, \n",
    "            random_state=np.random.randint(1, 1000)\n",
    "        )\n",
    "        \n",
    "        # CNNìš© 3D í˜•íƒœë¡œ ë³€í™˜\n",
    "        X_train_reshaped = X_train_split.reshape(-1, X_train_split.shape[1], 1)\n",
    "        X_val_reshaped = X_val_split.reshape(-1, X_val_split.shape[1], 1)\n",
    "        X_test_reshaped = X_test_rescaled.reshape(-1, X_test_rescaled.shape[1], 1)\n",
    "        \n",
    "        print(f\"ë°ì´í„° í˜•íƒœ: Train {X_train_reshaped.shape}, Val {X_val_reshaped.shape}, Test {X_test_reshaped.shape}\")\n",
    "        \n",
    "        # ìµœì¢… ëª¨ë¸ ìƒì„± ë° í›ˆë ¨\n",
    "        print(\"ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì¤‘...\")\n",
    "        input_shape = (X_train_split.shape[1], 1)\n",
    "        model = create_cnn_model(input_shape, best_params)\n",
    "        \n",
    "        # ì¡°ê¸° ì¢…ë£Œ ì„¤ì •\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=0 if STATISTICAL_CONFIG['enable_repetition'] else 1\n",
    "        )\n",
    "        \n",
    "        # ëª¨ë¸ í•™ìŠµ\n",
    "        history = model.fit(\n",
    "            X_train_reshaped, \n",
    "            X_train_split,  # 2D íƒ€ê²Ÿ\n",
    "            epochs=EXPERIMENTAL_CONFIG['max_epochs'],\n",
    "            batch_size=EXPERIMENTAL_CONFIG['batch_size'],\n",
    "            validation_data=(X_val_reshaped, X_val_split),\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=0 if STATISTICAL_CONFIG['enable_repetition'] else 1\n",
    "        )\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"í›ˆë ¨ ì™„ë£Œ: {training_time:.2f}ì´ˆ\")\n",
    "        \n",
    "        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€\n",
    "        print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€ ì¤‘...\")\n",
    "        reconstruction_errors = calculate_reconstruction_error(model, X_test_reshaped, X_test_rescaled)\n",
    "        \n",
    "        # í‰ê°€\n",
    "        metrics = evaluate_with_optimal_threshold(y_test, reconstruction_errors)\n",
    "        \n",
    "        result = {\n",
    "            'method': 'CNN Autoencoder',\n",
    "            'type': 'Deep Learning',\n",
    "            'training_time': training_time,\n",
    "            'model': model,\n",
    "            'history': history,\n",
    "            'scaler': normal_scaler,\n",
    "            'best_params': best_params,\n",
    "            'reconstruction_errors': reconstruction_errors,\n",
    "            **metrics\n",
    "        }\n",
    "        \n",
    "        # ê²°ê³¼ ì¶œë ¥ (ë°˜ë³µ ì‹¤í—˜ ì‹œ ê°„ì†Œí™”)\n",
    "        if STATISTICAL_CONFIG['enable_repetition']:\n",
    "            print(f\"G-Mean: {metrics['gmean']:.4f}, í›ˆë ¨ì‹œê°„: {training_time:.2f}ì´ˆ\")\n",
    "        else:\n",
    "            additional_info = {\n",
    "                'í›ˆë ¨ì‹œê°„': f\"{training_time:.2f}ì´ˆ\",\n",
    "                'ëª¨ë¸ íƒ€ì…': '1D CNN Reconstruction',\n",
    "                'ìµœì  ì„ê³„ê°’': f\"{metrics['threshold']:.6f}\",\n",
    "                'ëª¨ë¸ íŒŒë¼ë¯¸í„°': f\"{model.count_params():,}\"\n",
    "            }\n",
    "            print_results(\"CNN Autoencoder\", metrics, additional_info)\n",
    "        \n",
    "        print(\"âœ… CNN Autoencoder ì™„ë£Œ\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ CNN Autoencoder ì‹¤íŒ¨: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def train_cnn():\n",
    "    \"\"\"\n",
    "    CNN Autoencoder í›ˆë ¨ (ë°˜ë³µ ì‹¤í—˜ ì§€ì›)\n",
    "    ì‹¤í—˜ì„¤ê³„.txt êµ¬í˜„: ì—¬ëŸ¬ ë²ˆì˜ ë…ë¦½ì  ì‹¤í–‰ í›„ í†µê³„ì  ìš”ì•½\n",
    "    \"\"\"\n",
    "    return run_multiple_experiments(_train_cnn_single, 'CNN Autoencoder')\n",
    "\n",
    "# CNN Autoencoder ì‹¤í–‰\n",
    "print(\"ğŸš€ CNN Autoencoder ë°©ë²• ì‹¤í–‰ ì¤‘...\")\n",
    "cnn_result = train_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53367fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Classical Autoencoder ë°©ë²• ì‹¤í–‰ ì¤‘...\n",
      "\n",
      "============================================================\n",
      "ğŸ¤– METHOD 4: CLASSICAL AUTOENCODER í›ˆë ¨ ì‹œì‘\n",
      "============================================================\n",
      "ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
      "Epoch 1/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - loss: 0.4881 - mae: 0.5324 - val_loss: 0.5659 - val_mae: 0.5350\n",
      "Epoch 2/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4905 - mae: 0.5027 - val_loss: 0.5067 - val_mae: 0.4703\n",
      "Epoch 3/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.4413 - mae: 0.4579 - val_loss: 0.4682 - val_mae: 0.4319\n",
      "Epoch 4/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.4411 - mae: 0.4296 - val_loss: 0.4500 - val_mae: 0.4129\n",
      "Epoch 5/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3684 - mae: 0.4045 - val_loss: 0.4389 - val_mae: 0.4017\n",
      "Epoch 6/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4133 - mae: 0.4079 - val_loss: 0.4359 - val_mae: 0.3989\n",
      "Epoch 7/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3498 - mae: 0.3931 - val_loss: 0.4266 - val_mae: 0.3883\n",
      "Epoch 8/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.4120 - mae: 0.3974 - val_loss: 0.4214 - val_mae: 0.3866\n",
      "Epoch 9/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3841 - mae: 0.3935 - val_loss: 0.4140 - val_mae: 0.3784\n",
      "Epoch 10/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3822 - mae: 0.3934 - val_loss: 0.4078 - val_mae: 0.3736\n",
      "Epoch 11/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3544 - mae: 0.3891 - val_loss: 0.3989 - val_mae: 0.3680\n",
      "Epoch 12/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3865 - mae: 0.3909 - val_loss: 0.3955 - val_mae: 0.3688\n",
      "Epoch 13/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3409 - mae: 0.3772 - val_loss: 0.3874 - val_mae: 0.3599\n",
      "Epoch 14/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3181 - mae: 0.3685 - val_loss: 0.3851 - val_mae: 0.3602\n",
      "Epoch 15/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3388 - mae: 0.3714 - val_loss: 0.3777 - val_mae: 0.3534\n",
      "Epoch 16/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3128 - mae: 0.3668 - val_loss: 0.3732 - val_mae: 0.3510\n",
      "Epoch 17/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3152 - mae: 0.3686 - val_loss: 0.3693 - val_mae: 0.3479\n",
      "Epoch 18/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2943 - mae: 0.3564 - val_loss: 0.3674 - val_mae: 0.3466\n",
      "Epoch 19/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2885 - mae: 0.3573 - val_loss: 0.3626 - val_mae: 0.3422\n",
      "Epoch 20/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2986 - mae: 0.3582 - val_loss: 0.3601 - val_mae: 0.3396\n",
      "Epoch 21/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2741 - mae: 0.3507 - val_loss: 0.3570 - val_mae: 0.3376\n",
      "Epoch 22/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3030 - mae: 0.3597 - val_loss: 0.3574 - val_mae: 0.3372\n",
      "Epoch 23/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2954 - mae: 0.3559 - val_loss: 0.3561 - val_mae: 0.3349\n",
      "Epoch 24/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2991 - mae: 0.3560 - val_loss: 0.3548 - val_mae: 0.3314\n",
      "Epoch 25/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2661 - mae: 0.3407 - val_loss: 0.3523 - val_mae: 0.3326\n",
      "Epoch 26/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3015 - mae: 0.3624 - val_loss: 0.3500 - val_mae: 0.3305\n",
      "Epoch 27/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2505 - mae: 0.3379 - val_loss: 0.3477 - val_mae: 0.3267\n",
      "Epoch 28/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2711 - mae: 0.3417 - val_loss: 0.3494 - val_mae: 0.3289\n",
      "Epoch 29/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2638 - mae: 0.3414 - val_loss: 0.3489 - val_mae: 0.3275\n",
      "Epoch 30/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2491 - mae: 0.3349 - val_loss: 0.3482 - val_mae: 0.3269\n",
      "Epoch 31/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2459 - mae: 0.3349 - val_loss: 0.3462 - val_mae: 0.3234\n",
      "Epoch 32/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2556 - mae: 0.3351 - val_loss: 0.3440 - val_mae: 0.3218\n",
      "Epoch 33/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2740 - mae: 0.3394 - val_loss: 0.3482 - val_mae: 0.3230\n",
      "Epoch 34/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2475 - mae: 0.3287 - val_loss: 0.3449 - val_mae: 0.3199\n",
      "Epoch 35/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2497 - mae: 0.3304 - val_loss: 0.3413 - val_mae: 0.3209\n",
      "Epoch 36/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2483 - mae: 0.3330 - val_loss: 0.3374 - val_mae: 0.3177\n",
      "Epoch 37/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2585 - mae: 0.3360 - val_loss: 0.3378 - val_mae: 0.3183\n",
      "Epoch 38/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2497 - mae: 0.3341 - val_loss: 0.3380 - val_mae: 0.3197\n",
      "Epoch 39/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2379 - mae: 0.3215 - val_loss: 0.3382 - val_mae: 0.3190\n",
      "Epoch 40/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2446 - mae: 0.3275 - val_loss: 0.3328 - val_mae: 0.3174\n",
      "Epoch 41/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2604 - mae: 0.3338 - val_loss: 0.3311 - val_mae: 0.3143\n",
      "Epoch 42/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2234 - mae: 0.3198 - val_loss: 0.3303 - val_mae: 0.3100\n",
      "Epoch 43/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2340 - mae: 0.3201 - val_loss: 0.3297 - val_mae: 0.3115\n",
      "Epoch 44/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2249 - mae: 0.3174 - val_loss: 0.3289 - val_mae: 0.3110\n",
      "Epoch 45/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2313 - mae: 0.3195 - val_loss: 0.3279 - val_mae: 0.3086\n",
      "Epoch 46/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2280 - mae: 0.3177 - val_loss: 0.3304 - val_mae: 0.3113\n",
      "Epoch 47/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2397 - mae: 0.3192 - val_loss: 0.3327 - val_mae: 0.3118\n",
      "Epoch 48/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2274 - mae: 0.3171 - val_loss: 0.3284 - val_mae: 0.3084\n",
      "Epoch 49/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2196 - mae: 0.3140 - val_loss: 0.3275 - val_mae: 0.3088\n",
      "Epoch 50/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2197 - mae: 0.3105 - val_loss: 0.3316 - val_mae: 0.3095\n",
      "Epoch 51/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2175 - mae: 0.3105 - val_loss: 0.3303 - val_mae: 0.3094\n",
      "Epoch 52/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2366 - mae: 0.3162 - val_loss: 0.3259 - val_mae: 0.3061\n",
      "Epoch 53/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2102 - mae: 0.3141 - val_loss: 0.3255 - val_mae: 0.3050\n",
      "Epoch 54/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2078 - mae: 0.3055 - val_loss: 0.3268 - val_mae: 0.3067\n",
      "Epoch 55/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2297 - mae: 0.3160 - val_loss: 0.3232 - val_mae: 0.3037\n",
      "Epoch 56/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2247 - mae: 0.3115 - val_loss: 0.3197 - val_mae: 0.3012\n",
      "Epoch 57/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2235 - mae: 0.3125 - val_loss: 0.3247 - val_mae: 0.3038\n",
      "Epoch 58/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2228 - mae: 0.3117 - val_loss: 0.3255 - val_mae: 0.3039\n",
      "Epoch 59/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2078 - mae: 0.3033 - val_loss: 0.3295 - val_mae: 0.3068\n",
      "Epoch 60/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2121 - mae: 0.3094 - val_loss: 0.3253 - val_mae: 0.3051\n",
      "Epoch 61/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2128 - mae: 0.3100 - val_loss: 0.3263 - val_mae: 0.3043\n",
      "Epoch 62/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2032 - mae: 0.3043 - val_loss: 0.3242 - val_mae: 0.3014\n",
      "Epoch 63/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2179 - mae: 0.3087 - val_loss: 0.3225 - val_mae: 0.3009\n",
      "Epoch 64/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2254 - mae: 0.3093 - val_loss: 0.3220 - val_mae: 0.3016\n",
      "Epoch 65/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2221 - mae: 0.3111 - val_loss: 0.3239 - val_mae: 0.3028\n",
      "Epoch 66/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2352 - mae: 0.3200 - val_loss: 0.3246 - val_mae: 0.3015\n",
      "Epoch 67/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2078 - mae: 0.2989 - val_loss: 0.3227 - val_mae: 0.3003\n",
      "Epoch 68/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2102 - mae: 0.3068 - val_loss: 0.3211 - val_mae: 0.2987\n",
      "Epoch 69/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2114 - mae: 0.3081 - val_loss: 0.3196 - val_mae: 0.2969\n",
      "Epoch 70/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2021 - mae: 0.2983 - val_loss: 0.3235 - val_mae: 0.3008\n",
      "Epoch 71/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2149 - mae: 0.3030 - val_loss: 0.3225 - val_mae: 0.3019\n",
      "Epoch 72/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2080 - mae: 0.3011 - val_loss: 0.3208 - val_mae: 0.2978\n",
      "Epoch 73/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2337 - mae: 0.3109 - val_loss: 0.3210 - val_mae: 0.2987\n",
      "Epoch 74/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2043 - mae: 0.3010 - val_loss: 0.3209 - val_mae: 0.2981\n",
      "Epoch 75/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2194 - mae: 0.3083 - val_loss: 0.3200 - val_mae: 0.2984\n",
      "Epoch 76/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2080 - mae: 0.3060 - val_loss: 0.3215 - val_mae: 0.3005\n",
      "Epoch 77/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1990 - mae: 0.2979 - val_loss: 0.3213 - val_mae: 0.2979\n",
      "Epoch 78/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2167 - mae: 0.3073 - val_loss: 0.3210 - val_mae: 0.2979\n",
      "Epoch 79/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2058 - mae: 0.3011 - val_loss: 0.3192 - val_mae: 0.2976\n",
      "Epoch 80/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2128 - mae: 0.2999 - val_loss: 0.3248 - val_mae: 0.2996\n",
      "Epoch 81/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2026 - mae: 0.2939 - val_loss: 0.3224 - val_mae: 0.2993\n",
      "Epoch 82/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2180 - mae: 0.3062 - val_loss: 0.3204 - val_mae: 0.2976\n",
      "Epoch 83/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1968 - mae: 0.3008 - val_loss: 0.3208 - val_mae: 0.2984\n",
      "Epoch 84/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1986 - mae: 0.2969 - val_loss: 0.3181 - val_mae: 0.2968\n",
      "Epoch 85/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1965 - mae: 0.2943 - val_loss: 0.3157 - val_mae: 0.2953\n",
      "Epoch 86/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1951 - mae: 0.2914 - val_loss: 0.3214 - val_mae: 0.2987\n",
      "Epoch 87/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2160 - mae: 0.3026 - val_loss: 0.3153 - val_mae: 0.2948\n",
      "Epoch 88/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1994 - mae: 0.2961 - val_loss: 0.3160 - val_mae: 0.2957\n",
      "Epoch 89/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2144 - mae: 0.3021 - val_loss: 0.3175 - val_mae: 0.2971\n",
      "Epoch 90/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1817 - mae: 0.2906 - val_loss: 0.3153 - val_mae: 0.2936\n",
      "Epoch 91/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1960 - mae: 0.2939 - val_loss: 0.3161 - val_mae: 0.2949\n",
      "Epoch 92/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1830 - mae: 0.2894 - val_loss: 0.3149 - val_mae: 0.2932\n",
      "Epoch 93/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1825 - mae: 0.2884 - val_loss: 0.3151 - val_mae: 0.2925\n",
      "Epoch 94/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1853 - mae: 0.2901 - val_loss: 0.3161 - val_mae: 0.2943\n",
      "Epoch 95/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1970 - mae: 0.2943 - val_loss: 0.3178 - val_mae: 0.2964\n",
      "Epoch 96/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1927 - mae: 0.2938 - val_loss: 0.3126 - val_mae: 0.2907\n",
      "Epoch 97/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2090 - mae: 0.3030 - val_loss: 0.3144 - val_mae: 0.2929\n",
      "Epoch 98/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1945 - mae: 0.2940 - val_loss: 0.3140 - val_mae: 0.2935\n",
      "Epoch 99/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1936 - mae: 0.2956 - val_loss: 0.3126 - val_mae: 0.2907\n",
      "Epoch 100/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1967 - mae: 0.2959 - val_loss: 0.3112 - val_mae: 0.2895\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€ ì¤‘...\n",
      "\n",
      "ğŸ“Š Classical Autoencoder ì„±ëŠ¥ ê²°ê³¼:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  ğŸ¯ AUC-ROC:  0.9203\n",
      "  âš–ï¸  ì •í™•ë„:    0.8684\n",
      "  ğŸª ì •ë°€ë„:    0.8500\n",
      "  ğŸ” ì¬í˜„ìœ¨:    0.8947\n",
      "  ğŸ† F1-Score: 0.8718\n",
      "  ğŸ“ G-Mean:   0.8680\n",
      "  ğŸ­ íŠ¹ì´ë„:    0.8421\n",
      "  ğŸ“‹ ì¶”ê°€ ì •ë³´:\n",
      "     â€¢ í›ˆë ¨ì‹œê°„: 11.12ì´ˆ\n",
      "     â€¢ ëª¨ë¸ íƒ€ì…: Dense Layer Autoencoder\n",
      "     â€¢ ìµœì  ì„ê³„ê°’: 0.281153\n",
      "     â€¢ ëª¨ë¸ íŒŒë¼ë¯¸í„°: 14,414\n",
      "     â€¢ ì¸ì½”ë”© ì°¨ì›: 16\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ… Classical Autoencoder ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Method 4: Classical Autoencoder (ì „í†µì  ì‹ ê²½ë§ ì˜¤í† ì¸ì½”ë”)\n",
    "# ==========================================\n",
    "\n",
    "def create_autoencoder(input_dim, params):\n",
    "    \"\"\"Classical Autoencoder ëª¨ë¸ ìƒì„± (more_qubits ê¸°ë°˜)\"\"\"\n",
    "    encoding_dim = params['encoding_dim']\n",
    "    \n",
    "    # ì¸ì½”ë”\n",
    "    encoder_layers = [Dense(encoding_dim * 4, activation='relu', input_shape=(input_dim,))]\n",
    "    \n",
    "    # íˆë“  ë ˆì´ì–´ ì¶”ê°€\n",
    "    for i in range(params['hidden_layers']):\n",
    "        encoder_layers.extend([\n",
    "            Dense(encoding_dim * 2, activation='relu'),\n",
    "            Dropout(params['dropout_rate'])\n",
    "        ])\n",
    "    \n",
    "    encoder_layers.append(Dense(encoding_dim, activation='relu'))\n",
    "    \n",
    "    # ë””ì½”ë”\n",
    "    decoder_layers = [Dense(encoding_dim * 2, activation='relu')]\n",
    "    \n",
    "    for i in range(params['hidden_layers']):\n",
    "        decoder_layers.extend([\n",
    "            Dense(encoding_dim * 4, activation='relu'),\n",
    "            Dropout(params['dropout_rate'])\n",
    "        ])\n",
    "    \n",
    "    decoder_layers.append(Dense(input_dim, activation='linear'))\n",
    "    \n",
    "    # ëª¨ë¸ ìƒì„±\n",
    "    model = Sequential(encoder_layers + decoder_layers)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=params['learning_rate']),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def optimize_ae_hyperparameters(X_train, X_val, y_val):\n",
    "    \"\"\"Autoencoder í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\"\"\"\n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    \n",
    "    param_sampler = ParameterSampler(AE_PARAM_GRID, n_iter=HYPERPARAMETER_SEARCH['search_iterations'], \n",
    "                                   random_state=np.random.randint(1, 1000))\n",
    "    \n",
    "    print(f\"Classical Autoencoder í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘ ({HYPERPARAMETER_SEARCH['search_iterations']}íšŒ ë°˜ë³µ)\")\n",
    "    \n",
    "    for i, params in enumerate(param_sampler):\n",
    "        try:\n",
    "            print(f\"  ì¡°í•© {i+1}/{HYPERPARAMETER_SEARCH['search_iterations']}: {params}\")\n",
    "            \n",
    "            # ëª¨ë¸ ìƒì„±\n",
    "            model = create_autoencoder(X_train.shape[1], params)\n",
    "            \n",
    "            # í•™ìŠµ\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=10,\n",
    "                restore_best_weights=True,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            model.fit(\n",
    "                X_train,\n",
    "                X_train,  # ìê¸° ìì‹ ì„ ì¬êµ¬ì„±\n",
    "                epochs=30,\n",
    "                batch_size=EXPERIMENTAL_CONFIG['batch_size'],\n",
    "                validation_split=0.2,\n",
    "                callbacks=[early_stopping],\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # ê²€ì¦ ë°ì´í„°ë¡œ í‰ê°€\n",
    "            reconstructions = model.predict(X_val, verbose=0)\n",
    "            reconstruction_errors = np.mean(np.square(X_val - reconstructions), axis=1)\n",
    "            \n",
    "            # í‰ê°€\n",
    "            metrics = evaluate_with_optimal_threshold(y_val, reconstruction_errors)\n",
    "            score = metrics['gmean']\n",
    "            \n",
    "            print(f\"    -> G-Mean: {score:.4f}\")\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = params\n",
    "                print(f\"    âœ“ ìƒˆë¡œìš´ ìµœê³  ì ìˆ˜!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    âœ— ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if best_params is None:\n",
    "        print(\"  ëª¨ë“  ì¡°í•© ì‹¤íŒ¨. ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©.\")\n",
    "        best_params = {\n",
    "            'learning_rate': 0.001,\n",
    "            'encoding_dim': 16,\n",
    "            'hidden_layers': 2,\n",
    "            'dropout_rate': 0.2\n",
    "        }\n",
    "    \n",
    "    print(f\"Classical Autoencoder ìµœì í™” ì™„ë£Œ. ìµœê³  G-Mean: {best_score:.4f}\")\n",
    "    return best_params, best_score\n",
    "\n",
    "def _train_ae_single():\n",
    "    \"\"\"ë‹¨ì¼ Classical Autoencoder ëª¨ë¸ í•™ìŠµ (ë°˜ë³µ ì‹¤í—˜ìš© ë‚´ë¶€ í•¨ìˆ˜)\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ¤– METHOD 4: CLASSICAL AUTOENCODER í›ˆë ¨ ì‹œì‘\")\n",
    "    print(\"=\"*60)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë˜ëŠ” ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
    "        if should_optimize_hyperparameters():\n",
    "            # ê²€ì¦ ë°ì´í„° ë¶„í• \n",
    "            X_train_val, X_val_normal = train_test_split(\n",
    "                X_train_normal_scaled, test_size=HYPERPARAMETER_SEARCH['validation_split'], \n",
    "                random_state=np.random.randint(1, 1000)\n",
    "            )\n",
    "            \n",
    "            # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ fraud ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸°\n",
    "            fraud_mask = y_test == 1\n",
    "            X_fraud_val = X_test_scaled[fraud_mask][:len(X_val_normal)//10]\n",
    "            \n",
    "            X_val = np.vstack([X_val_normal, X_fraud_val])\n",
    "            y_val = np.hstack([np.zeros(len(X_val_normal)), np.ones(len(X_fraud_val))])\n",
    "            \n",
    "            print(\"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘...\")\n",
    "            best_params, best_score = optimize_ae_hyperparameters(X_train_val, X_val, y_val)\n",
    "            print(f\"ìµœì  íŒŒë¼ë¯¸í„°: {best_params}\")\n",
    "        else:\n",
    "            # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
    "            best_params = get_default_params('classical_autoencoder')\n",
    "        \n",
    "        # ìµœì¢… ëª¨ë¸ í›ˆë ¨\n",
    "        print(\"ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì¤‘...\")\n",
    "        model = create_autoencoder(X_train_normal_scaled.shape[1], best_params)\n",
    "        \n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='loss',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train_normal_scaled,\n",
    "            X_train_normal_scaled,\n",
    "            epochs=EXPERIMENTAL_CONFIG['max_epochs'],\n",
    "            batch_size=EXPERIMENTAL_CONFIG['batch_size'],\n",
    "            validation_split=0.1,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=0 if STATISTICAL_CONFIG['enable_repetition'] else 1\n",
    "        )\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€\n",
    "        print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€ ì¤‘...\")\n",
    "        reconstructions = model.predict(X_test_scaled, verbose=0)\n",
    "        reconstruction_errors = np.mean(np.square(X_test_scaled - reconstructions), axis=1)\n",
    "        \n",
    "        # í‰ê°€\n",
    "        metrics = evaluate_with_optimal_threshold(y_test, reconstruction_errors)\n",
    "        \n",
    "        result = {\n",
    "            'method': 'Classical Autoencoder',\n",
    "            'type': 'Deep Learning',\n",
    "            'training_time': training_time,\n",
    "            'model': model,\n",
    "            'history': history,\n",
    "            'best_params': best_params,\n",
    "            'reconstruction_errors': reconstruction_errors,\n",
    "            **metrics\n",
    "        }\n",
    "        \n",
    "        # ê²°ê³¼ ì¶œë ¥ (ë°˜ë³µ ì‹¤í—˜ ì‹œ ê°„ì†Œí™”)\n",
    "        if STATISTICAL_CONFIG['enable_repetition']:\n",
    "            print(f\"G-Mean: {metrics['gmean']:.4f}, í›ˆë ¨ì‹œê°„: {training_time:.2f}ì´ˆ\")\n",
    "        else:\n",
    "            additional_info = {\n",
    "                'í›ˆë ¨ì‹œê°„': f\"{training_time:.2f}ì´ˆ\",\n",
    "                'ëª¨ë¸ íƒ€ì…': 'Dense Layer Autoencoder',\n",
    "                'ìµœì  ì„ê³„ê°’': f\"{metrics['threshold']:.6f}\",\n",
    "                'ëª¨ë¸ íŒŒë¼ë¯¸í„°': f\"{model.count_params():,}\",\n",
    "                'ì¸ì½”ë”© ì°¨ì›': f\"{best_params['encoding_dim']}\"\n",
    "            }\n",
    "            print_results(\"Classical Autoencoder\", metrics, additional_info)\n",
    "        \n",
    "        print(\"âœ… Classical Autoencoder ì™„ë£Œ\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Classical Autoencoder ì‹¤íŒ¨: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def train_ae():\n",
    "    \"\"\"\n",
    "    Classical Autoencoder í›ˆë ¨ (ë°˜ë³µ ì‹¤í—˜ ì§€ì›)\n",
    "    ì‹¤í—˜ì„¤ê³„.txt êµ¬í˜„: ì—¬ëŸ¬ ë²ˆì˜ ë…ë¦½ì  ì‹¤í–‰ í›„ í†µê³„ì  ìš”ì•½\n",
    "    \"\"\"\n",
    "    return run_multiple_experiments(_train_ae_single, 'Classical Autoencoder')\n",
    "\n",
    "# Classical Autoencoder ì‹¤í–‰\n",
    "print(\"ğŸš€ Classical Autoencoder ë°©ë²• ì‹¤í–‰ ì¤‘...\")\n",
    "ae_result = train_ae()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffde29f8",
   "metadata": {},
   "source": [
    "# âš›ï¸ Quantum Machine Learning Methods (ì–‘ì ë°©ë²• 4ê°€ì§€)\n",
    "\n",
    "ì´ ì„¹ì…˜ì—ì„œëŠ” **more_qubits íŒŒì¼ì˜ ê²€ì¦ëœ 2ê°€ì§€ ë°©ë²•**ê³¼ **NewFileì˜ í˜ì‹ ì ì¸ 2ê°€ì§€ ë°©ë²•**ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ“‹ êµ¬í˜„ ìˆœì„œ\n",
    "\n",
    "### 5. **QAE Angle** - ê°ë„ ì„ë² ë”© ê¸°ë°˜ ì–‘ì ì˜¤í† ì¸ì½”ë” *(more_qubits ê¸°ë°˜)*\n",
    "- **ì›ë¦¬**: ì…ë ¥ íŠ¹ì„±ì„ RY íšŒì „ê°ìœ¼ë¡œ ì„ë² ë”©í•˜ì—¬ ì–‘ì ìƒíƒœ ìƒì„±\n",
    "- **ì•„í‚¤í…ì²˜**: ë³€ë¶„ ë ˆì´ì–´ (RY, RZ, RX + CNOT ì–½í˜)\n",
    "- **ì¸¡ì •**: PauliZ ê¸°ëŒ“ê°’ìœ¼ë¡œ ì¶©ì‹¤ë„ ì¶”ì •\n",
    "- **íë¹„íŠ¸**: 4ê°œ (PCA ì°¨ì›ê³¼ ë™ì¼)\n",
    "\n",
    "### 6. **Enhanced qVAE** - ê³ ê¸‰ ì–‘ì ë³€ë¶„ ì˜¤í† ì¸ì½”ë” *(more_qubits ê¸°ë°˜)*\n",
    "- **ì›ë¦¬**: ë°ì´í„° ì¬ì—…ë¡œë”©, ë³‘ë ¬ ì„ë² ë”©, SWAP í…ŒìŠ¤íŠ¸ ë“± ê³ ê¸‰ ê¸°ë²• ì ìš©\n",
    "- **ê³ ê¸‰ ê¸°ëŠ¥**:\n",
    "  - ğŸ“¡ **ë°ì´í„° ì¬ì—…ë¡œë”©**: ê° ë³€ë¶„ ë ˆì´ì–´ì—ì„œ ë°ì´í„° ì¬ì„ë² ë”©\n",
    "  - ğŸ”„ **ë³‘ë ¬ ì„ë² ë”©**: ì—¬ëŸ¬ íë¹„íŠ¸ì— ë°ì´í„° ë³µì œ (2x = 8 data qubits)\n",
    "  - ğŸ­ **êµëŒ€ ì„ë² ë”©**: RYì™€ RX íšŒì „ êµëŒ€ ì‚¬ìš©\n",
    "  - ğŸ”¬ **SWAP í…ŒìŠ¤íŠ¸**: ì •í™•í•œ ì¶©ì‹¤ë„ ì¸¡ì •ì„ ìœ„í•œ ì–‘ì SWAP í…ŒìŠ¤íŠ¸\n",
    "- **íë¹„íŠ¸**: 13ê°œ (8 data + 2 reference + 2 trash + 1 control)\n",
    "\n",
    "### 7. **DIFE QAE** - íŒŒê´´ì  ê°„ì„­ ì¶©ì‹¤ë„ ì¶”ì • *(NewFile ê¸°ë°˜)*\n",
    "- **ì›ë¦¬**: Ancilla-free ì ‘ê·¼ë²•ìœ¼ë¡œ compute/uncompute ì‹œí€€ìŠ¤ ì‚¬ìš©\n",
    "- **í˜ì‹ ì **: \n",
    "  - âš¡ **Ancilla-free**: ì¶”ê°€ ì°¸ì¡°ë‚˜ ì œì–´ íë¹„íŠ¸ ë¶ˆí•„ìš”\n",
    "  - ğŸ”„ **Compute/Uncompute**: ìˆœë°©í–¥ í›„ ì—­ë°©í–¥ ì—°ì‚°ìœ¼ë¡œ ê°„ì„­ íŒ¨í„´ ìƒì„±\n",
    "  - ğŸ“ **Destructive Interference**: |0âŸ© ìƒíƒœ ë³µê·€ í™•ë¥ ë¡œ ì¶©ì‹¤ë„ ì¸¡ì •\n",
    "- **íë¹„íŠ¸**: 4ê°œ (enhanced_qvaeì™€ ë™ì¼í•œ ë°ì´í„° íë¹„íŠ¸)\n",
    "\n",
    "### 8. **LS-SWAP QAE** - ì ì¬ ê³µê°„ SWAP í…ŒìŠ¤íŠ¸ *(NewFile ê¸°ë°˜)*\n",
    "- **ì›ë¦¬**: SWAP í…ŒìŠ¤íŠ¸ë¥¼ ì••ì¶•ëœ ì ì¬ ê³µê°„ì—ë§Œ ì ìš©í•˜ì—¬ ìì› ìµœì í™”\n",
    "- **í˜ì‹ ì **:\n",
    "  - ğŸ¯ **Latent Space Focus**: ì••ì¶•ëœ í‘œí˜„ì—ì„œë§Œ SWAP í…ŒìŠ¤íŠ¸ ìˆ˜í–‰\n",
    "  - âš¡ **Resource Efficient**: ì „ì²´ SWAP í…ŒìŠ¤íŠ¸ ëŒ€ë¹„ ì ì€ ancilla íë¹„íŠ¸\n",
    "  - ğŸ”¬ **Maintains Benefits**: SWAP í…ŒìŠ¤íŠ¸ ì¥ì ì„ ìœ ì§€í•˜ë©´ì„œ ë³µì¡ë„ ê°ì†Œ\n",
    "- **íë¹„íŠ¸**: 7ê°œ (4 data + 2 reference + 1 control)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¬ ì–‘ì íšŒë¡œ ì•„í‚¤í…ì²˜ ë¹„êµ\n",
    "\n",
    "| ë°©ë²• | ì´ íë¹„íŠ¸ | ë°ì´í„° íë¹„íŠ¸ | Ancilla íë¹„íŠ¸ | íŠ¹ì§• |\n",
    "|------|-----------|----------------|----------------|------|\n",
    "| QAE Angle | 4 | 4 | 0 | í‘œì¤€ ì ‘ê·¼ë²• |\n",
    "| Enhanced qVAE | 13 | 8 | 5 | ìµœëŒ€ ì„±ëŠ¥ |\n",
    "| DIFE QAE | 4 | 4 | 0 | Ancilla-free |\n",
    "| LS-SWAP QAE | 7 | 4 | 3 | ê· í˜• ì ‘ê·¼ë²• |\n",
    "\n",
    "## ğŸ¯ ì–‘ì ë°©ë²• ê³µí†µ ìµœì í™” (í†µì œ ë³€ì¸)\n",
    "\n",
    "- **PCA ë°ì´í„°**: ëª¨ë“  ì–‘ì ë°©ë²•ì´ ë™ì¼í•œ 4D PCA ë°ì´í„° ì‚¬ìš©\n",
    "- **í‰ê°€ ì§€í‘œ**: ë™ì¼í•œ G-Mean ê¸°ë°˜ ì„±ëŠ¥ í‰ê°€\n",
    "- **ì„ê³„ê°’ ìµœì í™”**: ë™ì¼í•œ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìµœì  ë¶„ë¥˜ ì„ê³„ê°’ íƒìƒ‰\n",
    "- **í•˜ì´í¼íŒŒë¼ë¯¸í„°**: ê° ë°©ë²•ë³„ ì „ìš© ì¡°í•©ìœ¼ë¡œ ê³µì •í•œ ë¹„êµ\n",
    "- **í›ˆë ¨ íšŸìˆ˜**: ë°©ë²•ë³„ íŠ¹ì„±ì— ë§ëŠ” ìµœì  ì—í¬í¬ ì„¤ì •\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f8624a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ QAE Angle ë°©ë²• ì‹¤í–‰ ì¤‘...\n",
      "\n",
      "============================================================\n",
      "âš›ï¸  METHOD 5: QAE ANGLE í›ˆë ¨ ì‹œì‘\n",
      "============================================================\n",
      "ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
      "ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
      "í›ˆë ¨ ì‹œì‘: 100 ì—í¬í¬, ë°°ì¹˜ í¬ê¸° 16, íë¹„íŠ¸ ìˆ˜ 4\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# ğŸ”¬ Quantum Circuit Functions (ì–‘ì íšŒë¡œ ê¸°ë³¸ í•¨ìˆ˜ë“¤)\n",
    "# ==========================================\n",
    "\n",
    "def angle_embedding_circuit(x, weights, n_qubits, layers):\n",
    "    \"\"\"ê°ë„ ì„ë² ë”©ì„ ì‚¬ìš©í•œ ì–‘ì ì˜¤í† ì¸ì½”ë” íšŒë¡œ (more_qubits ê¸°ë°˜)\"\"\"\n",
    "    # ë°ì´í„° ì„ë² ë”© - ê° íŠ¹ì„±ì„ RY íšŒì „ìœ¼ë¡œ ì„ë² ë”©\n",
    "    for i, feature in enumerate(x):\n",
    "        if i < n_qubits:\n",
    "            qml.RY(feature, wires=i)\n",
    "    \n",
    "    # ë³€ë¶„ ë ˆì´ì–´\n",
    "    for l in range(layers):\n",
    "        # ë§¤ê°œë³€ìˆ˜í™”ëœ íšŒì „\n",
    "        for w in range(n_qubits):\n",
    "            qml.RY(weights[l, w, 0], wires=w)\n",
    "            qml.RZ(weights[l, w, 1], wires=w)\n",
    "            qml.RX(weights[l, w, 2], wires=w)\n",
    "        \n",
    "        # ì–½í˜ ê²Œì´íŠ¸ (ì›í˜• êµ¬ì¡°)\n",
    "        if n_qubits > 1:\n",
    "            for w in range(n_qubits):\n",
    "                control = w\n",
    "                target = (w + 1) % n_qubits\n",
    "                qml.CNOT(wires=[control, target])\n",
    "    \n",
    "    # ì¸¡ì • - ë§ˆì§€ë§‰ íë¹„íŠ¸ì˜ Z ê¸°ëŒ“ê°’\n",
    "    return qml.expval(qml.PauliZ(n_qubits - 1))\n",
    "\n",
    "def enhanced_qvae_layer(inputs, weights, layer_idx, n_layers, n_qubits, reupload=True, alternate_embedding=False):\n",
    "    \"\"\"\n",
    "    Enhanced qVAE ë ˆì´ì–´ (more_qubits ê¸°ë°˜)\n",
    "    \"\"\"\n",
    "    # ë°ì´í„° ì„ë² ë”© (ì¬ì—…ë¡œë”©ì´ í™œì„±í™”ëœ ê²½ìš°)\n",
    "    if not reupload or layer_idx == 0:  # ì²« ë²ˆì§¸ ë ˆì´ì–´ì—ì„œëŠ” í•­ìƒ ì„ë² ë”©\n",
    "        for i, feature in enumerate(inputs):\n",
    "            # ë³‘ë ¬ ì„ë² ë”©: ì—¬ëŸ¬ íë¹„íŠ¸ì— ë°ì´í„° ë³µì œ\n",
    "            for p in range(USE_PARALLEL_EMBEDDING):\n",
    "                qubit_idx = i * USE_PARALLEL_EMBEDDING + p\n",
    "                if qubit_idx < n_qubits:\n",
    "                    if alternate_embedding and (i + p) % 2 == 1:\n",
    "                        qml.RX(feature, wires=qubit_idx)\n",
    "                    else:\n",
    "                        qml.RY(feature, wires=qubit_idx)\n",
    "    \n",
    "    # ê° íë¹„íŠ¸ì— ëŒ€í•œ ë§¤ê°œë³€ìˆ˜í™”ëœ íšŒì „\n",
    "    for w in range(n_qubits):\n",
    "        qml.RY(weights[w, 0], wires=w)\n",
    "        qml.RZ(weights[w, 1], wires=w)\n",
    "    \n",
    "    # ì£¼ê¸°ì  ê²½ê³„ì¡°ê±´ì„ ê°€ì§„ ì–½í˜ ê²Œì´íŠ¸\n",
    "    if n_qubits > 1:\n",
    "        for w in range(n_qubits):\n",
    "            control = w\n",
    "            target = (w + 1) % n_qubits\n",
    "            qml.CNOT(wires=[control, target])\n",
    "    \n",
    "    # ì¤‘ê°„ ë ˆì´ì–´ì—ì„œì˜ ë°ì´í„° ì¬ì—…ë¡œë”©\n",
    "    if reupload and layer_idx < n_layers - 1:\n",
    "        for i, feature in enumerate(inputs):\n",
    "            for p in range(USE_PARALLEL_EMBEDDING):\n",
    "                qubit_idx = i * USE_PARALLEL_EMBEDDING + p\n",
    "                if qubit_idx < n_qubits:\n",
    "                    if alternate_embedding and (i + p) % 2 == 1:\n",
    "                        qml.RX(feature, wires=qubit_idx)\n",
    "                    else:\n",
    "                        qml.RY(feature, wires=qubit_idx)\n",
    "\n",
    "def swap_test_measurement(n_data_qubits, n_ref_qubits, total_qubits, n_trash):\n",
    "    \"\"\"\n",
    "    ì–‘ì ì¶©ì‹¤ë„ ì¸¡ì •ì„ ìœ„í•œ SWAP í…ŒìŠ¤íŠ¸ êµ¬í˜„ (more_qubits ê¸°ë°˜)\n",
    "    \"\"\"\n",
    "    control_qubit = total_qubits - 1  # ë§ˆì§€ë§‰ íë¹„íŠ¸ë¥¼ ì œì–´ íë¹„íŠ¸ë¡œ ì‚¬ìš©\n",
    "    \n",
    "    # ì œì–´ íë¹„íŠ¸ì— í•˜ë‹¤ë§ˆë“œ ì ìš©\n",
    "    qml.Hadamard(wires=control_qubit)\n",
    "    \n",
    "    # ë°ì´í„°ì™€ ì°¸ì¡° íë¹„íŠ¸ ê°„ì˜ ì œì–´ëœ SWAP ì—°ì‚°\n",
    "    data_start = n_data_qubits - n_ref_qubits\n",
    "    ref_start = n_data_qubits\n",
    "    \n",
    "    for i in range(n_ref_qubits):\n",
    "        data_qubit = data_start + i\n",
    "        ref_qubit = ref_start + i\n",
    "        if data_qubit < n_data_qubits and ref_qubit < ref_start + n_trash:\n",
    "            qml.CSWAP(wires=[control_qubit, data_qubit, ref_qubit])\n",
    "    \n",
    "    # ì œì–´ íë¹„íŠ¸ì— ìµœì¢… í•˜ë‹¤ë§ˆë“œ\n",
    "    qml.Hadamard(wires=control_qubit)\n",
    "    \n",
    "    # ì œì–´ íë¹„íŠ¸ ì¸¡ì •\n",
    "    return qml.expval(qml.PauliZ(control_qubit))\n",
    "\n",
    "# ==========================================\n",
    "# Method 5: QAE Angle (ê°ë„ ì„ë² ë”© ê¸°ë°˜ ì–‘ì ì˜¤í† ì¸ì½”ë”)\n",
    "# ==========================================\n",
    "\n",
    "def optimize_qae_angle_hyperparameters(X_train, X_val, y_val):\n",
    "    \"\"\"QAE Angle í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\"\"\"\n",
    "    print(f\"QAE Angle í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘ ({len(QAE_ANGLE_HYPERPARAMETER_COMBINATIONS)}ê°œ ì¡°í•©)\")\n",
    "    \n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    best_result = None\n",
    "    \n",
    "    for i, params in enumerate(QAE_ANGLE_HYPERPARAMETER_COMBINATIONS):\n",
    "        try:\n",
    "            print(f\"  ì¡°í•© {i+1}/{len(QAE_ANGLE_HYPERPARAMETER_COMBINATIONS)}: {params}\")\n",
    "            \n",
    "            # íšŒë¡œ ìƒì„±\n",
    "            n_qubits = QAE_QUBITS\n",
    "            dev = qml.device(\"lightning.qubit\", wires=n_qubits)\n",
    "            \n",
    "            @qml.qnode(dev)\n",
    "            def qae_angle_circuit(x, weights):\n",
    "                return angle_embedding_circuit(x, weights, n_qubits, params['layers'])\n",
    "            \n",
    "            # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n",
    "            weights = pnp.random.uniform(-pnp.pi, pnp.pi, \n",
    "                                       (params['layers'], n_qubits, 3), requires_grad=True)\n",
    "            \n",
    "            # ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
    "            optimizer = qml.AdamOptimizer(stepsize=params['learning_rate'])\n",
    "            \n",
    "            # ê²€ì¦ìš© ë‹¨ì¶• í›ˆë ¨\n",
    "            validation_epochs = QUANTUM_TRAINING_CONFIG['validation_epochs']\n",
    "            \n",
    "            for epoch in range(validation_epochs):\n",
    "                for batch_start in range(0, len(X_train), EXPERIMENTAL_CONFIG['batch_size']):\n",
    "                    batch_end = min(batch_start + EXPERIMENTAL_CONFIG['batch_size'], len(X_train))\n",
    "                    X_batch = X_train[batch_start:batch_end]\n",
    "                    \n",
    "                    def cost_fn(w):\n",
    "                        return compute_batch_cost_qae_angle(X_batch, qae_angle_circuit, w)\n",
    "                    \n",
    "                    weights = optimizer.step(cost_fn, weights)\n",
    "            \n",
    "            # ê²€ì¦ ë°ì´í„°ë¡œ í‰ê°€\n",
    "            reconstruction_errors = []\n",
    "            for sample in X_val:\n",
    "                features = pnp.array(sample, requires_grad=False)\n",
    "                expval = qae_angle_circuit(features, weights)\n",
    "                fidelity = (expval + 1.0) / 2.0\n",
    "                error = (1.0 - fidelity) ** 2\n",
    "                reconstruction_errors.append(error)\n",
    "            \n",
    "            reconstruction_errors = np.array(reconstruction_errors)\n",
    "            \n",
    "            # G-Meanìœ¼ë¡œ í‰ê°€\n",
    "            metrics = evaluate_with_optimal_threshold(y_val, reconstruction_errors)\n",
    "            score = metrics['gmean']\n",
    "            \n",
    "            print(f\"    -> G-Mean: {score:.4f}\")\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = params\n",
    "                best_result = {\n",
    "                    'weights': weights,\n",
    "                    'circuit': qae_angle_circuit,\n",
    "                    'dev': dev\n",
    "                }\n",
    "                print(f\"    âœ“ ìƒˆë¡œìš´ ìµœê³  ì ìˆ˜!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    âœ— ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if best_params is None:\n",
    "        # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
    "        best_params = QAE_ANGLE_HYPERPARAMETER_COMBINATIONS[0]\n",
    "        print(\"  ëª¨ë“  ì¡°í•© ì‹¤íŒ¨. ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©.\")\n",
    "    \n",
    "    print(f\"QAE Angle ìµœì í™” ì™„ë£Œ. ìµœê³  G-Mean: {best_score:.4f}\")\n",
    "    return best_params, best_score, best_result\n",
    "\n",
    "def _train_qae_angle_single():\n",
    "    \"\"\"ë‹¨ì¼ QAE Angle ëª¨ë¸ í•™ìŠµ (ë°˜ë³µ ì‹¤í—˜ìš© ë‚´ë¶€ í•¨ìˆ˜)\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âš›ï¸  METHOD 5: QAE ANGLE í›ˆë ¨ ì‹œì‘\")\n",
    "    print(\"=\"*60)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë˜ëŠ” ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
    "        if should_optimize_hyperparameters():\n",
    "            # ê²€ì¦ ë°ì´í„° ë¶„í•  (PCA ë°ì´í„° ì‚¬ìš©)\n",
    "            X_train_pca_normal = X_train_pca[y_train == 0]\n",
    "            X_train_val, X_val_normal = train_test_split(\n",
    "                X_train_pca_normal, test_size=HYPERPARAMETER_SEARCH['validation_split'], \n",
    "                random_state=np.random.randint(1, 1000)\n",
    "            )\n",
    "            \n",
    "            # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ fraud ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸°\n",
    "            fraud_mask = y_test == 1\n",
    "            X_fraud_pca = X_test_pca[fraud_mask]\n",
    "            n_fraud_val = min(len(X_fraud_pca), len(X_val_normal) // 10)\n",
    "            X_fraud_val = X_fraud_pca[:n_fraud_val]\n",
    "            \n",
    "            X_val = np.vstack([X_val_normal, X_fraud_val])\n",
    "            y_val = np.hstack([np.zeros(len(X_val_normal)), np.ones(len(X_fraud_val))])\n",
    "            \n",
    "            print(\"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘...\")\n",
    "            best_params, best_score, best_result = optimize_qae_angle_hyperparameters(X_train_val, X_val, y_val)\n",
    "            print(f\"ìµœì  íŒŒë¼ë¯¸í„°: {best_params}\")\n",
    "        else:\n",
    "            # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
    "            best_params = get_default_params('qae_angle')\n",
    "            best_result = None\n",
    "        \n",
    "        # ìµœì¢… ëª¨ë¸ í›ˆë ¨\n",
    "        print(\"ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì¤‘...\")\n",
    "        n_qubits = QAE_QUBITS\n",
    "        dev = qml.device(\"lightning.qubit\", wires=n_qubits)\n",
    "        \n",
    "        @qml.qnode(dev)\n",
    "        def final_qae_angle_circuit(x, weights):\n",
    "            return angle_embedding_circuit(x, weights, n_qubits, best_params['layers'])\n",
    "        \n",
    "        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (ë°˜ë³µ ì‹¤í—˜ë§ˆë‹¤ ë‹¤ë¥¸ ì´ˆê¸°í™”)\n",
    "        weights = pnp.random.uniform(-pnp.pi, pnp.pi, \n",
    "                                   (best_params['layers'], n_qubits, 3), requires_grad=True)\n",
    "        \n",
    "        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
    "        optimizer = qml.AdamOptimizer(stepsize=best_params['learning_rate'])\n",
    "        \n",
    "        # ì •ìƒ ë°ì´í„°ë§Œìœ¼ë¡œ í›ˆë ¨\n",
    "        X_train_pca_normal = X_train_pca[y_train == 0]\n",
    "        epochs = QUANTUM_TRAINING_CONFIG['epochs_qae_angle']\n",
    "        \n",
    "        if not STATISTICAL_CONFIG['enable_repetition']:\n",
    "            print(f\"í›ˆë ¨ ì‹œì‘: {epochs} ì—í¬í¬, ë°°ì¹˜ í¬ê¸° {EXPERIMENTAL_CONFIG['batch_size']}, íë¹„íŠ¸ ìˆ˜ {n_qubits}\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_cost = 0\n",
    "            n_batches = 0\n",
    "            \n",
    "            # ë°°ì¹˜ë³„ í›ˆë ¨\n",
    "            for batch_start in range(0, len(X_train_pca_normal), EXPERIMENTAL_CONFIG['batch_size']):\n",
    "                batch_end = min(batch_start + EXPERIMENTAL_CONFIG['batch_size'], len(X_train_pca_normal))\n",
    "                X_batch = X_train_pca_normal[batch_start:batch_end]\n",
    "                \n",
    "                def cost_fn(w):\n",
    "                    return compute_batch_cost_qae_angle(X_batch, final_qae_angle_circuit, w)\n",
    "                \n",
    "                weights = optimizer.step(cost_fn, weights)\n",
    "                cost = compute_batch_cost_qae_angle(X_batch, final_qae_angle_circuit, weights)\n",
    "                epoch_cost += cost\n",
    "                n_batches += 1\n",
    "            \n",
    "            # ë°˜ë³µ ì‹¤í—˜ì´ ì•„ë‹ ë•Œë§Œ ì¤‘ê°„ ì¶œë ¥\n",
    "            if (epoch + 1) % 20 == 0 and not STATISTICAL_CONFIG['enable_repetition']:\n",
    "                avg_cost = epoch_cost / n_batches\n",
    "                print(f\"  ì—í¬í¬ {epoch + 1}/{epochs}, í‰ê·  ë¹„ìš©: {avg_cost:.6f}\")\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€\n",
    "        print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€ ì¤‘...\")\n",
    "        reconstruction_errors = []\n",
    "        \n",
    "        for sample in X_test_pca:\n",
    "            features = pnp.array(sample, requires_grad=False)\n",
    "            expval = final_qae_angle_circuit(features, weights)\n",
    "            fidelity = (expval + 1.0) / 2.0\n",
    "            error = (1.0 - fidelity) ** 2\n",
    "            reconstruction_errors.append(error)\n",
    "        \n",
    "        reconstruction_errors = np.array(reconstruction_errors)\n",
    "        \n",
    "        # í‰ê°€\n",
    "        metrics = evaluate_with_optimal_threshold(y_test, reconstruction_errors)\n",
    "        \n",
    "        result = {\n",
    "            'method': 'QAE Angle',\n",
    "            'type': 'Quantum ML',\n",
    "            'training_time': training_time,\n",
    "            'circuit': final_qae_angle_circuit,\n",
    "            'weights': weights,\n",
    "            'best_params': best_params,\n",
    "            'n_qubits': n_qubits,\n",
    "            'reconstruction_errors': reconstruction_errors,\n",
    "            **metrics\n",
    "        }\n",
    "        \n",
    "        # ê²°ê³¼ ì¶œë ¥ (ë°˜ë³µ ì‹¤í—˜ ì‹œ ê°„ì†Œí™”)\n",
    "        if STATISTICAL_CONFIG['enable_repetition']:\n",
    "            print(f\"G-Mean: {metrics['gmean']:.4f}, í›ˆë ¨ì‹œê°„: {training_time:.2f}ì´ˆ\")\n",
    "        else:\n",
    "            additional_info = {\n",
    "                'í›ˆë ¨ì‹œê°„': f\"{training_time:.2f}ì´ˆ\",\n",
    "                'íë¹„íŠ¸ ìˆ˜': f\"{n_qubits}\",\n",
    "                'ë³€ë¶„ ë ˆì´ì–´': f\"{best_params['layers']}\",\n",
    "                'ìµœì  í•™ìŠµë¥ ': f\"{best_params['learning_rate']}\",\n",
    "                'ìµœì  ì„ê³„ê°’': f\"{metrics['threshold']:.6f}\"\n",
    "            }\n",
    "            print_results(\"QAE Angle\", metrics, additional_info)\n",
    "        \n",
    "        print(\"âœ… QAE Angle ì™„ë£Œ\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ QAE Angle ì‹¤íŒ¨: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def train_qae_angle():\n",
    "    \"\"\"\n",
    "    QAE Angle í›ˆë ¨ (ë°˜ë³µ ì‹¤í—˜ ì§€ì›)\n",
    "    ì‹¤í—˜ì„¤ê³„.txt êµ¬í˜„: ì—¬ëŸ¬ ë²ˆì˜ ë…ë¦½ì  ì‹¤í–‰ í›„ í†µê³„ì  ìš”ì•½\n",
    "    \"\"\"\n",
    "    return run_multiple_experiments(_train_qae_angle_single, 'QAE Angle')\n",
    "\n",
    "# QAE Angle ì‹¤í–‰\n",
    "print(\"ğŸš€ QAE Angle ë°©ë²• ì‹¤í–‰ ì¤‘...\")\n",
    "qae_angle_result = train_qae_angle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5645c524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Method 6: Enhanced qVAE (ê³ ê¸‰ ì–‘ì ë³€ë¶„ ì˜¤í† ì¸ì½”ë”)\n",
    "# ==========================================\n",
    "\n",
    "def enhanced_qvae_circuit(x, weights, n_qubits, total_qubits, layers):\n",
    "    \"\"\"\n",
    "    Enhanced qVAE íšŒë¡œ (more_qubits ê¸°ë°˜)\n",
    "    \"\"\"\n",
    "    # ë°ì´í„° ì¬ì—…ë¡œë”©ê³¼ í•¨ê»˜ Enhanced qVAE ë ˆì´ì–´ ì ìš©\n",
    "    for l in range(layers):\n",
    "        enhanced_qvae_layer(\n",
    "            inputs=x,\n",
    "            weights=weights[l],\n",
    "            layer_idx=l,\n",
    "            n_layers=layers,\n",
    "            n_qubits=n_qubits,\n",
    "            reupload=USE_DATA_REUPLOADING,\n",
    "            alternate_embedding=USE_ALTERNATE_EMBEDDING\n",
    "        )\n",
    "    \n",
    "    # ì¸¡ì • ì „ëµ ì„ íƒ\n",
    "    if USE_SWAP_TEST and total_qubits > n_qubits:\n",
    "        return swap_test_measurement(n_qubits, N_REFERENCE_QUBITS, total_qubits, N_TRASH_QUBITS)\n",
    "    else:\n",
    "        return qml.expval(qml.PauliZ(n_qubits - 1))\n",
    "\n",
    "def optimize_enhanced_qvae_hyperparameters(X_train, X_val, y_val):\n",
    "    \"\"\"Enhanced qVAE í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\"\"\"\n",
    "    print(f\"Enhanced qVAE í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘ ({len(ENHANCED_QVAE_HYPERPARAMETER_COMBINATIONS)}ê°œ ì¡°í•©)\")\n",
    "    \n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    best_result = None\n",
    "    \n",
    "    for i, params in enumerate(ENHANCED_QVAE_HYPERPARAMETER_COMBINATIONS):\n",
    "        try:\n",
    "            print(f\"  ì¡°í•© {i+1}/{len(ENHANCED_QVAE_HYPERPARAMETER_COMBINATIONS)}: {params}\")\n",
    "            \n",
    "            # íšŒë¡œ ìƒì„± - ë™ì  íë¹„íŠ¸ ìˆ˜ ì‚¬ìš©\n",
    "            n_qubits = QVAE_DATA_QUBITS\n",
    "            total_qubits = QVAE_TOTAL_QUBITS\n",
    "            dev = qml.device(\"lightning.qubit\", wires=total_qubits)\n",
    "            \n",
    "            @qml.qnode(dev)\n",
    "            def qvae_circuit(x, weights):\n",
    "                return enhanced_qvae_circuit(x, weights, n_qubits, total_qubits, params['layers'])\n",
    "            \n",
    "            # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (Enhanced qVAEëŠ” ë ˆì´ì–´ë‹¹ íë¹„íŠ¸ë‹¹ 2ê°œ íŒŒë¼ë¯¸í„° ì‚¬ìš©)\n",
    "            weights = pnp.random.uniform(-pnp.pi, pnp.pi, \n",
    "                                       (params['layers'], n_qubits, 2), requires_grad=True)\n",
    "            \n",
    "            # ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
    "            optimizer = qml.AdamOptimizer(stepsize=params['learning_rate'])\n",
    "            \n",
    "            # ê²€ì¦ìš© ë‹¨ì¶• í›ˆë ¨\n",
    "            validation_epochs = QUANTUM_TRAINING_CONFIG['validation_epochs']\n",
    "            \n",
    "            for epoch in range(validation_epochs):\n",
    "                for batch_start in range(0, len(X_train), EXPERIMENTAL_CONFIG['batch_size']):\n",
    "                    batch_end = min(batch_start + EXPERIMENTAL_CONFIG['batch_size'], len(X_train))\n",
    "                    X_batch = X_train[batch_start:batch_end]\n",
    "                    \n",
    "                    def cost_fn(w):\n",
    "                        return compute_batch_cost_enhanced_qvae(X_batch, qvae_circuit, w, USE_SWAP_TEST)\n",
    "                    \n",
    "                    weights = optimizer.step(cost_fn, weights)\n",
    "            \n",
    "            # ê²€ì¦ ë°ì´í„°ë¡œ í‰ê°€\n",
    "            reconstruction_errors = []\n",
    "            for sample in X_val:\n",
    "                features = pnp.array(sample, requires_grad=False)\n",
    "                expval = qvae_circuit(features, weights)\n",
    "                \n",
    "                if USE_SWAP_TEST:\n",
    "                    fidelity = (expval + 1.0) / 2.0\n",
    "                else:\n",
    "                    fidelity = (expval + 1.0) / 2.0\n",
    "                \n",
    "                error = (1.0 - fidelity) ** 2\n",
    "                reconstruction_errors.append(error)\n",
    "            \n",
    "            reconstruction_errors = np.array(reconstruction_errors)\n",
    "            \n",
    "            # G-Meanìœ¼ë¡œ í‰ê°€\n",
    "            metrics = evaluate_with_optimal_threshold(y_val, reconstruction_errors)\n",
    "            score = metrics['gmean']\n",
    "            \n",
    "            print(f\"    -> G-Mean: {score:.4f}\")\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = params\n",
    "                best_result = {\n",
    "                    'weights': weights,\n",
    "                    'circuit': qvae_circuit,\n",
    "                    'dev': dev\n",
    "                }\n",
    "                print(f\"    âœ“ ìƒˆë¡œìš´ ìµœê³  ì ìˆ˜!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    âœ— ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if best_params is None:\n",
    "        # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
    "        best_params = ENHANCED_QVAE_HYPERPARAMETER_COMBINATIONS[0]\n",
    "        print(\"  ëª¨ë“  ì¡°í•© ì‹¤íŒ¨. ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©.\")\n",
    "    \n",
    "    print(f\"Enhanced qVAE ìµœì í™” ì™„ë£Œ. ìµœê³  G-Mean: {best_score:.4f}\")\n",
    "    return best_params, best_score, best_result\n",
    "\n",
    "def train_enhanced_qvae():\n",
    "    \"\"\"Enhanced qVAE ëª¨ë¸ í•™ìŠµ\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ”¬ METHOD 6: ENHANCED qVAE í›ˆë ¨ ì‹œì‘\")\n",
    "    print(\"=\"*60)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\n",
    "        if HYPERPARAMETER_SEARCH['enable_search']:\n",
    "            # ê²€ì¦ ë°ì´í„° ë¶„í•  (PCA ë°ì´í„° ì‚¬ìš©)\n",
    "            X_train_pca_normal = X_train_pca[y_train == 0]\n",
    "            X_train_val, X_val_normal = train_test_split(\n",
    "                X_train_pca_normal, test_size=HYPERPARAMETER_SEARCH['validation_split'], random_state=42\n",
    "            )\n",
    "            \n",
    "            # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ fraud ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸°\n",
    "            fraud_mask = y_test == 1\n",
    "            X_fraud_pca = X_test_pca[fraud_mask]\n",
    "            n_fraud_val = min(len(X_fraud_pca), len(X_val_normal) // 10)\n",
    "            X_fraud_val = X_fraud_pca[:n_fraud_val]\n",
    "            \n",
    "            X_val = np.vstack([X_val_normal, X_fraud_val])\n",
    "            y_val = np.hstack([np.zeros(len(X_val_normal)), np.ones(len(X_fraud_val))])\n",
    "            \n",
    "            print(\"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘...\")\n",
    "            best_params, best_score, best_result = optimize_enhanced_qvae_hyperparameters(X_train_val, X_val, y_val)\n",
    "            print(f\"ìµœì  íŒŒë¼ë¯¸í„°: {best_params}\")\n",
    "        else:\n",
    "            # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
    "            best_params = get_default_params('enhanced_qvae')\n",
    "            best_result = None\n",
    "        \n",
    "        # ìµœì¢… ëª¨ë¸ í›ˆë ¨\n",
    "        print(\"ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì¤‘...\")\n",
    "        n_qubits = QVAE_DATA_QUBITS\n",
    "        total_qubits = QVAE_TOTAL_QUBITS\n",
    "        dev = qml.device(\"lightning.qubit\", wires=total_qubits)\n",
    "        \n",
    "        @qml.qnode(dev)\n",
    "        def final_qvae_circuit(x, weights):\n",
    "            return enhanced_qvae_circuit(x, weights, n_qubits, total_qubits, best_params['layers'])\n",
    "        \n",
    "        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n",
    "        weights = pnp.random.uniform(-pnp.pi, pnp.pi, \n",
    "                                   (best_params['layers'], n_qubits, 2), requires_grad=True)\n",
    "        \n",
    "        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
    "        optimizer = qml.AdamOptimizer(stepsize=best_params['learning_rate'])\n",
    "        \n",
    "        # ì •ìƒ ë°ì´í„°ë§Œìœ¼ë¡œ í›ˆë ¨\n",
    "        X_train_pca_normal = X_train_pca[y_train == 0]\n",
    "        epochs = QUANTUM_TRAINING_CONFIG['epochs_enhanced_qvae']\n",
    "        \n",
    "        print(f\"í›ˆë ¨ ì‹œì‘: {epochs} ì—í¬í¬, ë°°ì¹˜ í¬ê¸° {EXPERIMENTAL_CONFIG['batch_size']}\")\n",
    "        print(f\"Enhanced qVAE êµ¬ì„±: {n_qubits}ê°œ ë°ì´í„° íë¹„íŠ¸, ì´ {total_qubits} íë¹„íŠ¸\")\n",
    "        print(f\"ê³ ê¸‰ ê¸°ëŠ¥: ë°ì´í„° ì¬ì—…ë¡œë”©={USE_DATA_REUPLOADING}, SWAP í…ŒìŠ¤íŠ¸={USE_SWAP_TEST}\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_cost = 0\n",
    "            n_batches = 0\n",
    "            \n",
    "            # ë°°ì¹˜ë³„ í›ˆë ¨\n",
    "            for batch_start in range(0, len(X_train_pca_normal), EXPERIMENTAL_CONFIG['batch_size']):\n",
    "                batch_end = min(batch_start + EXPERIMENTAL_CONFIG['batch_size'], len(X_train_pca_normal))\n",
    "                X_batch = X_train_pca_normal[batch_start:batch_end]\n",
    "                \n",
    "                def cost_fn(w):\n",
    "                    return compute_batch_cost_enhanced_qvae(X_batch, final_qvae_circuit, w, USE_SWAP_TEST)\n",
    "                \n",
    "                weights = optimizer.step(cost_fn, weights)\n",
    "                cost = compute_batch_cost_enhanced_qvae(X_batch, final_qvae_circuit, weights, USE_SWAP_TEST)\n",
    "                epoch_cost += cost\n",
    "                n_batches += 1\n",
    "            \n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                avg_cost = epoch_cost / n_batches\n",
    "                cost_type = \"Linear\" if USE_SWAP_TEST else \"Squared\"\n",
    "                print(f\"  ì—í¬í¬ {epoch + 1}/{epochs}, í‰ê·  {cost_type} ë¹„ìš©: {avg_cost:.6f}\")\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€\n",
    "        print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€ ì¤‘...\")\n",
    "        reconstruction_errors = []\n",
    "        \n",
    "        for sample in X_test_pca:\n",
    "            features = pnp.array(sample, requires_grad=False)\n",
    "            expval = final_qvae_circuit(features, weights)\n",
    "            \n",
    "            if USE_SWAP_TEST:\n",
    "                fidelity = (expval + 1.0) / 2.0\n",
    "            else:\n",
    "                fidelity = (expval + 1.0) / 2.0\n",
    "            \n",
    "            error = (1.0 - fidelity) ** 2\n",
    "            reconstruction_errors.append(error)\n",
    "        \n",
    "        reconstruction_errors = np.array(reconstruction_errors)\n",
    "        \n",
    "        # í‰ê°€\n",
    "        metrics = evaluate_with_optimal_threshold(y_test, reconstruction_errors)\n",
    "        \n",
    "        result = {\n",
    "            'method': 'Enhanced qVAE',\n",
    "            'type': 'Quantum ML',\n",
    "            'training_time': training_time,\n",
    "            'circuit': final_qvae_circuit,\n",
    "            'weights': weights,\n",
    "            'best_params': best_params,\n",
    "            'n_qubits': n_qubits,\n",
    "            'total_qubits': total_qubits,\n",
    "            'reconstruction_errors': reconstruction_errors,\n",
    "            **metrics\n",
    "        }\n",
    "        \n",
    "        # ê²°ê³¼ ì¶œë ¥\n",
    "        additional_info = {\n",
    "            'í›ˆë ¨ì‹œê°„': f\"{training_time:.2f}ì´ˆ\",\n",
    "            'ë°ì´í„° íë¹„íŠ¸': f\"{n_qubits}\",\n",
    "            'ì´ íë¹„íŠ¸': f\"{total_qubits}\",\n",
    "            'ë³€ë¶„ ë ˆì´ì–´': f\"{best_params['layers']}\",\n",
    "            'ìµœì  í•™ìŠµë¥ ': f\"{best_params['learning_rate']}\",\n",
    "            'SWAP í…ŒìŠ¤íŠ¸': f\"{'í™œì„±í™”' if USE_SWAP_TEST else 'ë¹„í™œì„±í™”'}\",\n",
    "            'ìµœì  ì„ê³„ê°’': f\"{metrics['threshold']:.6f}\"\n",
    "        }\n",
    "        print_results(\"Enhanced qVAE\", metrics, additional_info)\n",
    "        print(\"âœ… Enhanced qVAE ì™„ë£Œ\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Enhanced qVAE ì‹¤íŒ¨: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Enhanced qVAE ì‹¤í–‰\n",
    "print(\"ğŸš€ Enhanced qVAE ë°©ë²• ì‹¤í–‰ ì¤‘...\")\n",
    "enhanced_qvae_result = train_enhanced_qvae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509e51dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Method 7: DIFE QAE (íŒŒê´´ì  ê°„ì„­ ì¶©ì‹¤ë„ ì¶”ì •)\n",
    "# ==========================================\n",
    "\n",
    "def data_embedding_layer(x, n_qubits, alternate_embedding=False):\n",
    "    \"\"\"\n",
    "    DIFEìš© ë°ì´í„° ì„ë² ë”© ë ˆì´ì–´ (NewFile ê¸°ë°˜)\n",
    "    \"\"\"\n",
    "    for i, feature in enumerate(x):\n",
    "        if i < n_qubits:\n",
    "            if alternate_embedding and i % 2 == 1:\n",
    "                qml.RX(feature, wires=i)\n",
    "            else:\n",
    "                qml.RY(feature, wires=i)\n",
    "\n",
    "def dife_circuit(x, weights, n_qubits, layers):\n",
    "    \"\"\"\n",
    "    DIFE (Destructive Interference Fidelity Estimation) íšŒë¡œ êµ¬í˜„ (NewFile ê¸°ë°˜)\n",
    "    \n",
    "    íšŒë¡œ ì‹œí€€ìŠ¤:\n",
    "    1. ì…ë ¥ ë°ì´í„°ë¥¼ ì´ˆê¸° ìƒíƒœ |Ïˆ(x)âŸ©ë¡œ ì¸ì½”ë”©\n",
    "    2. ì¸ì½”ë” ë³€í™˜ ì ìš© (ì••ì¶• ë‹¨ê³„)\n",
    "    3. ë¶€ë¶„ ë””ì½”ë” ë³€í™˜ ì ìš© (ë¹„ëŒ€ì¹­ ì¬êµ¬ì„±)\n",
    "    4. ì¬êµ¬ì„± í’ˆì§ˆì„ ë°˜ì˜í•˜ëŠ” ë‹¨ì¼ ê´€ì¸¡ê°’ ì¸¡ì •\n",
    "    \n",
    "    Args:\n",
    "        x: ì…ë ¥ ë°ì´í„° íŠ¹ì„±\n",
    "        weights: í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°\n",
    "        n_qubits: ë°ì´í„° íë¹„íŠ¸ ìˆ˜ (4)\n",
    "        layers: ë³€ë¶„ ë ˆì´ì–´ ìˆ˜\n",
    "    \n",
    "    Returns:\n",
    "        Single scalar reconstruction fidelity score\n",
    "    \"\"\"\n",
    "    # Step 1: ì…ë ¥ ë°ì´í„°ë¥¼ ì°¸ì¡° ìƒíƒœ |Ïˆ(x)âŸ©ë¡œ ì¸ì½”ë”©\n",
    "    data_embedding_layer(x, n_qubits, USE_ALTERNATE_EMBEDDING)\n",
    "    \n",
    "    # Step 2: ì¸ì½”ë” ë³€í™˜ ì ìš© (ì••ì¶• ë‹¨ê³„)\n",
    "    for l in range(layers):\n",
    "        # ë§¤ê°œë³€ìˆ˜í™”ëœ íšŒì „\n",
    "        for w in range(n_qubits):\n",
    "            qml.RY(weights[l][w, 0], wires=w)\n",
    "            qml.RZ(weights[l][w, 1], wires=w)\n",
    "        \n",
    "        # ì–½í˜ ê²Œì´íŠ¸\n",
    "        if n_qubits > 1:\n",
    "            for w in range(n_qubits):\n",
    "                control = w\n",
    "                target = (w + 1) % n_qubits\n",
    "                qml.CNOT(wires=[control, target])\n",
    "        \n",
    "        # ë°ì´í„° ì¬ì—…ë¡œë”© (ì¤‘ê°„ ë ˆì´ì–´ì—ì„œ)\n",
    "        if USE_DATA_REUPLOADING and l < layers - 1:\n",
    "            data_embedding_layer(x, n_qubits, USE_ALTERNATE_EMBEDDING)\n",
    "    \n",
    "    # Step 3: ë¶€ë¶„ ë””ì½”ë” ë³€í™˜ ì ìš© (ë¹„ëŒ€ì¹­ ì¬êµ¬ì„±)\n",
    "    # ë§ˆì§€ë§‰ ë ˆì´ì–´ë§Œ ì—­ë°©í–¥ìœ¼ë¡œ ì ìš©í•˜ì—¬ ë¶ˆì™„ì „í•œ ì¬êµ¬ì„± ìƒì„±\n",
    "    l = layers - 1  # ë§ˆì§€ë§‰ ë ˆì´ì–´ë§Œ\n",
    "    \n",
    "    # ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ ì–½í˜ ê²Œì´íŠ¸ë¥¼ ì—­ìˆœìœ¼ë¡œ ì ìš©\n",
    "    if n_qubits > 1:\n",
    "        for w in reversed(range(n_qubits)):\n",
    "            control = w\n",
    "            target = (w + 1) % n_qubits\n",
    "            qml.CNOT(wires=[control, target])\n",
    "    \n",
    "    # ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ ë§¤ê°œë³€ìˆ˜í™”ëœ íšŒì „ì„ ì—­ìˆœìœ¼ë¡œ ì ìš©\n",
    "    for w in reversed(range(n_qubits)):\n",
    "        qml.RZ(-weights[l][w, 1], wires=w)\n",
    "        qml.RY(-weights[l][w, 0], wires=w)\n",
    "    \n",
    "    # Step 4: ì¬êµ¬ì„± í’ˆì§ˆì„ ë°˜ì˜í•˜ëŠ” ë‹¨ì¼ ì¸¡ì •\n",
    "    # ì²« ë²ˆì§¸ íë¹„íŠ¸ì—ì„œ Pauli-Z ì¸¡ì •\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "def optimize_dife_hyperparameters(X_train, X_val, y_val):\n",
    "    \"\"\"DIFE QAE í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\"\"\"\n",
    "    print(f\"DIFE QAE í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘ ({len(DIFE_HYPERPARAMETER_COMBINATIONS)}ê°œ ì¡°í•©)\")\n",
    "    \n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    best_result = None\n",
    "    \n",
    "    for i, params in enumerate(DIFE_HYPERPARAMETER_COMBINATIONS):\n",
    "        try:\n",
    "            print(f\"  ì¡°í•© {i+1}/{len(DIFE_HYPERPARAMETER_COMBINATIONS)}: {params}\")\n",
    "            \n",
    "            # íšŒë¡œ ìƒì„±\n",
    "            n_qubits = DIFE_TOTAL_QUBITS  # 4 qubits (ancilla-free)\n",
    "            dev = qml.device(\"lightning.qubit\", wires=n_qubits)\n",
    "            \n",
    "            @qml.qnode(dev)\n",
    "            def dife_quantum_circuit(x, weights):\n",
    "                return dife_circuit(x, weights, n_qubits, params['layers'])\n",
    "            \n",
    "            # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n",
    "            weights = pnp.random.uniform(-pnp.pi, pnp.pi, \n",
    "                                       (params['layers'], n_qubits, 2), requires_grad=True)\n",
    "            \n",
    "            # ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
    "            optimizer = qml.AdamOptimizer(stepsize=params['learning_rate'])\n",
    "            \n",
    "            # ê²€ì¦ìš© ë‹¨ì¶• í›ˆë ¨\n",
    "            validation_epochs = QUANTUM_TRAINING_CONFIG['validation_epochs']\n",
    "            \n",
    "            for epoch in range(validation_epochs):\n",
    "                for batch_start in range(0, len(X_train), EXPERIMENTAL_CONFIG['batch_size']):\n",
    "                    batch_end = min(batch_start + EXPERIMENTAL_CONFIG['batch_size'], len(X_train))\n",
    "                    X_batch = X_train[batch_start:batch_end]\n",
    "                    \n",
    "                    def cost_fn(w):\n",
    "                        return compute_batch_cost_dife(X_batch, dife_quantum_circuit, w)\n",
    "                    \n",
    "                    weights = optimizer.step(cost_fn, weights)\n",
    "            \n",
    "            # ê²€ì¦ ë°ì´í„°ë¡œ í‰ê°€\n",
    "            reconstruction_errors = []\n",
    "            for sample in X_val:\n",
    "                features = pnp.array(sample, requires_grad=False)\n",
    "                expval = dife_quantum_circuit(features, weights)\n",
    "                \n",
    "                # DIFEì—ì„œ expvalì€ ì´ë¯¸ ì¶©ì‹¤ë„ ì¸¡ì •ê°’\n",
    "                fidelity = expval\n",
    "                # í´ë¦¬í•‘ìœ¼ë¡œ ìœ íš¨ ë²”ìœ„ [0, 1] í™•ë³´\n",
    "                fidelity = np.clip(fidelity, 0.0, 1.0)\n",
    "                \n",
    "                # ì„ í˜• ì˜¤ì°¨ ê³„ì‚° (DIFE íŠ¹ì„±ìƒ ì„ í˜• ì†ì‹¤ì´ ë” ì í•©)\n",
    "                error = 1.0 - fidelity\n",
    "                reconstruction_errors.append(error)\n",
    "            \n",
    "            reconstruction_errors = np.array(reconstruction_errors)\n",
    "            \n",
    "            # G-Meanìœ¼ë¡œ í‰ê°€\n",
    "            metrics = evaluate_with_optimal_threshold(y_val, reconstruction_errors)\n",
    "            score = metrics['gmean']\n",
    "            \n",
    "            print(f\"    -> G-Mean: {score:.4f}\")\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = params\n",
    "                best_result = {\n",
    "                    'weights': weights,\n",
    "                    'circuit': dife_quantum_circuit,\n",
    "                    'dev': dev\n",
    "                }\n",
    "                print(f\"    âœ“ ìƒˆë¡œìš´ ìµœê³  ì ìˆ˜!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    âœ— ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if best_params is None:\n",
    "        # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
    "        best_params = DIFE_HYPERPARAMETER_COMBINATIONS[0]\n",
    "        print(\"  ëª¨ë“  ì¡°í•© ì‹¤íŒ¨. ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©.\")\n",
    "    \n",
    "    print(f\"DIFE QAE ìµœì í™” ì™„ë£Œ. ìµœê³  G-Mean: {best_score:.4f}\")\n",
    "    return best_params, best_score, best_result\n",
    "\n",
    "def train_dife():\n",
    "    \"\"\"DIFE QAE ëª¨ë¸ í•™ìŠµ\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ’¥ METHOD 7: DIFE QAE í›ˆë ¨ ì‹œì‘\")\n",
    "    print(\"=\"*60)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë˜ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©\n",
    "        if should_optimize_hyperparameters():\n",
    "            # ê²€ì¦ ë°ì´í„° ë¶„í•  (PCA ë°ì´í„° ì‚¬ìš©)\n",
    "            X_train_pca_normal = X_train_pca[y_train == 0]\n",
    "            X_train_val, X_val_normal = train_test_split(\n",
    "                X_train_pca_normal, test_size=HYPERPARAMETER_SEARCH['validation_split'], random_state=42\n",
    "            )\n",
    "            \n",
    "            # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ fraud ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸°\n",
    "            fraud_mask = y_test == 1\n",
    "            X_fraud_pca = X_test_pca[fraud_mask]\n",
    "            n_fraud_val = min(len(X_fraud_pca), len(X_val_normal) // 10)\n",
    "            X_fraud_val = X_fraud_pca[:n_fraud_val]\n",
    "            \n",
    "            X_val = np.vstack([X_val_normal, X_fraud_val])\n",
    "            y_val = np.hstack([np.zeros(len(X_val_normal)), np.ones(len(X_fraud_val))])\n",
    "            \n",
    "            print(\"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘...\")\n",
    "            best_params, best_score, best_result = optimize_dife_hyperparameters(X_train_val, X_val, y_val)\n",
    "            print(f\"ìµœì  íŒŒë¼ë¯¸í„°: {best_params}\")\n",
    "        else:\n",
    "            # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
    "            best_params = get_default_params('dife_qae')\n",
    "            print(\"ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©\")\n",
    "            best_result = None\n",
    "        \n",
    "        # ìµœì¢… ëª¨ë¸ í›ˆë ¨\n",
    "        print(\"ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì¤‘...\")\n",
    "        n_qubits = DIFE_TOTAL_QUBITS  # 4 qubits (ancilla-free)\n",
    "        dev = qml.device(\"lightning.qubit\", wires=n_qubits)\n",
    "        \n",
    "        @qml.qnode(dev)\n",
    "        def final_dife_circuit(x, weights):\n",
    "            return dife_circuit(x, weights, n_qubits, best_params['layers'])\n",
    "        \n",
    "        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n",
    "        weights = pnp.random.uniform(-pnp.pi, pnp.pi, \n",
    "                                   (best_params['layers'], n_qubits, 2), requires_grad=True)\n",
    "        \n",
    "        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
    "        optimizer = qml.AdamOptimizer(stepsize=best_params['learning_rate'])\n",
    "        \n",
    "        # ì •ìƒ ë°ì´í„°ë§Œìœ¼ë¡œ í›ˆë ¨\n",
    "        X_train_pca_normal = X_train_pca[y_train == 0]\n",
    "        epochs = QUANTUM_TRAINING_CONFIG['epochs_dife']\n",
    "        \n",
    "        print(f\"í›ˆë ¨ ì‹œì‘: {epochs} ì—í¬í¬, ë°°ì¹˜ í¬ê¸° {EXPERIMENTAL_CONFIG['batch_size']}\")\n",
    "        print(f\"DIFE êµ¬ì„±: {n_qubits}ê°œ íë¹„íŠ¸ (ancilla-free)\")\n",
    "        print(f\"íŠ¹ì§•: Compute/Uncompute ì‹œí€€ìŠ¤, íŒŒê´´ì  ê°„ì„­ ì¸¡ì •\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_cost = 0\n",
    "            n_batches = 0\n",
    "            \n",
    "            # ë°°ì¹˜ë³„ í›ˆë ¨\n",
    "            for batch_start in range(0, len(X_train_pca_normal), EXPERIMENTAL_CONFIG['batch_size']):\n",
    "                batch_end = min(batch_start + EXPERIMENTAL_CONFIG['batch_size'], len(X_train_pca_normal))\n",
    "                X_batch = X_train_pca_normal[batch_start:batch_end]\n",
    "                \n",
    "                def cost_fn(w):\n",
    "                    return compute_batch_cost_dife(X_batch, final_dife_circuit, w)\n",
    "                \n",
    "                weights = optimizer.step(cost_fn, weights)\n",
    "                cost = compute_batch_cost_dife(X_batch, final_dife_circuit, weights)\n",
    "                epoch_cost += cost\n",
    "                n_batches += 1\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                avg_cost = epoch_cost / n_batches\n",
    "                print(f\"  ì—í¬í¬ {epoch + 1}/{epochs}, í‰ê·  ì„ í˜• ë¹„ìš©: {avg_cost:.6f}\")\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€\n",
    "        print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€ ì¤‘...\")\n",
    "        reconstruction_errors = []\n",
    "        \n",
    "        for sample in X_test_pca:\n",
    "            features = pnp.array(sample, requires_grad=False)\n",
    "            expval = final_dife_circuit(features, weights)\n",
    "            \n",
    "            # DIFEì—ì„œ expvalì€ ì´ë¯¸ ì¶©ì‹¤ë„ ì¸¡ì •ê°’\n",
    "            fidelity = expval\n",
    "            # í´ë¦¬í•‘ìœ¼ë¡œ ìœ íš¨ ë²”ìœ„ [0, 1] í™•ë³´\n",
    "            fidelity = np.clip(fidelity, 0.0, 1.0)\n",
    "            \n",
    "            # ì„ í˜• ì˜¤ì°¨ ê³„ì‚°\n",
    "            error = 1.0 - fidelity\n",
    "            reconstruction_errors.append(error)\n",
    "        \n",
    "        reconstruction_errors = np.array(reconstruction_errors)\n",
    "        \n",
    "        # í‰ê°€\n",
    "        metrics = evaluate_with_optimal_threshold(y_test, reconstruction_errors)\n",
    "        \n",
    "        result = {\n",
    "            'method': 'DIFE QAE',\n",
    "            'type': 'Quantum ML',\n",
    "            'training_time': training_time,\n",
    "            'circuit': final_dife_circuit,\n",
    "            'weights': weights,\n",
    "            'best_params': best_params,\n",
    "            'n_qubits': n_qubits,\n",
    "            'total_qubits': n_qubits,  # DIFEëŠ” ancilla-free\n",
    "            'reconstruction_errors': reconstruction_errors,\n",
    "            **metrics\n",
    "        }\n",
    "        \n",
    "        # ê²°ê³¼ ì¶œë ¥\n",
    "        additional_info = {\n",
    "            'í›ˆë ¨ì‹œê°„': f\"{training_time:.2f}ì´ˆ\",\n",
    "            'ì´ íë¹„íŠ¸': f\"{n_qubits} (ancilla-free)\",\n",
    "            'ë³€ë¶„ ë ˆì´ì–´': f\"{best_params['layers']}\",\n",
    "            'ìµœì  í•™ìŠµë¥ ': f\"{best_params['learning_rate']}\",\n",
    "            'íŠ¹ì§•': 'Destructive Interference',\n",
    "            'ìµœì  ì„ê³„ê°’': f\"{metrics['threshold']:.6f}\"\n",
    "        }\n",
    "        print_results(\"DIFE QAE\", metrics, additional_info)\n",
    "        print(\"âœ… DIFE QAE ì™„ë£Œ\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ DIFE QAE ì‹¤íŒ¨: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# DIFE QAE ì‹¤í–‰\n",
    "print(\"ğŸš€ DIFE QAE ë°©ë²• ì‹¤í–‰ ì¤‘...\")\n",
    "dife_result = train_dife()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02bf443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Method 8: LS-SWAP QAE (ì ì¬ ê³µê°„ SWAP í…ŒìŠ¤íŠ¸)\n",
    "# ==========================================\n",
    "\n",
    "def latent_space_swap_test(n_data_qubits, n_latent, total_qubits):\n",
    "    \"\"\"\n",
    "    ì ì¬ ê³µê°„ì— ì œí•œëœ SWAP í…ŒìŠ¤íŠ¸ êµ¬í˜„ (NewFile ê¸°ë°˜)\n",
    "    \n",
    "    ì´ ìì› ìµœì í™”ëœ SWAP í…ŒìŠ¤íŠ¸ëŠ” ì••ì¶•ëœ ì ì¬ ìƒíƒœì˜ ì¶©ì‹¤ë„ê°€ \n",
    "    ì „ì²´ ì‹œìŠ¤í…œì˜ ì¶©ì‹¤ë„ì— ëŒ€í•œ ì¶©ë¶„í•œ ëŒ€ë¦¬ ì§€í‘œë¼ëŠ” ê°€ì • í•˜ì— ì‘ë™í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Qubit layout:\n",
    "    - Latent qubits: wires 0 to n_latent-1 (ì²˜ìŒ 2ê°œ ë°ì´í„° íë¹„íŠ¸)\n",
    "    - Data qubits: wires 0 to n_data_qubits-1 (ëª¨ë“  ë°ì´í„° íë¹„íŠ¸)\n",
    "    - Reference qubits: wires n_data_qubits to n_data_qubits+n_latent-1\n",
    "    - Control qubit: wire total_qubits-1 (ë§ˆì§€ë§‰ íë¹„íŠ¸)\n",
    "    \n",
    "    Args:\n",
    "        n_data_qubits: ë°ì´í„° íë¹„íŠ¸ ìˆ˜ (4)\n",
    "        n_latent: ì ì¬ ê³µê°„ íë¹„íŠ¸ ìˆ˜ (2)  \n",
    "        total_qubits: íšŒë¡œì˜ ì´ íë¹„íŠ¸ ìˆ˜ (7)\n",
    "    \n",
    "    Returns:\n",
    "        ì œì–´ íë¹„íŠ¸ì—ì„œì˜ PauliZ ê¸°ëŒ“ê°’\n",
    "    \"\"\"\n",
    "    control_qubit = total_qubits - 1  # ë§ˆì§€ë§‰ íë¹„íŠ¸ë¥¼ ì œì–´ íë¹„íŠ¸ë¡œ ì‚¬ìš©\n",
    "    \n",
    "    # ì œì–´ íë¹„íŠ¸ì— í•˜ë‹¤ë§ˆë“œ ì ìš©\n",
    "    qml.Hadamard(wires=control_qubit)\n",
    "    \n",
    "    # ì ì¬ ê³µê°„ê³¼ ì°¸ì¡° íë¹„íŠ¸ ê°„ì˜ ì œì–´ëœ SWAP ì—°ì‚°\n",
    "    for i in range(n_latent):\n",
    "        latent_qubit = i                           # ì ì¬ ê³µê°„ íë¹„íŠ¸ (0, 1)\n",
    "        reference_qubit = n_data_qubits + i        # ì°¸ì¡° íë¹„íŠ¸ (4, 5)\n",
    "        qml.CSWAP(wires=[control_qubit, latent_qubit, reference_qubit])\n",
    "    \n",
    "    # ì œì–´ íë¹„íŠ¸ì— ìµœì¢… í•˜ë‹¤ë§ˆë“œ\n",
    "    qml.Hadamard(wires=control_qubit)\n",
    "    \n",
    "    # ì œì–´ íë¹„íŠ¸ ì¸¡ì •\n",
    "    return qml.expval(qml.PauliZ(control_qubit))\n",
    "\n",
    "def ls_swap_circuit(x, weights, n_qubits, total_qubits, layers):\n",
    "    \"\"\"\n",
    "    LS-SWAP (Latent Space SWAP Test) íšŒë¡œ êµ¬í˜„ (NewFile ê¸°ë°˜)\n",
    "    \n",
    "    íšŒë¡œ ì‹œí€€ìŠ¤:\n",
    "    1. ë°ì´í„° íë¹„íŠ¸ì— í–¥ìƒëœ qVAE ì¸ì½”ë” ì ìš©\n",
    "    2. ì ì¬ íë¹„íŠ¸(0,1)ì™€ ì°¸ì¡° íë¹„íŠ¸(4,5) ê°„ SWAP í…ŒìŠ¤íŠ¸ ìˆ˜í–‰  \n",
    "    3. ì¶©ì‹¤ë„ ì¶”ì •ì„ ìœ„í•œ ì œì–´ íë¹„íŠ¸ ì¸¡ì •\n",
    "    \n",
    "    Args:\n",
    "        x: ì…ë ¥ ë°ì´í„° íŠ¹ì„±\n",
    "        weights: í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°\n",
    "        n_qubits: ë°ì´í„° íë¹„íŠ¸ ìˆ˜ (4)\n",
    "        total_qubits: ancillaë¥¼ í¬í•¨í•œ ì´ íë¹„íŠ¸ ìˆ˜ (7)\n",
    "        layers: ë³€ë¶„ ë ˆì´ì–´ ìˆ˜\n",
    "        \n",
    "    Returns:\n",
    "        ì¶©ì‹¤ë„ ì¶”ì •ì„ ìœ„í•œ SWAP í…ŒìŠ¤íŠ¸ ê¸°ëŒ“ê°’\n",
    "    \"\"\"\n",
    "    # Enhanced qVAE ì¸ì½”ë”ë¥¼ ë°ì´í„° íë¹„íŠ¸ì— ì ìš©\n",
    "    for l in range(layers):\n",
    "        # ë§¤ê°œë³€ìˆ˜í™”ëœ íšŒì „\n",
    "        for w in range(n_qubits):\n",
    "            qml.RY(weights[l][w, 0], wires=w)\n",
    "            qml.RZ(weights[l][w, 1], wires=w)\n",
    "        \n",
    "        # ì–½í˜ ê²Œì´íŠ¸\n",
    "        if n_qubits > 1:\n",
    "            for w in range(n_qubits):\n",
    "                control = w\n",
    "                target = (w + 1) % n_qubits\n",
    "                qml.CNOT(wires=[control, target])\n",
    "        \n",
    "        # ë°ì´í„° ì¬ì—…ë¡œë”© (ì¤‘ê°„ ë ˆì´ì–´ì—ì„œ)\n",
    "        if USE_DATA_REUPLOADING and l < layers - 1:\n",
    "            data_embedding_layer(x, n_qubits, USE_ALTERNATE_EMBEDDING)\n",
    "    \n",
    "    # ì ì¬ ê³µê°„ SWAP í…ŒìŠ¤íŠ¸ ìˆ˜í–‰\n",
    "    return latent_space_swap_test(n_qubits, LS_SWAP_LATENT_QUBITS, total_qubits)\n",
    "\n",
    "def optimize_ls_swap_hyperparameters(X_train, X_val, y_val):\n",
    "    \"\"\"LS-SWAP QAE í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\"\"\"\n",
    "    print(f\"LS-SWAP QAE í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘ ({len(LS_SWAP_HYPERPARAMETER_COMBINATIONS)}ê°œ ì¡°í•©)\")\n",
    "    \n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    best_result = None\n",
    "    \n",
    "    for i, params in enumerate(LS_SWAP_HYPERPARAMETER_COMBINATIONS):\n",
    "        try:\n",
    "            print(f\"  ì¡°í•© {i+1}/{len(LS_SWAP_HYPERPARAMETER_COMBINATIONS)}: {params}\")\n",
    "            \n",
    "            # íšŒë¡œ ìƒì„±\n",
    "            n_qubits = QAE_QUBITS  # 4 data qubits\n",
    "            total_qubits = LS_SWAP_TOTAL_QUBITS  # 7 total qubits\n",
    "            dev = qml.device(\"lightning.qubit\", wires=total_qubits)\n",
    "            \n",
    "            @qml.qnode(dev)\n",
    "            def ls_swap_quantum_circuit(x, weights):\n",
    "                return ls_swap_circuit(x, weights, n_qubits, total_qubits, params['layers'])\n",
    "            \n",
    "            # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n",
    "            weights = pnp.random.uniform(-pnp.pi, pnp.pi, \n",
    "                                       (params['layers'], n_qubits, 2), requires_grad=True)\n",
    "            \n",
    "            # ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
    "            optimizer = qml.AdamOptimizer(stepsize=params['learning_rate'])\n",
    "            \n",
    "            # ê²€ì¦ìš© ë‹¨ì¶• í›ˆë ¨\n",
    "            validation_epochs = QUANTUM_TRAINING_CONFIG['validation_epochs']\n",
    "            \n",
    "            for epoch in range(validation_epochs):\n",
    "                for batch_start in range(0, len(X_train), EXPERIMENTAL_CONFIG['batch_size']):\n",
    "                    batch_end = min(batch_start + EXPERIMENTAL_CONFIG['batch_size'], len(X_train))\n",
    "                    X_batch = X_train[batch_start:batch_end]\n",
    "                    \n",
    "                    def cost_fn(w):\n",
    "                        return compute_batch_cost_ls_swap(X_batch, ls_swap_quantum_circuit, w)\n",
    "                    \n",
    "                    weights = optimizer.step(cost_fn, weights)\n",
    "            \n",
    "            # ê²€ì¦ ë°ì´í„°ë¡œ í‰ê°€\n",
    "            reconstruction_errors = []\n",
    "            for sample in X_val:\n",
    "                features = pnp.array(sample, requires_grad=False)\n",
    "                expval = ls_swap_quantum_circuit(features, weights)\n",
    "                \n",
    "                # LS-SWAP test ì¶©ì‹¤ë„ ë³€í™˜\n",
    "                fidelity = (expval + 1.0) / 2.0\n",
    "                # í´ë¦¬í•‘ìœ¼ë¡œ ìœ íš¨ ë²”ìœ„ [0, 1] í™•ë³´\n",
    "                fidelity = np.clip(fidelity, 0.0, 1.0)\n",
    "                \n",
    "                # ì„ í˜• ì˜¤ì°¨ ê³„ì‚°\n",
    "                error = 1.0 - fidelity\n",
    "                reconstruction_errors.append(error)\n",
    "            \n",
    "            reconstruction_errors = np.array(reconstruction_errors)\n",
    "            \n",
    "            # G-Meanìœ¼ë¡œ í‰ê°€\n",
    "            metrics = evaluate_with_optimal_threshold(y_val, reconstruction_errors)\n",
    "            score = metrics['gmean']\n",
    "            \n",
    "            print(f\"    -> G-Mean: {score:.4f}\")\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = params\n",
    "                best_result = {\n",
    "                    'weights': weights,\n",
    "                    'circuit': ls_swap_quantum_circuit,\n",
    "                    'dev': dev\n",
    "                }\n",
    "                print(f\"    âœ“ ìƒˆë¡œìš´ ìµœê³  ì ìˆ˜!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    âœ— ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if best_params is None:\n",
    "        # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
    "        best_params = LS_SWAP_HYPERPARAMETER_COMBINATIONS[0]\n",
    "        print(\"  ëª¨ë“  ì¡°í•© ì‹¤íŒ¨. ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©.\")\n",
    "    \n",
    "    print(f\"LS-SWAP QAE ìµœì í™” ì™„ë£Œ. ìµœê³  G-Mean: {best_score:.4f}\")\n",
    "    return best_params, best_score, best_result\n",
    "\n",
    "def train_ls_swap():\n",
    "    \"\"\"LS-SWAP QAE ëª¨ë¸ í•™ìŠµ\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ”„ METHOD 8: LS-SWAP QAE í›ˆë ¨ ì‹œì‘\")\n",
    "    print(\"=\"*60)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë˜ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©\n",
    "        if should_optimize_hyperparameters():\n",
    "            # ê²€ì¦ ë°ì´í„° ë¶„í•  (PCA ë°ì´í„° ì‚¬ìš©)\n",
    "            X_train_pca_normal = X_train_pca[y_train == 0]\n",
    "            X_train_val, X_val_normal = train_test_split(\n",
    "                X_train_pca_normal, test_size=HYPERPARAMETER_SEARCH['validation_split'], random_state=42\n",
    "            )\n",
    "            \n",
    "            # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ fraud ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸°\n",
    "            fraud_mask = y_test == 1\n",
    "            X_fraud_pca = X_test_pca[fraud_mask]\n",
    "            n_fraud_val = min(len(X_fraud_pca), len(X_val_normal) // 10)\n",
    "            X_fraud_val = X_fraud_pca[:n_fraud_val]\n",
    "            \n",
    "            X_val = np.vstack([X_val_normal, X_fraud_val])\n",
    "            y_val = np.hstack([np.zeros(len(X_val_normal)), np.ones(len(X_fraud_val))])\n",
    "            \n",
    "            print(\"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘...\")\n",
    "            best_params, best_score, best_result = optimize_ls_swap_hyperparameters(X_train_val, X_val, y_val)\n",
    "            print(f\"ìµœì  íŒŒë¼ë¯¸í„°: {best_params}\")\n",
    "        else:\n",
    "            # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
    "            best_params = get_default_params('ls_swap_qae')\n",
    "            print(\"ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©\")\n",
    "            best_result = None\n",
    "        \n",
    "        # ìµœì¢… ëª¨ë¸ í›ˆë ¨\n",
    "        print(\"ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì¤‘...\")\n",
    "        n_qubits = QAE_QUBITS  # 4 data qubits\n",
    "        total_qubits = LS_SWAP_TOTAL_QUBITS  # 7 total qubits\n",
    "        dev = qml.device(\"lightning.qubit\", wires=total_qubits)\n",
    "        \n",
    "        @qml.qnode(dev)\n",
    "        def final_ls_swap_circuit(x, weights):\n",
    "            return ls_swap_circuit(x, weights, n_qubits, total_qubits, best_params['layers'])\n",
    "        \n",
    "        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n",
    "        weights = pnp.random.uniform(-pnp.pi, pnp.pi, \n",
    "                                   (best_params['layers'], n_qubits, 2), requires_grad=True)\n",
    "        \n",
    "        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
    "        optimizer = qml.AdamOptimizer(stepsize=best_params['learning_rate'])\n",
    "        \n",
    "        # ì •ìƒ ë°ì´í„°ë§Œìœ¼ë¡œ í›ˆë ¨\n",
    "        X_train_pca_normal = X_train_pca[y_train == 0]\n",
    "        epochs = QUANTUM_TRAINING_CONFIG['epochs_ls_swap']\n",
    "        \n",
    "        print(f\"í›ˆë ¨ ì‹œì‘: {epochs} ì—í¬í¬, ë°°ì¹˜ í¬ê¸° {EXPERIMENTAL_CONFIG['batch_size']}\")\n",
    "        print(f\"LS-SWAP êµ¬ì„±: {n_qubits}ê°œ ë°ì´í„° íë¹„íŠ¸, {total_qubits}ê°œ ì´ íë¹„íŠ¸\")\n",
    "        print(f\"íŠ¹ì§•: ì ì¬ ê³µê°„ SWAP í…ŒìŠ¤íŠ¸ ({LS_SWAP_LATENT_QUBITS}ê°œ ì ì¬ íë¹„íŠ¸)\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_cost = 0\n",
    "            n_batches = 0\n",
    "            \n",
    "            # ë°°ì¹˜ë³„ í›ˆë ¨\n",
    "            for batch_start in range(0, len(X_train_pca_normal), EXPERIMENTAL_CONFIG['batch_size']):\n",
    "                batch_end = min(batch_start + EXPERIMENTAL_CONFIG['batch_size'], len(X_train_pca_normal))\n",
    "                X_batch = X_train_pca_normal[batch_start:batch_end]\n",
    "                \n",
    "                def cost_fn(w):\n",
    "                    return compute_batch_cost_ls_swap(X_batch, final_ls_swap_circuit, w)\n",
    "                \n",
    "                weights = optimizer.step(cost_fn, weights)\n",
    "                cost = compute_batch_cost_ls_swap(X_batch, final_ls_swap_circuit, weights)\n",
    "                epoch_cost += cost\n",
    "                n_batches += 1\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                avg_cost = epoch_cost / n_batches\n",
    "                print(f\"  ì—í¬í¬ {epoch + 1}/{epochs}, í‰ê·  ì„ í˜• ë¹„ìš©: {avg_cost:.6f}\")\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€\n",
    "        print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€ ì¤‘...\")\n",
    "        reconstruction_errors = []\n",
    "        \n",
    "        for sample in X_test_pca:\n",
    "            features = pnp.array(sample, requires_grad=False)\n",
    "            expval = final_ls_swap_circuit(features, weights)\n",
    "            \n",
    "            # LS-SWAP test ì¶©ì‹¤ë„ ë³€í™˜\n",
    "            fidelity = (expval + 1.0) / 2.0\n",
    "            # í´ë¦¬í•‘ìœ¼ë¡œ ìœ íš¨ ë²”ìœ„ [0, 1] í™•ë³´\n",
    "            fidelity = np.clip(fidelity, 0.0, 1.0)\n",
    "            \n",
    "            # ì„ í˜• ì˜¤ì°¨ ê³„ì‚°\n",
    "            error = 1.0 - fidelity\n",
    "            reconstruction_errors.append(error)\n",
    "        \n",
    "        reconstruction_errors = np.array(reconstruction_errors)\n",
    "        \n",
    "        # í‰ê°€\n",
    "        metrics = evaluate_with_optimal_threshold(y_test, reconstruction_errors)\n",
    "        \n",
    "        result = {\n",
    "            'method': 'LS-SWAP QAE',\n",
    "            'type': 'Quantum ML',\n",
    "            'training_time': training_time,\n",
    "            'circuit': final_ls_swap_circuit,\n",
    "            'weights': weights,\n",
    "            'best_params': best_params,\n",
    "            'n_qubits': n_qubits,\n",
    "            'total_qubits': total_qubits,\n",
    "            'reconstruction_errors': reconstruction_errors,\n",
    "            **metrics\n",
    "        }\n",
    "        \n",
    "        # ê²°ê³¼ ì¶œë ¥\n",
    "        additional_info = {\n",
    "            'í›ˆë ¨ì‹œê°„': f\"{training_time:.2f}ì´ˆ\",\n",
    "            'ë°ì´í„° íë¹„íŠ¸': f\"{n_qubits}\",\n",
    "            'ì´ íë¹„íŠ¸': f\"{total_qubits}\",\n",
    "            'ì ì¬ íë¹„íŠ¸': f\"{LS_SWAP_LATENT_QUBITS}\",\n",
    "            'ë³€ë¶„ ë ˆì´ì–´': f\"{best_params['layers']}\",\n",
    "            'ìµœì  í•™ìŠµë¥ ': f\"{best_params['learning_rate']}\",\n",
    "            'íŠ¹ì§•': 'Latent Space SWAP Test',\n",
    "            'ìµœì  ì„ê³„ê°’': f\"{metrics['threshold']:.6f}\"\n",
    "        }\n",
    "        print_results(\"LS-SWAP QAE\", metrics, additional_info)\n",
    "        print(\"âœ… LS-SWAP QAE ì™„ë£Œ\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ LS-SWAP QAE ì‹¤íŒ¨: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# LS-SWAP QAE ì‹¤í–‰\n",
    "print(\"ğŸš€ LS-SWAP QAE ë°©ë²• ì‹¤í–‰ ì¤‘...\")\n",
    "ls_swap_result = train_ls_swap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80c7a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# ğŸ”„ ì „ì²´ ì‹¤í—˜ ê²°ê³¼ ìˆ˜ì§‘ ë° í†µê³„ì  ë¶„ì„ (ì‹¤í—˜ì„¤ê³„.txt êµ¬í˜„)\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ ì „ì²´ ì‹¤í—˜ ê²°ê³¼ ìˆ˜ì§‘ ë° í†µê³„ì  ë¶„ì„ (ì‹¤í—˜ì„¤ê³„.txt êµ¬í˜„)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ê²°ê³¼ ìˆ˜ì§‘\n",
    "results = []\n",
    "result_vars = [\n",
    "    ('rf_result', 'ğŸŒ² Random Forest'),\n",
    "    ('if_result', 'ğŸ” IsolationForest'), \n",
    "    ('cnn_result', 'ğŸ—ï¸ CNN Autoencoder'),\n",
    "    ('ae_result', 'âš™ï¸ Classical Autoencoder'),\n",
    "    ('qae_angle_result', 'âš›ï¸ QAE Angle'),\n",
    "    ('enhanced_qvae_result', 'ğŸ”— Enhanced qVAE'),\n",
    "    ('dife_result', 'ğŸš« DIFE QAE'),\n",
    "    ('ls_swap_result', 'ğŸ”„ LS-SWAP QAE')\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ“Š ê²°ê³¼ ìˆ˜ì§‘ ì¤‘...\")\n",
    "for var_name, method_name in result_vars:\n",
    "    if var_name in globals() and globals()[var_name] is not None:\n",
    "        result = globals()[var_name]\n",
    "        results.append(result)\n",
    "        print(f\"  âœ… {method_name}: {result['method']}\")\n",
    "    else:\n",
    "        print(f\"  âŒ {method_name}: ê²°ê³¼ ì—†ìŒ\")\n",
    "\n",
    "print(f\"\\nì´ {len(results)}ê°œ ë°©ë²•ì˜ ê²°ê³¼ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "if len(results) == 0:\n",
    "    print(\"âŒ ìˆ˜ì§‘ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ìœ„ì˜ ì…€ë“¤ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "else:\n",
    "    # ==========================================\n",
    "    # ğŸ“Š ë°˜ë³µ ì‹¤í—˜ í†µê³„ ë¶„ì„ (ì‹¤í—˜ì„¤ê³„.txt êµ¬í˜„)\n",
    "    # ==========================================\n",
    "    \n",
    "    if STATISTICAL_CONFIG['enable_repetition']:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ğŸ“ˆ ë°˜ë³µ ì‹¤í—˜ í†µê³„ì  ë¶„ì„ (ì‹¤í—˜ì„¤ê³„.txt êµ¬í˜„)\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"ë°˜ë³µ íšŸìˆ˜: {STATISTICAL_CONFIG['n_repetitions']}íšŒ\")\n",
    "        print(f\"ì‹ ë¢°ë„ ìˆ˜ì¤€: {STATISTICAL_CONFIG['confidence_level']*100}%\")\n",
    "        print(\"ê²°ê³¼ í˜•ì‹: í‰ê·  Â± í‘œì¤€í¸ì°¨ [95% ì‹ ë¢°êµ¬ê°„]\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # ê° ë°©ë²•ì˜ í†µê³„ì  ìš”ì•½ ì¶œë ¥\n",
    "        for result in results:\n",
    "            if 'statistical_summary' in result:\n",
    "                summary = result['statistical_summary']\n",
    "                print_statistical_results(summary)\n",
    "                print()\n",
    "        \n",
    "        # ë°©ë²• ê°„ í†µê³„ì  ë¹„êµ\n",
    "        print(\"\\n\udcca ë°©ë²• ê°„ ì„±ëŠ¥ ë¹„êµ (í‰ê·  Â± í‘œì¤€í¸ì°¨)\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        comparison_data = []\n",
    "        for result in results:\n",
    "            if 'statistical_summary' in result:\n",
    "                summary = result['statistical_summary']\n",
    "                if 'gmean' in summary:\n",
    "                    gmean_data = summary['gmean']\n",
    "                    comparison_data.append({\n",
    "                        'ë°©ë²•': result['method'],\n",
    "                        'ìœ í˜•': result['type'],\n",
    "                        'G-Mean_í‰ê· ': gmean_data['mean'],\n",
    "                        'G-Mean_í‘œì¤€í¸ì°¨': gmean_data['std'],\n",
    "                        'ì‹ ë¢°êµ¬ê°„_í•˜í•œ': gmean_data.get('ci_lower', gmean_data['mean']),\n",
    "                        'ì‹ ë¢°êµ¬ê°„_ìƒí•œ': gmean_data.get('ci_upper', gmean_data['mean']),\n",
    "                        'ë³€ë™ê³„ìˆ˜': gmean_data.get('cv', 0),\n",
    "                        'ë°˜ë³µ_ìˆ˜': gmean_data['n']\n",
    "                    })\n",
    "        \n",
    "        if comparison_data:\n",
    "            df_stats = pd.DataFrame(comparison_data)\n",
    "            df_stats = df_stats.sort_values('G-Mean_í‰ê· ', ascending=False)\n",
    "            \n",
    "            print(\"ğŸ† G-Mean ì„±ëŠ¥ ìˆœìœ„ (í‰ê·  Â± í‘œì¤€í¸ì°¨):\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            for i, (_, row) in enumerate(df_stats.iterrows(), 1):\n",
    "                mean_val = row['G-Mean_í‰ê· ']\n",
    "                std_val = row['G-Mean_í‘œì¤€í¸ì°¨']\n",
    "                ci_lower = row['ì‹ ë¢°êµ¬ê°„_í•˜í•œ']\n",
    "                ci_upper = row['ì‹ ë¢°êµ¬ê°„_ìƒí•œ']\n",
    "                cv = row['ë³€ë™ê³„ìˆ˜']\n",
    "                \n",
    "                stability = \"ë§¤ìš°ì•ˆì •\" if cv < 0.05 else \"ì•ˆì •\" if cv < 0.1 else \"ë³´í†µ\" if cv < 0.2 else \"ë¶ˆì•ˆì •\"\n",
    "                \n",
    "                print(f\"{i:2d}. {row['ë°©ë²•']:20s} | {mean_val:.4f} Â± {std_val:.4f}\")\n",
    "                print(f\"    95% ì‹ ë¢°êµ¬ê°„: [{ci_lower:.4f}, {ci_upper:.4f}] | ì•ˆì •ì„±: {stability}\")\n",
    "                print(f\"    ìœ í˜•: {row['ìœ í˜•']:15s} | ë³€ë™ê³„ìˆ˜: {cv:.4f}\")\n",
    "                print()\n",
    "        \n",
    "        # ìœ í˜•ë³„ í†µê³„ ë¹„êµ\n",
    "        print(\"\\nğŸ“ˆ ìœ í˜•ë³„ ì„±ëŠ¥ í†µê³„ (Classical vs Quantum)\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        classical_stats = [data for data in comparison_data if data['ìœ í˜•'] == 'Classical ML']\n",
    "        quantum_stats = [data for data in comparison_data if data['ìœ í˜•'] == 'Quantum ML']\n",
    "        dl_stats = [data for data in comparison_data if data['ìœ í˜•'] == 'Deep Learning']\n",
    "        \n",
    "        if classical_stats:\n",
    "            classical_means = [d['G-Mean_í‰ê· '] for d in classical_stats]\n",
    "            classical_mean = np.mean(classical_means)\n",
    "            classical_std = np.std(classical_means, ddof=1) if len(classical_means) > 1 else 0\n",
    "            print(f\"ğŸ›ï¸  Classical ML: {classical_mean:.4f} Â± {classical_std:.4f} ({len(classical_stats)}ê°œ ë°©ë²•)\")\n",
    "            \n",
    "        if dl_stats:\n",
    "            dl_means = [d['G-Mean_í‰ê· '] for d in dl_stats]\n",
    "            dl_mean = np.mean(dl_means)\n",
    "            dl_std = np.std(dl_means, ddof=1) if len(dl_means) > 1 else 0\n",
    "            print(f\"ğŸ§  Deep Learning: {dl_mean:.4f} Â± {dl_std:.4f} ({len(dl_stats)}ê°œ ë°©ë²•)\")\n",
    "            \n",
    "        if quantum_stats:\n",
    "            quantum_means = [d['G-Mean_í‰ê· '] for d in quantum_stats]\n",
    "            quantum_mean = np.mean(quantum_means)\n",
    "            quantum_std = np.std(quantum_means, ddof=1) if len(quantum_means) > 1 else 0\n",
    "            print(f\"âš›ï¸  Quantum ML: {quantum_mean:.4f} Â± {quantum_std:.4f} ({len(quantum_stats)}ê°œ ë°©ë²•)\")\n",
    "        \n",
    "        # í†µê³„ì  ìœ ì˜ì„± ê²€ì¦ (Mann-Whitney U test)\n",
    "        if len(classical_stats) >= 2 and len(quantum_stats) >= 2:\n",
    "            from scipy import stats\n",
    "            \n",
    "            classical_values = []\n",
    "            quantum_values = []\n",
    "            \n",
    "            for result in results:\n",
    "                if 'statistical_summary' in result and result['type'] == 'Classical ML':\n",
    "                    if 'gmean' in result['statistical_summary']:\n",
    "                        classical_values.extend(result['statistical_summary']['gmean']['values'])\n",
    "                elif 'statistical_summary' in result and result['type'] == 'Quantum ML':\n",
    "                    if 'gmean' in result['statistical_summary']:\n",
    "                        quantum_values.extend(result['statistical_summary']['gmean']['values'])\n",
    "            \n",
    "            if len(classical_values) >= 2 and len(quantum_values) >= 2:\n",
    "                statistic, p_value = stats.mannwhitneyu(\n",
    "                    classical_values, quantum_values, alternative='two-sided'\n",
    "                )\n",
    "                \n",
    "                print(f\"\\nğŸ”¬ í†µê³„ì  ìœ ì˜ì„± ê²€ì¦:\")\n",
    "                print(f\"   Mann-Whitney U ê²€ì •: p-value = {p_value:.4f}\")\n",
    "                if p_value < 0.05:\n",
    "                    print(f\"   âœ… Classicalê³¼ Quantum ê°„ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ ì¡´ì¬ (p < 0.05)\")\n",
    "                else:\n",
    "                    print(f\"   âŒ Classicalê³¼ Quantum ê°„ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ ì—†ìŒ (p â‰¥ 0.05)\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # ğŸ“‹ ê¸°ì¡´ ê²°ê³¼ í…Œì´ë¸” (ëŒ€í‘œê°’ ë˜ëŠ” í‰ê· ê°’)\n",
    "    # ==========================================\n",
    "    \n",
    "    # ê²°ê³¼ ì •ë¦¬\n",
    "    df_results = pd.DataFrame([\n",
    "        {\n",
    "            'ë°©ë²•': r['method'],\n",
    "            'ìœ í˜•': r['type'],\n",
    "            'í›ˆë ¨ì‹œê°„(ì´ˆ)': r['training_time'],\n",
    "            'G-Mean': r['gmean'], \n",
    "            'F1-Score': r['f1_score'],\n",
    "            'AUC': r['auc'],\n",
    "            'Precision': r['precision'],\n",
    "            'Recall': r['recall'],\n",
    "            'Accuracy': r['accuracy'],\n",
    "            'ìµœì  ì„ê³„ê°’': r['threshold'],\n",
    "            'ë°˜ë³µì‹¤í—˜': 'Yes' if r.get('is_statistical_summary', False) else 'No'\n",
    "        } for r in results\n",
    "    ])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ“‹ ì „ì²´ ë°©ë²• ì„±ëŠ¥ ë¹„êµí‘œ\")\n",
    "    if STATISTICAL_CONFIG['enable_repetition']:\n",
    "        print(\"(ë°˜ë³µ ì‹¤í—˜ í‰ê· ê°’ ê¸°ë°˜)\")\n",
    "    print(\"=\"*80)\n",
    "    print(df_results.to_string(index=False, float_format='%.4f'))\n",
    "    \n",
    "    # ì„±ëŠ¥ ìˆœìœ„\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ† ì„±ëŠ¥ ìˆœìœ„ (G-Mean ê¸°ì¤€)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    df_sorted = df_results.sort_values('G-Mean', ascending=False)\n",
    "    for i, (_, row) in enumerate(df_sorted.iterrows(), 1):\n",
    "        print(f\"{i:2d}. {row['ë°©ë²•']:20s} | G-Mean: {row['G-Mean']:.4f} | {row['ìœ í˜•']}\")\n",
    "    \n",
    "    # ì „ì²´ ìµœê³  ì„±ëŠ¥\n",
    "    best_overall = df_results.loc[df_results['G-Mean'].idxmax()]\n",
    "    print(f\"\\nğŸ¥‡ ì „ì²´ ìµœê³  ì„±ëŠ¥: {best_overall['ë°©ë²•']} (G-Mean: {best_overall['G-Mean']:.4f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ¯ ì‹¤í—˜ ì™„ë£Œ ìš”ì•½\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"âœ… ì´ {len(results)}ê°œ ë°©ë²• ë¹„êµ ì™„ë£Œ\")\n",
    "    \n",
    "    if STATISTICAL_CONFIG['enable_repetition']:\n",
    "        total_experiments = len(results) * STATISTICAL_CONFIG['n_repetitions']\n",
    "        print(f\"\udd04 ë°˜ë³µ ì‹¤í—˜: {STATISTICAL_CONFIG['n_repetitions']}íšŒ Ã— {len(results)}ê°œ ë°©ë²• = {total_experiments}ê°œ ì´ ì‹¤í–‰\")\n",
    "        print(f\"\ud83dğŸ“Š í†µê³„ ë¶„ì„: í‰ê· , í‘œì¤€í¸ì°¨, 95% ì‹ ë¢°êµ¬ê°„ ê³„ì‚°\")\n",
    "        print(f\"ğŸ¯ ì‹ ë¢°ì„±: {STATISTICAL_CONFIG['confidence_level']*100}% ì‹ ë¢°ë„ ê¸°ë°˜\")\n",
    "    else:\n",
    "        print(f\"âš¡ ë‹¨ì¼ ì‹¤í–‰ ëª¨ë“œ\")\n",
    "    \n",
    "    print(f\"ğŸ“Š ë°ì´í„°ì…‹: ì‹ ìš©ì¹´ë“œ ì‚¬ê¸° íƒì§€ ({len(X_test)}ê°œ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ)\")\n",
    "    print(f\"âš–ï¸ í´ë˜ìŠ¤ ë¶ˆê· í˜•: {(y_test == 1).sum()}ê°œ ì‚¬ê¸° ê±°ë˜ ({(y_test == 1).mean()*100:.2f}%)\")\n",
    "    print(f\"ğŸ¯ í‰ê°€ ì§€í‘œ: G-Mean (ë¶ˆê· í˜• ë°ì´í„° ìµœì í™”)\")\n",
    "    print(f\"ğŸ† ìµœê³  ì„±ëŠ¥ ë‹¬ì„±: {best_overall['ë°©ë²•']} ({best_overall['ìœ í˜•']})\")\n",
    "    \n",
    "    # ë°©ë²•ë¡ ë³„ íŠ¹ì§• ìš”ì•½\n",
    "    print(f\"\\nğŸ“‹ ë°©ë²•ë¡ ë³„ íŠ¹ì§•:\")\n",
    "    method_features = {\n",
    "        'Random Forest': 'ì•™ìƒë¸” ê¸°ë°˜ íŠ¹ì„± ì¬êµ¬ì„±',\n",
    "        'IsolationForest': 'ë¹„ì§€ë„ ì´ìƒ íƒì§€ ì „ë¬¸',\n",
    "        'CNN Autoencoder': 'í•©ì„±ê³± ì‹ ê²½ë§ ê¸°ë°˜ ì°¨ì› ì¶•ì†Œ',\n",
    "        'Classical Autoencoder': 'ì™„ì „ ì—°ê²° ì‹ ê²½ë§ ê¸°ë°˜ ì°¨ì› ì¶•ì†Œ',\n",
    "        'QAE Angle': 'ì–‘ì ê°ë„ ì¸ì½”ë”© + ë³€ë¶„ íšŒë¡œ',\n",
    "        'Enhanced qVAE': 'í–¥ìƒëœ ì–‘ì ë³€ë¶„ ì˜¤í† ì¸ì½”ë”',\n",
    "        'DIFE QAE': 'íŒŒê´´ì  ê°„ì„­ + ancilla-free',\n",
    "        'LS-SWAP QAE': 'ì ì¬ ê³µê°„ SWAP í…ŒìŠ¤íŠ¸'\n",
    "    }\n",
    "    \n",
    "    for result in results:\n",
    "        method = result['method']\n",
    "        if method in method_features:\n",
    "            repetition_info = f\" ({STATISTICAL_CONFIG['n_repetitions']}íšŒ í‰ê· )\" if STATISTICAL_CONFIG['enable_repetition'] else \"\"\n",
    "            print(f\"  â€¢ {method}{repetition_info}: {method_features[method]}\")\n",
    "    \n",
    "    print(f\"\\nâš¡ ì‹¤í—˜ ì„¤ê³„ ì›ì¹™ (ì‹¤í—˜ì„¤ê³„.txt êµ¬í˜„):\")\n",
    "    print(f\"  â€¢ í†µì œëœ ë³€ìˆ˜: ë™ì¼í•œ ë°ì´í„° ë¶„í• , ì „ì²˜ë¦¬, í‰ê°€ ì§€í‘œ\")\n",
    "    print(f\"  â€¢ ë…ë¦½ ë³€ìˆ˜: 8ê°€ì§€ ë°©ë²•ë¡  (Classical 4ê°œ + Quantum 4ê°œ)\")\n",
    "    print(f\"  â€¢ ì¢…ì† ë³€ìˆ˜: AUC-ROC, G-Mean, F1-Score, ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨\")\n",
    "    print(f\"  â€¢ ë°˜ë³µ ì¸¡ì •: {'ê° ë°©ë²• %díšŒ ë…ë¦½ ì‹¤í–‰' % STATISTICAL_CONFIG['n_repetitions'] if STATISTICAL_CONFIG['enable_repetition'] else 'ë‹¨ì¼ ì‹¤í–‰'}\")\n",
    "    print(f\"  â€¢ ë‚œìˆ˜ ì‹œë“œ: {'ë…ë¦½ ì‹œë“œ (%s)' % str(experiment_seeds) if STATISTICAL_CONFIG['enable_repetition'] else 'ê³ ì • ì‹œë“œ (42)'}\")\n",
    "    print(f\"  â€¢ í†µê³„ ë¶„ì„: {'í‰ê· Â±í‘œì¤€í¸ì°¨, 95% ì‹ ë¢°êµ¬ê°„' if STATISTICAL_CONFIG['enable_repetition'] else 'ë‹¨ì¼ê°’'}\")\n",
    "    print(f\"  â€¢ ì¬í˜„ì„±: ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‹œë“œ ë™ê¸°í™”\")\n",
    "    print(f\"  â€¢ ì–‘ì ìµœì í™”: 4D PCA íŠ¹ì§•ìœ¼ë¡œ ì–‘ì íšŒë¡œ íš¨ìœ¨ì„± ê·¹ëŒ€í™”\")\n",
    "    \n",
    "    print(f\"\\nğŸ”¬ ê¸°ìˆ ì  êµ¬í˜„:\")\n",
    "    print(f\"  â€¢ ì–‘ì í”„ë ˆì„ì›Œí¬: PennyLane v{qml.__version__}\")\n",
    "    print(f\"  â€¢ í´ë˜ì‹ ML: scikit-learn\")\n",
    "    print(f\"  â€¢ ë”¥ëŸ¬ë‹: TensorFlow/Keras\") \n",
    "    print(f\"  â€¢ ì–‘ì íë¹„íŠ¸: {QAE_QUBITS}ê°œ (ë°ì´í„°) + ë³´ì¡° íë¹„íŠ¸\")\n",
    "    print(f\"  â€¢ ì‹¤í–‰ í™˜ê²½: Lightning ì‹œë®¬ë ˆì´í„°\")\n",
    "    \n",
    "    # ì‹¤í—˜ ê²°ê³¼ ì €ì¥ (ì„ íƒì )\n",
    "    if STATISTICAL_CONFIG.get('save_individual_results', False):\n",
    "        save_experiment_results()\n",
    "\n",
    "print(f\"\\nğŸ‰ ì‹¤í—˜ì„¤ê³„.txt ê¸°ë°˜ {'ë°˜ë³µ ì¸¡ì • í†µê³„' if STATISTICAL_CONFIG['enable_repetition'] else 'ë‹¨ì¼'} ì‹¤í—˜ ì™„ë£Œ!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8335dbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# ê³ ê¸‰ ì‹œê°í™” ë° í†µê³„ ë¶„ì„\n",
    "# ==========================================\n",
    "\n",
    "if len(results) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ“Š ê³ ê¸‰ ì‹œê°í™” ë° í†µê³„ ë¶„ì„\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. ì„±ëŠ¥ ë¹„êµ ë°” ì°¨íŠ¸\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('8-Method Comprehensive Comparison: Classical vs Quantum ML for Fraud Detection', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # G-Mean ë¹„êµ\n",
    "    ax1 = axes[0, 0]\n",
    "    methods = [r['method'] for r in results]\n",
    "    gmeans = [r['gmean'] for r in results]\n",
    "    colors = ['skyblue' if r['type'] == 'Classical ML' else 'lightcoral' for r in results]\n",
    "    \n",
    "    bars = ax1.bar(range(len(methods)), gmeans, color=colors, alpha=0.8)\n",
    "    ax1.set_title('G-Mean Comparison', fontweight='bold')\n",
    "    ax1.set_ylabel('G-Mean Score')\n",
    "    ax1.set_xticks(range(len(methods)))\n",
    "    ax1.set_xticklabels([m.replace(' ', '\\n') for m in methods], rotation=45, ha='right')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ê°’ í‘œì‹œ\n",
    "    for bar, gmean in zip(bars, gmeans):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                f'{gmean:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # F1-Score ë¹„êµ\n",
    "    ax2 = axes[0, 1] \n",
    "    f1_scores = [r['f1'] for r in results]\n",
    "    bars2 = ax2.bar(range(len(methods)), f1_scores, color=colors, alpha=0.8)\n",
    "    ax2.set_title('F1-Score Comparison', fontweight='bold')\n",
    "    ax2.set_ylabel('F1-Score')\n",
    "    ax2.set_xticks(range(len(methods)))\n",
    "    ax2.set_xticklabels([m.replace(' ', '\\n') for m in methods], rotation=45, ha='right')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    for bar, f1 in zip(bars2, f1_scores):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                f'{f1:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # AUC ë¹„êµ\n",
    "    ax3 = axes[1, 0]\n",
    "    aucs = [r['auc'] for r in results]\n",
    "    bars3 = ax3.bar(range(len(methods)), aucs, color=colors, alpha=0.8)\n",
    "    ax3.set_title('AUC Comparison', fontweight='bold')\n",
    "    ax3.set_ylabel('AUC Score')\n",
    "    ax3.set_xticks(range(len(methods)))\n",
    "    ax3.set_xticklabels([m.replace(' ', '\\n') for m in methods], rotation=45, ha='right')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    for bar, auc in zip(bars3, aucs):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                f'{auc:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # í›ˆë ¨ ì‹œê°„ ë¹„êµ\n",
    "    ax4 = axes[1, 1]\n",
    "    training_times = [r['training_time'] for r in results]\n",
    "    bars4 = ax4.bar(range(len(methods)), training_times, color=colors, alpha=0.8)\n",
    "    ax4.set_title('Training Time Comparison', fontweight='bold')\n",
    "    ax4.set_ylabel('Training Time (seconds)')\n",
    "    ax4.set_xticks(range(len(methods)))\n",
    "    ax4.set_xticklabels([m.replace(' ', '\\n') for m in methods], rotation=45, ha='right')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.set_yscale('log')  # ë¡œê·¸ ìŠ¤ì¼€ì¼ë¡œ ì‹œê°„ ì°¨ì´ ì‹œê°í™”\n",
    "    \n",
    "    for bar, time in zip(bars4, training_times):\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height * 1.1,\n",
    "                f'{time:.1f}s', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # ë²”ë¡€ ì¶”ê°€\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor='skyblue', label='Classical ML'),\n",
    "                      Patch(facecolor='lightcoral', label='Quantum ML')]\n",
    "    fig.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(0.98, 0.98))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. ë ˆì´ë” ì°¨íŠ¸ - ìµœê³  ì„±ëŠ¥ ë°©ë²•ë“¤ ë¹„êµ\n",
    "    print(\"\\nğŸ“ˆ ìƒìœ„ 4ê°œ ë°©ë²• ìƒì„¸ ë¹„êµ\")\n",
    "    \n",
    "    df_sorted = df_results.sort_values('G-Mean', ascending=False)\n",
    "    top_methods = df_sorted.head(4)\n",
    "    \n",
    "    # ë ˆì´ë” ì°¨íŠ¸ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„\n",
    "    categories = ['G-Mean', 'F1-Score', 'AUC', 'Precision', 'Recall', 'Accuracy']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    colors = ['red', 'blue', 'green', 'orange']\n",
    "    \n",
    "    for i, (_, method_data) in enumerate(top_methods.iterrows()):\n",
    "        values = [\n",
    "            method_data['G-Mean'],\n",
    "            method_data['F1-Score'], \n",
    "            method_data['AUC'],\n",
    "            method_data['Precision'],\n",
    "            method_data['Recall'],\n",
    "            method_data['Accuracy']\n",
    "        ]\n",
    "        \n",
    "        # ë ˆì´ë” ì°¨íŠ¸ë¥¼ ìœ„í•´ ì²« ë²ˆì§¸ ê°’ì„ ë§ˆì§€ë§‰ì— ì¶”ê°€\n",
    "        values += values[:1]\n",
    "        \n",
    "        # ê°ë„ ê³„ì‚°\n",
    "        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "        angles += angles[:1]\n",
    "        \n",
    "        # í”Œë¡¯\n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=method_data['ë°©ë²•'], color=colors[i])\n",
    "        ax.fill(angles, values, alpha=0.1, color=colors[i])\n",
    "    \n",
    "    # ë ˆì´ë” ì°¨íŠ¸ ì„¤ì •\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(categories)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title('Top 4 Methods Performance Radar Chart', size=14, fontweight='bold', pad=20)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # 3. í†µê³„ì  ìœ ì˜ì„± ê²€ì¦\n",
    "    print(\"\\nğŸ”¬ í†µê³„ì  ë¶„ì„\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    if len(classical_results) > 0 and len(quantum_results) > 0:\n",
    "        from scipy import stats\n",
    "        \n",
    "        print(\"ğŸ“Š Classical vs Quantum ML ì„±ëŠ¥ ë¹„êµ:\")\n",
    "        print(f\"Classical ML G-Mean: {classical_results['G-Mean'].mean():.4f} Â± {classical_results['G-Mean'].std():.4f}\")\n",
    "        print(f\"Quantum ML G-Mean: {quantum_results['G-Mean'].mean():.4f} Â± {quantum_results['G-Mean'].std():.4f}\")\n",
    "        \n",
    "        # Mann-Whitney U í…ŒìŠ¤íŠ¸ (ë¹„ëª¨ìˆ˜ì  ê²€ì •)\n",
    "        if len(classical_results) >= 2 and len(quantum_results) >= 2:\n",
    "            statistic, p_value = stats.mannwhitneyu(\n",
    "                classical_results['G-Mean'], \n",
    "                quantum_results['G-Mean'], \n",
    "                alternative='two-sided'\n",
    "            )\n",
    "            print(f\"Mann-Whitney U í…ŒìŠ¤íŠ¸: p-value = {p_value:.4f}\")\n",
    "            if p_value < 0.05:\n",
    "                print(\"âœ… í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ ì¡´ì¬ (p < 0.05)\")\n",
    "            else:\n",
    "                print(\"âŒ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ ì—†ìŒ (p â‰¥ 0.05)\")\n",
    "        else:\n",
    "            print(\"âš ï¸  í†µê³„ ê²€ì •ì„ ìœ„í•œ ì¶©ë¶„í•œ ìƒ˜í”Œ ìˆ˜ ë¶€ì¡±\")\n",
    "    \n",
    "    # 4. ì„±ëŠ¥ íš¨ìœ¨ì„± ë¶„ì„\n",
    "    print(f\"\\nâš¡ ì„±ëŠ¥ íš¨ìœ¨ì„± ë¶„ì„ (G-Mean / í›ˆë ¨ì‹œê°„)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    efficiency_data = []\n",
    "    for result in results:\n",
    "        efficiency = result['gmean'] / result['training_time'] if result['training_time'] > 0 else 0\n",
    "        efficiency_data.append({\n",
    "            'ë°©ë²•': result['method'],\n",
    "            'ìœ í˜•': result['type'], \n",
    "            'G-Mean': result['gmean'],\n",
    "            'í›ˆë ¨ì‹œê°„': result['training_time'],\n",
    "            'íš¨ìœ¨ì„±': efficiency\n",
    "        })\n",
    "    \n",
    "    df_efficiency = pd.DataFrame(efficiency_data)\n",
    "    df_efficiency = df_efficiency.sort_values('íš¨ìœ¨ì„±', ascending=False)\n",
    "    \n",
    "    print(\"ğŸƒâ€â™‚ï¸ íš¨ìœ¨ì„± ìˆœìœ„ (G-Mean per second):\")\n",
    "    for i, (_, row) in enumerate(df_efficiency.iterrows(), 1):\n",
    "        print(f\"{i:2d}. {row['ë°©ë²•']:20s} | {row['íš¨ìœ¨ì„±']:.6f} | {row['ìœ í˜•']}\")\n",
    "    \n",
    "    # 5. í˜¼ë™ í–‰ë ¬ íˆíŠ¸ë§µ (ìµœê³  ì„±ëŠ¥ ë°©ë²•)\n",
    "    best_method = df_results.loc[df_results['G-Mean'].idxmax()]\n",
    "    best_result = next(r for r in results if r['method'] == best_method['ë°©ë²•'])\n",
    "    \n",
    "    print(f\"\\nğŸ¯ ìµœê³  ì„±ëŠ¥ ë°©ë²• ({best_method['ë°©ë²•']}) ìƒì„¸ ë¶„ì„\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    if 'reconstruction_errors' in best_result:\n",
    "        threshold = best_result['threshold']\n",
    "        y_pred = (best_result['reconstruction_errors'] > threshold).astype(int)\n",
    "        \n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=['Normal', 'Fraud'],\n",
    "                   yticklabels=['Normal', 'Fraud'])\n",
    "        plt.title(f'Confusion Matrix - {best_method[\"ë°©ë²•\"]}', fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.show()\n",
    "        \n",
    "        # ë¶„ë¥˜ ë¦¬í¬íŠ¸\n",
    "        from sklearn.metrics import classification_report\n",
    "        print(\"ğŸ“‹ ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
    "        print(classification_report(y_test, y_pred, target_names=['Normal', 'Fraud']))\n",
    "        \n",
    "        # ROC ê³¡ì„ \n",
    "        from sklearn.metrics import roc_curve\n",
    "        fpr, tpr, _ = roc_curve(y_test, best_result['reconstruction_errors'])\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, color='red', lw=2, label=f'ROC curve (AUC = {best_result[\"auc\"]:.3f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curve - {best_method[\"ë°©ë²•\"]}', fontweight='bold')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "print(\"\\nğŸ‰ ì „ì²´ ì‹œê°í™” ë° ë¶„ì„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a817d66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# ğŸ“‹ ì‹¤í—˜ì„¤ê³„.txt êµ¬í˜„ ì™„ë£Œ ìš”ì•½\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"ğŸ‰\"*20)\n",
    "print(\"ğŸ“‹ ì‹¤í—˜ì„¤ê³„.txt 'ì—°êµ¬ ì„¤ê³„ë¥¼ ìœ„í•œ êµ¬ì²´ì ì¸ ì œì•ˆ' êµ¬í˜„ ì™„ë£Œ ë³´ê³ ì„œ\")\n",
    "print(\"ğŸ‰\"*20)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… êµ¬í˜„ëœ í•µì‹¬ ê¸°ëŠ¥ë“¤\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "implementation_status = [\n",
    "    (\"ğŸ² ë‚œìˆ˜ ì‹œë“œ ê´€ë¦¬\", \"ì™„ë£Œ\", [\n",
    "        \"â€¢ get_experiment_seeds(): ê° ë°˜ë³µë§ˆë‹¤ ë…ë¦½ì ì¸ ì‹œë“œ ìƒì„±\",\n",
    "        \"â€¢ set_experiment_seed(): ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ë™ê¸°í™” (NumPy, TF, PennyLane)\",\n",
    "        f\"â€¢ ì‹¤í—˜ë³„ ë…ë¦½ ì‹œë“œ: {experiment_seeds if STATISTICAL_CONFIG['enable_repetition'] else [EXPERIMENTAL_CONFIG['random_state']]}\",\n",
    "        \"â€¢ í™˜ê²½ë³€ìˆ˜ ì„¤ì •ìœ¼ë¡œ ì¬í˜„ì„± ë³´ì¥\"\n",
    "    ]),\n",
    "    \n",
    "    (\"ğŸ”„ ë°˜ë³µ íšŸìˆ˜ ì„¤ì •\", \"ì™„ë£Œ\", [\n",
    "        f\"â€¢ ê° ë°©ë²•ë³„ {STATISTICAL_CONFIG['n_repetitions']}íšŒ ë…ë¦½ ì‹¤í–‰\",\n",
    "        f\"â€¢ ë°˜ë³µ ì‹¤í—˜ í™œì„±í™”: {STATISTICAL_CONFIG['enable_repetition']}\",\n",
    "        \"â€¢ run_multiple_experiments(): ë°˜ë³µ ì‹¤í—˜ ë˜í¼ í•¨ìˆ˜\",\n",
    "        \"â€¢ ê°œë³„ ê²°ê³¼ ì €ì¥ ë° ê´€ë¦¬\"\n",
    "    ]),\n",
    "    \n",
    "    (\"ğŸ“Š ê²°ê³¼ ê¸°ë¡ êµ¬ì¡°\", \"ì™„ë£Œ\", [\n",
    "        \"â€¢ experiment_results_store: ì „ì—­ ê²°ê³¼ ì €ì¥ì†Œ\",\n",
    "        \"â€¢ individual_results: ê° ë°˜ë³µ ì‹¤í–‰ ê²°ê³¼ ë³´ê´€\",\n",
    "        \"â€¢ statistical_summary: í†µê³„ì  ìš”ì•½ ì •ë³´\",\n",
    "        \"â€¢ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ ê²°ê³¼ êµ¬ì¡°\"\n",
    "    ]),\n",
    "    \n",
    "    (\"ğŸ“ˆ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ê³„ì‚°\", \"ì™„ë£Œ\", [\n",
    "        f\"â€¢ compute_statistical_summary_advanced(): ê³ ê¸‰ í†µê³„ ê³„ì‚°\",\n",
    "        f\"â€¢ t-ë¶„í¬ ê¸°ë°˜ 95% ì‹ ë¢°êµ¬ê°„ ({STATISTICAL_CONFIG['confidence_level']*100}%)\",\n",
    "        \"â€¢ ë³€ë™ê³„ìˆ˜(CV)ë¡œ ì•ˆì •ì„± í‰ê°€\",\n",
    "        \"â€¢ í‰ê·  Â± í‘œì¤€í¸ì°¨ í˜•ì‹ ë³´ê³ \"\n",
    "    ]),\n",
    "    \n",
    "    (\"ğŸ¯ ìµœì¢… ê²°ê³¼ ë³´ê³ \", \"ì™„ë£Œ\", [\n",
    "        \"â€¢ print_statistical_results(): í†µê³„ ìš”ì•½ ì¶œë ¥\",\n",
    "        \"â€¢ ì‹ ë¢°êµ¬ê°„ [í•˜í•œ, ìƒí•œ] í‘œê¸°\",\n",
    "        \"â€¢ ì„±ëŠ¥ ì•ˆì •ì„± ë¶„ë¥˜ (ë§¤ìš°ì•ˆì •/ì•ˆì •/ë³´í†µ/ë¶ˆì•ˆì •)\",\n",
    "        \"â€¢ Mann-Whitney U ê²€ì •ìœ¼ë¡œ ìœ ì˜ì„± ë¶„ì„\"\n",
    "    ])\n",
    "]\n",
    "\n",
    "for category, status, details in implementation_status:\n",
    "    print(f\"\\n{category} - âœ… {status}\")\n",
    "    for detail in details:\n",
    "        print(f\"    {detail}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š ì‹¤í—˜ ì„¤ì • í˜„í™©\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"ğŸ”„ ë°˜ë³µ ì‹¤í—˜ ì„¤ì •:\")\n",
    "print(f\"    â€¢ í™œì„±í™”: {STATISTICAL_CONFIG['enable_repetition']}\")\n",
    "print(f\"    â€¢ ë°˜ë³µ íšŸìˆ˜: {STATISTICAL_CONFIG['n_repetitions']}íšŒ\")\n",
    "print(f\"    â€¢ ì‹ ë¢°ë„: {STATISTICAL_CONFIG['confidence_level']*100}%\")\n",
    "print(f\"    â€¢ ì‹œë“œ ê´€ë¦¬: ë…ë¦½ì  ì‹œë“œ ìƒì„±\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ í†µê³„ì  ë¶„ì„:\")\n",
    "print(f\"    â€¢ ê¸°ìˆ í†µê³„: í‰ê· , í‘œì¤€í¸ì°¨, ìµœì†Ÿê°’, ìµœëŒ“ê°’, ì¤‘ì•™ê°’\")\n",
    "print(f\"    â€¢ ì¶”ë¡ í†µê³„: t-ë¶„í¬ ê¸°ë°˜ ì‹ ë¢°êµ¬ê°„\")\n",
    "print(f\"    â€¢ ì•ˆì •ì„±: ë³€ë™ê³„ìˆ˜(CV) ê³„ì‚°\")\n",
    "print(f\"    â€¢ ê°€ì„¤ê²€ì •: Mann-Whitney U ê²€ì •\")\n",
    "\n",
    "print(f\"\\nğŸ¯ í†µì œ ë³€ì¸:\")\n",
    "print(f\"    â€¢ ë°ì´í„°: ë™ì¼í•œ preprocessed-creditcard.csv\")\n",
    "print(f\"    â€¢ ë¶„í• : ê³ ì • ë¹„ìœ¨ (train {100*(1-EXPERIMENTAL_CONFIG['test_size'])}%, test {EXPERIMENTAL_CONFIG['test_size']*100}%)\")\n",
    "print(f\"    â€¢ ì „ì²˜ë¦¬: StandardScaler í†µì¼ ì ìš©\")\n",
    "print(f\"    â€¢ í‰ê°€: ë™ì¼í•œ G-Mean ì„ê³„ê°’ ìµœì í™”\")\n",
    "print(f\"    â€¢ ë°°ì¹˜: ëª¨ë“  ë°©ë²• {EXPERIMENTAL_CONFIG['batch_size']} ê³ ì •\")\n",
    "\n",
    "print(f\"\\nâš›ï¸ ì–‘ì ë°©ë²• ì„¤ì •:\")\n",
    "print(f\"    â€¢ PCA ì°¨ì›: {EXPERIMENTAL_CONFIG['pca_dimensions']}D (ëª¨ë“  ì–‘ì ë°©ë²• í†µì¼)\")\n",
    "print(f\"    â€¢ ë³€ë¶„ ë ˆì´ì–´: {EXPERIMENTAL_CONFIG['quantum_layers']}ê°œ (ëª¨ë“  ì–‘ì ë°©ë²• ê³ ì •)\")\n",
    "print(f\"    â€¢ íë¹„íŠ¸ ìˆ˜: QAE({QAE_QUBITS}), Enhanced qVAE({QVAE_TOTAL_QUBITS}), DIFE({DIFE_TOTAL_QUBITS}), LS-SWAP({LS_SWAP_TOTAL_QUBITS})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ† ì‹¤í—˜ì„¤ê³„.txt ìš”êµ¬ì‚¬í•­ ëŒ€ì¡°í‘œ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "requirements_check = [\n",
    "    (\"ë‚œìˆ˜ ì‹œë“œ ê´€ë¦¬\", \"âœ… ì™„ë£Œ\", \"ê° ë°˜ë³µë§ˆë‹¤ ë‹¤ë¥¸ ë‚œìˆ˜ ì‹œë“œ ì„¤ì •\"),\n",
    "    (\"ë°˜ë³µ íšŸìˆ˜ ì„¤ì •\", \"âœ… ì™„ë£Œ\", f\"{STATISTICAL_CONFIG['n_repetitions']}íšŒ ë°˜ë³µ ì‹¤í–‰\"),\n",
    "    (\"ê²°ê³¼ ê¸°ë¡\", \"âœ… ì™„ë£Œ\", \"ê°œë³„ ì‹¤í–‰ ê²°ê³¼ ì €ì¥\"),\n",
    "    (\"í‰ê·  ê³„ì‚°\", \"âœ… ì™„ë£Œ\", \"ëª¨ë“  ì§€í‘œì˜ í‰ê· ê°’\"),\n",
    "    (\"í‘œì¤€í¸ì°¨ ê³„ì‚°\", \"âœ… ì™„ë£Œ\", \"í‘œë³¸ í‘œì¤€í¸ì°¨ (N-1)\"),\n",
    "    (\"ì‹ ë¢°êµ¬ê°„\", \"âœ… ì™„ë£Œ\", \"t-ë¶„í¬ ê¸°ë°˜ 95% êµ¬ê°„\"),\n",
    "    (\"ìµœì¢… ê²°ê³¼ ë³´ê³ \", \"âœ… ì™„ë£Œ\", \"'í‰ê·  Â± í‘œì¤€í¸ì°¨' í˜•ì‹\"),\n",
    "    (\"í†µê³„ì  ìœ ì˜ì„±\", \"âœ… ì™„ë£Œ\", \"Mann-Whitney U ê²€ì •\"),\n",
    "    (\"ì„±ëŠ¥ ì•ˆì •ì„±\", \"âœ… ì™„ë£Œ\", \"ë³€ë™ê³„ìˆ˜ë¡œ ì•ˆì •ì„± í‰ê°€\"),\n",
    "    (\"ì¬í˜„ì„± ë³´ì¥\", \"âœ… ì™„ë£Œ\", \"ì‹œë“œ ë™ê¸°í™” ë° í™˜ê²½ ì„¤ì •\")\n",
    "]\n",
    "\n",
    "for requirement, status, description in requirements_check:\n",
    "    print(f\"  {requirement:20s} | {status:10s} | {description}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ”¬ ê³¼í•™ì  ì—°êµ¬ í‘œì¤€ ì¤€ìˆ˜\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "scientific_standards = [\n",
    "    \"âœ… í†µì œëœ ì‹¤í—˜ ì„¤ê³„ (Controlled Experiment Design)\",\n",
    "    \"âœ… ë…ë¦½ì  ë°˜ë³µ ì¸¡ì • (Independent Replication)\",\n",
    "    \"âœ… í†µê³„ì  ì‹ ë¢°ë„ ë¶„ì„ (Statistical Confidence Analysis)\", \n",
    "    \"âœ… ê°€ì„¤ ê²€ì • (Hypothesis Testing)\",\n",
    "    \"âœ… ì¬í˜„ê°€ëŠ¥í•œ ì—°êµ¬ (Reproducible Research)\",\n",
    "    \"âœ… íˆ¬ëª…í•œ ë°©ë²•ë¡  (Transparent Methodology)\",\n",
    "    \"âœ… ì¢…í•©ì  í‰ê°€ ì§€í‘œ (Comprehensive Evaluation Metrics)\",\n",
    "    \"âœ… ë¶ˆí¸í–¥ ë¹„êµ (Unbiased Comparison)\"\n",
    "]\n",
    "\n",
    "for standard in scientific_standards:\n",
    "    print(f\"  {standard}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ ì‹¤í—˜ì˜ ê³¼í•™ì  ê°€ì¹˜:\")\n",
    "print(f\"    â€¢ í†µê³„ì  ì—„ë°€ì„±: {STATISTICAL_CONFIG['n_repetitions']}íšŒ ë…ë¦½ ì‹¤í–‰ìœ¼ë¡œ ìš°ì—° íš¨ê³¼ ì œê±°\")\n",
    "print(f\"    â€¢ ì‹ ë¢°ì„± í™•ë³´: 95% ì‹ ë¢°êµ¬ê°„ìœ¼ë¡œ ê²°ê³¼ì˜ ì‹ ë¢°ë„ ì •ëŸ‰í™”\")\n",
    "print(f\"    â€¢ ê³µì •í•œ ë¹„êµ: ëª¨ë“  ë°©ë²•ì— ë™ì¼í•œ ì‹¤í—˜ ì¡°ê±´ ì ìš©\")\n",
    "print(f\"    â€¢ ì¬í˜„ì„± ë³´ì¥: ì‹œë“œ ê´€ë¦¬ë¡œ ë™ì¼í•œ ì¡°ê±´ì—ì„œ ì¬ì‹¤í–‰ ê°€ëŠ¥\")\n",
    "print(f\"    â€¢ íˆ¬ëª…ì„± í™•ë³´: ëª¨ë“  ì‹¤í—˜ ê³¼ì •ê³¼ íŒŒë¼ë¯¸í„° ê³µê°œ\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ê²°ë¡ :\")\n",
    "print(f\"    ì‹¤í—˜ì„¤ê³„.txtì˜ 'ì—°êµ¬ ì„¤ê³„ë¥¼ ìœ„í•œ êµ¬ì²´ì ì¸ ì œì•ˆ'ì´ ì™„ì „íˆ êµ¬í˜„ë˜ì–´,\")\n",
    "print(f\"    ê³¼í•™ì ìœ¼ë¡œ ì—„ë°€í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì–‘ì-ë¹„ì–‘ì ë¨¸ì‹ ëŸ¬ë‹ ì„±ëŠ¥ ë¹„êµ ì—°êµ¬ê°€\")\n",
    "print(f\"    ìˆ˜í–‰ë  ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"\\n\" + \"ğŸ‰\"*20)\n",
    "print(\"ğŸ ì‹¤í—˜ì„¤ê³„.txt êµ¬í˜„ ì™„ë£Œ!\")\n",
    "print(\"ğŸ‰\"*20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my312",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
